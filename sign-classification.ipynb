{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorchvideo\n!pip install torchsummary","metadata":{"papermill":{"duration":16.353846,"end_time":"2022-05-26T12:44:16.340827","exception":false,"start_time":"2022-05-26T12:43:59.986981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T12:47:58.102272Z","iopub.execute_input":"2022-06-12T12:47:58.102655Z","iopub.status.idle":"2022-06-12T12:48:31.339382Z","shell.execute_reply.started":"2022-06-12T12:47:58.102573Z","shell.execute_reply":"2022-06-12T12:48:31.338481Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pytorchvideo\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m295.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fvcore\n  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting av\n  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting parameterized\n  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\nCollecting iopath\n  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\nRequirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.63.0)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.1.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.0.1)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.8.9)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.4.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->pytorchvideo) (5.1.1)\nBuilding wheels for collected packages: pytorchvideo, fvcore\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188714 sha256=8487ff971e9609e93c0b801def1ef03e656501f6c9c0a7c7966d9fc370b7e95e\n  Stored in directory: /root/.cache/pip/wheels/e8/51/05/053b29bac2400cbbae2fb7cfc41afd280d627bca7c9363ca80\n  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=3cfaf174babd5452aa28f268e2bf1d4c91aa0a42c13426291f2dce071f1d6055\n  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\nSuccessfully built pytorchvideo fvcore\nInstalling collected packages: parameterized, av, iopath, fvcore, pytorchvideo\nSuccessfully installed av-9.2.0 fvcore-0.1.5.post20220512 iopath-0.1.9 parameterized-0.8.1 pytorchvideo-0.1.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n) \nimport pickle\n\n\nimport os\n\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize\n#from torchvision.transforms._transforms_video import CenterCropVideo\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    UniformTemporalSubsample,\n)\n\nfrom tqdm import tqdm\n\nfrom collections import OrderedDict\n\nimport torch.optim as optim\n\nfrom torch.autograd import Variable\n\n\nimport torchvision.models as models\n\nimport sys\nfrom torchsummary import summary\n\n\n\n\n\n\nclass SignDataset(Dataset):\n    def __init__(self,x,y):\n        self.x = x\n        self.y = y\n\n\n    def __len__(self):\n\n        return len(self.y)\n\n        # length = 0\n        # with open(self.file, 'rb') as f:\n        #     data = pickle.load(f)\n\n        # for x in data:\n        #     length += len(data[x])\n        # return length\n\n\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n        ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.247058,"end_time":"2022-05-26T12:44:18.677408","exception":false,"start_time":"2022-05-26T12:44:16.43035","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T12:48:31.341195Z","iopub.execute_input":"2022-06-12T12:48:31.341416Z","iopub.status.idle":"2022-06-12T12:48:33.143286Z","shell.execute_reply.started":"2022-06-12T12:48:31.341387Z","shell.execute_reply":"2022-06-12T12:48:33.142551Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"IMAGE_HEIGHT = 720\nIMAGE_WIDTH = 800\nIMAGE_CHANNEL = 1\nNUM_FRAMES = 25\nNUM_CLASSES = 60\nneg = 0\n\n\n\n\n\ndef transform_data(x):\n    \n    transform =  ApplyTransformToKey(\n        key=\"video\",\n        transform=Compose(\n            [\n                Lambda(lambda x: x/255.0),\n                Normalize(([0.2948, 0.4811, 0.6757]), ([0.1593, 0.1641, 0.2440])),\n                Grayscale(num_output_channels=1),\n                CenterCrop([720,900]),\n                Resize([512,512]),\n                Lambda(lambda x: x.permute(1,2,3,0)),#(channel, frames(depth), height, width)\n#                 Lambda(lambda x: print(x.shape))\n\n            ]\n\n        ),\n    )\n    \n    return transform(x)\n\ndef lp_video(video,start_time, end_time):\n    video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n            #print(video_data[\"video\"].shape)\n \n    if video_data[\"video\"] is None:\n        return None        # or pass\n    else:\n    \n        #video_data[\"video\"] = Grayscale(num_output_channels=1)((video_data[\"video\"]).permute(1,0,2,3))\n    #             video_data[\"video\"] = video_data[\"video\"]/255\n                #print(video_data[\"video\"].shape)\n        video_data[\"video\"] = video_data[\"video\"].permute(1,0,2,3)\n#         std, mean = torch.std_mean(video_data[\"video\"],dim=[0,2,3])\n#         std = std/255.0\n#         #print(std)\n#         mean = mean/255.0\n        #print(mean)\n        video_data = transform_data( video_data)\n\n        return video_data[\"video\"]\n\ndef load_dataloaders(batch_size,start,end):\n\n    \n    train_inputs =[] #x\n    train_classes = [] #y\n    global neg\n\n    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n        data = pickle.load(f)\n\n\n# keys are files so iterate only limited files due to memory limitations.\n    for key in list(data.keys())[start:end]:\n        time_start = []\n        time_end = []\n        filename = key\n        print(\"file\",filename)\n        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n    # file functions\n        \n        for x in data[key]:\n            img_cls = x[0]\n            time_start.append(x[1])\n            time_end.append(x[2])\n            \n            \n            # start_time = x[1]\n            # end_time = x[2]\n#             vid = lp_video(video,x[1], x[2])\n\n#             for m in torch.unbind(vid, dim=3):\n#                 train_classes.append(img_cls)\n#                 train_inputs.append(m)\n\n\n            #some negative classes too\n        for i in range(len(time_start)-2):\n            if (time_start[i+1]- time_end[i])>1000.0:\n                start_t = time_end[i]+800.0\n                end_t = time_end[i]+880.0\n                vid = lp_video(video,start_t, end_t)\n                if vid is None:\n                    break\n                else:\n                    \n                    for m in torch.unbind(vid, dim=3):\n#                         if neg <= 1300:\n#                             train_classes.append(60)\n#                             train_inputs.append(m)\n                        neg += 1\n#                     neg += vid.size(3)\n        print(neg)\n            \n        \n            \n    print(\"count of neg class:\",neg)\n    signds = SignDataset(train_inputs, train_classes)\n    trainlen = int(len(signds)*0.8)\n    torch.manual_seed(0)\n    \n    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n  \n    return trainloader, valloader\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T13:57:53.758995Z","iopub.execute_input":"2022-06-12T13:57:53.759267Z","iopub.status.idle":"2022-06-12T13:57:53.778012Z","shell.execute_reply.started":"2022-06-12T13:57:53.759237Z","shell.execute_reply":"2022-06-12T13:57:53.777059Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"def load_testloader(batch_size,start,end):\n\n    time_start = []\n    time_end = []\n    train_inputs =[] #x\n    train_classes = [] #y\n    \n\n    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n        data = pickle.load(f)\n\n\n# keys are files so iterate only limited files due to memory limitations.\n    for key in list(data.keys())[start:end]:\n        filename = key\n        print(\"file\",filename)\n        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n    # file functions\n\n        for x in data[key]:\n            img_cls = x[0]\n            time_start.append(x[1])\n            time_end.append(x[2])\n            \n            \n            # start_time = x[1]\n            # end_time = x[2]\n            vid = lp_video(video,x[1], x[2])\n\n            for m in torch.unbind(vid, dim=3):\n                train_classes.append(img_cls)\n                train_inputs.append(m)\n\n\n            #some negative classes too\n        for i in range(0,len(time_start)//2):\n            if (time_start[i+1]- time_end[i])>1000.0:\n                start_t = time_end[i]+700.0\n                end_t = time_end[i]+780.0\n                vid = lp_video(video,start_t, end_t)\n                if vid is None:\n                    break\n                else:\n                    for m in torch.unbind(vid, dim=3):\n                        train_classes.append(60)\n                        train_inputs.append(m)\n\n            \n        \n            \n\n    signds = SignDataset(train_inputs, train_classes)\n    \n    \n\n    testloader = DataLoader(signds, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    return testloader","metadata":{"execution":{"iopub.status.busy":"2022-06-12T12:48:33.168400Z","iopub.execute_input":"2022-06-12T12:48:33.169126Z","iopub.status.idle":"2022-06-12T12:48:33.182027Z","shell.execute_reply.started":"2022-06-12T12:48:33.169084Z","shell.execute_reply":"2022-06-12T12:48:33.181078Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def load_vid(batch_size,start,end):\n\n\n    train_inputs =[] #x\n\n    video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Val_Set/VALIDATION/MSSL_VAL_SET_VIDEOS/p01_n131.mp4\")\n    # file functions\n\n    vid = lp_video(video,start, end)\n\n    for m in torch.unbind(vid, dim=3):\n        train_inputs.append(m)\n\n    \n\n    testloader = DataLoader(train_inputs, batch_size=batch_size, shuffle=True, num_workers=2)\n\n    return testloader","metadata":{"execution":{"iopub.status.busy":"2022-06-12T12:48:33.183251Z","iopub.execute_input":"2022-06-12T12:48:33.183628Z","iopub.status.idle":"2022-06-12T12:48:33.193745Z","shell.execute_reply.started":"2022-06-12T12:48:33.183588Z","shell.execute_reply":"2022-06-12T12:48:33.192780Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCreates a EfficientNetV2 Model as defined in:\nMingxing Tan, Quoc V. Le. (2021). \nEfficientNetV2: Smaller Models and Faster Training\narXiv preprint arXiv:2104.00298.\nimport from https://github.com/d-li14/mobilenetv2.pytorch\n\"\"\"\n\n\n\n__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\n# SiLU (Swish) activation function\nif hasattr(nn, 'SiLU'):\n    SiLU = nn.SiLU\nelse:\n    # For compatibility with old PyTorch versions\n    class SiLU(nn.Module):\n        def forward(self, x):\n            return x * torch.sigmoid(x)\n\n \nclass SELayer(nn.Module):\n    def __init__(self, inp, oup, reduction=4):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n                SiLU(),\n                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\ndef conv_3x3_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        SiLU()\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        SiLU()\n    )\n\n\nclass MBConv(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n        super(MBConv, self).__init__()\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.identity = stride == 1 and inp == oup\n        if use_se:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                SiLU(),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                SiLU(),\n                SELayer(inp, hidden_dim),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # fused\n                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                SiLU(),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n\n    def forward(self, x):\n        if self.identity:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass EffNetV2(nn.Module):\n    def __init__(self, cfgs, num_classes=61, width_mult=1.):\n        super(EffNetV2, self).__init__()\n        self.cfgs = cfgs\n\n        # building first layer\n        input_channel = _make_divisible(24 * width_mult, 8)\n        layers = [conv_3x3_bn(1, input_channel, 2)]\n        # building inverted residual blocks\n        block = MBConv\n        for t, c, n, s, use_se in self.cfgs:\n            output_channel = _make_divisible(c * width_mult, 8)\n            for i in range(n):\n                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n                input_channel = output_channel\n        self.features = nn.Sequential(*layers)\n        # building last several layers\n        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n        self.conv = conv_1x1_bn(input_channel, output_channel)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Linear(output_channel, num_classes)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.conv(x)\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.001)\n                m.bias.data.zero_()\n\n\ndef effnetv2_s(**kwargs):\n    \"\"\"\n    Constructs a EfficientNetV2-S model\n    \"\"\"\n    cfgs = [\n        # t, c, n, s, SE\n        [1,  24,  2, 1, 0],\n        [4,  48,  4, 2, 0],\n        [4,  64,  4, 2, 0],\n        [4, 128,  6, 2, 1],\n        [6, 160,  9, 1, 1],\n        [6, 256, 15, 2, 1],\n    ]\n    return EffNetV2(cfgs, **kwargs)\n\n\ndef effnetv2_m(**kwargs):\n    \"\"\"\n    Constructs a EfficientNetV2-M model\n    \"\"\"\n    cfgs = [\n        # t, c, n, s, SE\n        [1,  24,  3, 1, 0],\n        [4,  48,  5, 2, 0],\n        [4,  80,  5, 2, 0],\n        [4, 160,  7, 2, 1],\n        [6, 176, 14, 1, 1],\n        [6, 304, 18, 2, 1],\n        [6, 512,  5, 1, 1],\n    ]\n    return EffNetV2(cfgs, **kwargs)\n\n\ndef effnetv2_l(**kwargs):\n    \"\"\"\n    Constructs a EfficientNetV2-L model\n    \"\"\"\n    cfgs = [\n        # t, c, n, s, SE\n        [1,  32,  4, 1, 0],\n        [4,  64,  7, 2, 0],\n        [4,  96,  7, 2, 0],\n        [4, 192, 10, 2, 1],\n        [6, 224, 19, 1, 1],\n        [6, 384, 25, 2, 1],\n        [6, 640,  7, 1, 1],\n    ]\n    return EffNetV2(cfgs, **kwargs)\n\n\ndef effnetv2_xl(**kwargs):\n    \"\"\"\n    Constructs a EfficientNetV2-XL model\n    \"\"\"\n    cfgs = [\n        # t, c, n, s, SE\n        [1,  32,  4, 1, 0],\n        [4,  64,  8, 2, 0],\n        [4,  96,  8, 2, 0],\n        [4, 192, 16, 2, 1],\n        [6, 256, 24, 1, 1],\n        [6, 512, 32, 2, 1],\n        [6, 640,  8, 1, 1],\n    ]\n    return EffNetV2(cfgs, **kwargs)","metadata":{"papermill":{"duration":0.07329,"end_time":"2022-05-26T12:44:18.850959","exception":false,"start_time":"2022-05-26T12:44:18.777669","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T12:48:33.195359Z","iopub.execute_input":"2022-06-12T12:48:33.195818Z","iopub.status.idle":"2022-06-12T12:48:33.242207Z","shell.execute_reply.started":"2022-06-12T12:48:33.195761Z","shell.execute_reply":"2022-06-12T12:48:33.241474Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef evluate(model, device, loader, mode=\"Validate\"):\n    \n    \n    model.eval()\n\n    correct = 0\n    num_samples = 0\n    \n        \n    for batch_idx, data in enumerate(loader):\n        with torch.no_grad():    \n            data = data.to(device)\n            \n\n            output = model(data.half())  \n            \n            _,pred = output.max(1)# get the index of the max log-probability\n            print(pred)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-12T12:48:33.243610Z","iopub.execute_input":"2022-06-12T12:48:33.243882Z","iopub.status.idle":"2022-06-12T12:48:33.252696Z","shell.execute_reply.started":"2022-06-12T12:48:33.243844Z","shell.execute_reply":"2022-06-12T12:48:33.251879Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\ndef test(model, device, loader, mode=\"Validate\"):\n    \n    \n    model.eval()\n\n    correct = 0\n    num_samples = 0\n    \n        \n    for batch_idx, (data, target) in enumerate(loader):\n        with torch.no_grad():    \n            data = data.to(device)\n            target = (target).to(device)\n\n            output = model(data.half())  \n            \n            _,pred = output.max(1)# get the index of the max log-probability\n            num_samples += target.size(0)\n            correct += (pred==target).sum().item()\n            #print(\"correct\", correct)\n\n    acc = 100.0 * correct / num_samples\n    \n    print('{} Accuracy: {}/{} ({:.0f}%)'.format(\n                mode,correct, num_samples,\n                100. * correct / num_samples, acc))","metadata":{"execution":{"iopub.status.busy":"2022-06-12T12:48:33.254004Z","iopub.execute_input":"2022-06-12T12:48:33.254593Z","iopub.status.idle":"2022-06-12T12:48:33.263575Z","shell.execute_reply.started":"2022-06-12T12:48:33.254533Z","shell.execute_reply":"2022-06-12T12:48:33.262745Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n    \n\n\ndef train(model, device, train_loader,validloader,scheduler, optimizer,criterion,epochs,lr):\n    print(\"Train start\")\n    breakout = False\n    model.half()\n    model.cuda()\n\n    \n    model.train()\n\n    optimizer.param_groups[0]['lr'] = lr\n    \n    for epoch in range(epochs):\n        correct = 0\n        num_samples = 0\n        train_loss = 0\n        \n        for batch_idx, (data, target) in enumerate(train_loader):\n            \n            data = data.to(device)\n            target = target.to(device)\n\n            optimizer.zero_grad()\n            output = model(data.half())\n            \n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            \n            train_loss += loss.item()\n            \n            \n            _,pred = output.max(1)# get the index of the max log-probability\n            num_samples += target.size(0)\n            correct += (pred==target).sum().item()\n            \n        if optimizer.param_groups[0]['lr'] != 1e-3:    \n            scheduler.step()\n        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n        \n        \n        train_loss /= num_samples\n        \n        if (100 * correct / num_samples) >= 95:\n            \n            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n            \n            test(model,device,validloader, mode = \"Validating\")\n#             test(model, device, testloader, mode = \"Test before Training on validation set\")\n           # model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n#             test(model, device, testloader, mode = \"Test after training on validation set\")\n            \n            \n            \n            torch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer34.pt\")\n            \n            breakout = True\n            print(\"breakout\")\n            return model, optimizer\n            \n            \n            \n       \n        \n\n        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n                epoch+1, correct, num_samples,\n                100. * correct / num_samples, train_loss))\n        if (((epoch+1)%5) == 0) :\n            test(model,device,validloader)\n            model.train()\n            \n    if breakout:\n            print(\"breakout\")\n            return\n    \n    \n    \n    #model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n    test(model, device, validloader, mode= \"test set \")\n    \n    \n        \n    torch.save({\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer34.pt\")\n    return model, optimizer\n    \n    \n        \n","metadata":{"papermill":{"duration":0.053612,"end_time":"2022-05-26T12:44:18.933025","exception":false,"start_time":"2022-05-26T12:44:18.879413","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2022-06-12T12:48:33.266687Z","iopub.execute_input":"2022-06-12T12:48:33.267115Z","iopub.status.idle":"2022-06-12T12:48:33.281982Z","shell.execute_reply.started":"2022-06-12T12:48:33.267047Z","shell.execute_reply":"2022-06-12T12:48:33.281245Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"n_classes = 61\n\n\nmodel = nn.Sequential(OrderedDict([\n    ('frontend',effnetv2_m())\n]))\n\n\n\n\n\n# specify loss function (categorical cross-entropy)\n\n# specify optimizer and learning rate\noptimizer = optim.SGD(\n    [\n        \n        {\"params\": model.frontend.parameters(), \"lr\": 1e-2},\n  ],\n  momentum = 0.9\n)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nsamples_per_cls = [166.0, 370.0, 63.0, 232.0, 905.0, 102.0, 537.0, 993.0, 133.0, 154.0, 286.0, 435.0, 452.0, 779.0, 354.0, 136.0, 232.0, 2154.0, 374.0, 787.0, 337.0, 268.0, 954.0, 167.0, 543.0, 324.0, 669.0, 333.0, 211.0, 433.0, 140.0, 665.0, 107.0, 305.0, 162.0, 61.0, 1229.0, 958.0, 897.0, 543.0, 625.0, 334.0, 776.0, 111.0, 412.0, 583.0, 255.0, 103.0, 108.0, 201.0, 384.0, 632.0, 13.0, 322.0, 664.0, 435.0, 194.0, 322.0, 113.0, 462.0,3000.0]\nno_of_classes=61\nbeta=0.9999\neffective_num = 1.0 - np.power(beta, samples_per_cls)\nweights = (1.0 - beta) / np.array(effective_num)\nweights = weights / np.sum(weights) * no_of_classes\n    \n\nweights = torch.FloatTensor(weights).cuda()\n\ncriterion = nn.CrossEntropyLoss(weight = weights.half())\n\n# state = torch.load(\"../input/fmosel/model_optimizer34.pt\")\n# model.load_state_dict(state['model_state_dict'])\n# model.half()\n# model.cuda()\n# optimizer.load_state_dict(state['optimizer_state_dict'])","metadata":{"papermill":{"duration":0.355647,"end_time":"2022-05-26T12:44:19.31525","exception":false,"start_time":"2022-05-26T12:44:18.959603","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T12:48:33.285333Z","iopub.execute_input":"2022-06-12T12:48:33.285774Z","iopub.status.idle":"2022-06-12T12:48:37.251902Z","shell.execute_reply.started":"2022-06-12T12:48:33.285735Z","shell.execute_reply":"2022-06-12T12:48:37.250896Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))","metadata":{"papermill":{"duration":0.03812,"end_time":"2022-05-26T12:47:17.570051","exception":false,"start_time":"2022-05-26T12:47:17.531931","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T12:48:37.256231Z","iopub.execute_input":"2022-06-12T12:48:37.256578Z","iopub.status.idle":"2022-06-12T12:48:37.264357Z","shell.execute_reply.started":"2022-06-12T12:48:37.256524Z","shell.execute_reply":"2022-06-12T12:48:37.263602Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#summary(model, (1, 512, 640), device='cpu')#1334+986","metadata":{"papermill":{"duration":0.063852,"end_time":"2022-05-26T14:13:07.978109","exception":false,"start_time":"2022-05-26T14:13:07.914257","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-06-12T12:48:37.269022Z","iopub.execute_input":"2022-06-12T12:48:37.271363Z","iopub.status.idle":"2022-06-12T12:48:37.276371Z","shell.execute_reply.started":"2022-06-12T12:48:37.271320Z","shell.execute_reply":"2022-06-12T12:48:37.275690Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"trainloader, valloader = load_dataloaders(batch_size=16,start=60, end=108)#73 not included ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T13:58:01.271616Z","iopub.execute_input":"2022-06-12T13:58:01.272358Z","iopub.status.idle":"2022-06-12T14:00:07.310076Z","shell.execute_reply.started":"2022-06-12T13:58:01.272321Z","shell.execute_reply":"2022-06-12T14:00:07.307771Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"file p02_n098\n19\nfile p06_n013\n74\nfile p05_n006\n94\nfile p01_n063\n116\nfile p02_n097\n141\nfile p05_n065\n167\nfile p01_n108\n175\nfile p06_n018\n191\nfile p05_n009\n199\nfile p05_n071\n207\nfile p04_n091\n225\nfile p05_n075\n240\nfile p01_n012\n261\nfile p05_n076\n285\nfile p04_n005\n335\nfile p04_n046\n355\nfile p01_n045\n367\nfile p05_n041\n384\nfile p01_n008\n401\nfile p02_n101\n420\nfile p01_n085\n438\nfile p05_n064\n481\nfile p05_n017\n501\nfile p01_n028\n517\nfile p02_n079\n557\nfile p01_n010\n579\nfile p01_n082\n591\nfile p01_n107\n620\nfile p02_n088\n647\nfile p01_n087\n665\nfile p05_n086\n681\nfile p06_n024\n693\nfile p05_n070\n731\nfile p04_n016\n746\nfile p04_n068\n780\nfile p04_n026\n790\nfile p06_n039\n806\nfile p06_n014\n814\nfile p05_n059\n836\nfile p02_n051\n862\nfile p06_n056\n888\nfile p04_n080\n902\nfile p05_n062\n916\nfile p04_n034\n928\nfile p04_n049\n942\nfile p06_n033\n946\nfile p02_n100\n952\nfile p04_n030\n986\ncount of neg class: 986\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/4288256812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m108\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#73 not included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_33/474302568.py\u001b[0m in \u001b[0;36mload_dataloaders\u001b[0;34m(batch_size, start, end)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrainlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrainlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mvalloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_test_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                     \u001b[0;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0;32m--> 103\u001b[0;31m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"],"ename":"ValueError","evalue":"num_samples should be a positive integer value, but got num_samples=0","output_type":"error"}]},{"cell_type":"code","source":"# trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=6)#73 not included \n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n# model, optimizer = train(model,device,trainloader,valloader,scheduler,optimizer,criterion,30,1e-2)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T12:55:06.184238Z","iopub.status.idle":"2022-06-12T12:55:06.185323Z","shell.execute_reply.started":"2022-06-12T12:55:06.185055Z","shell.execute_reply":"2022-06-12T12:55:06.185085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainloader = load_testloader(batch_size=16,start=108, end=112)#73 not included \n# test(model,device,trainloader)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T12:55:06.186803Z","iopub.status.idle":"2022-06-12T12:55:06.187359Z","shell.execute_reply.started":"2022-06-12T12:55:06.187079Z","shell.execute_reply":"2022-06-12T12:55:06.187107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# trainloader= load_vid(batch_size=16,start=1000, end=3000)#73 not included \n# evluate(model,device,trainloader)","metadata":{"execution":{"iopub.status.busy":"2022-06-12T12:55:06.188986Z","iopub.status.idle":"2022-06-12T12:55:06.189679Z","shell.execute_reply.started":"2022-06-12T12:55:06.189396Z","shell.execute_reply":"2022-06-12T12:55:06.189422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}