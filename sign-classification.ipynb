{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04136d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:14:52.128651Z",
     "iopub.status.busy": "2022-06-22T03:14:52.127637Z",
     "iopub.status.idle": "2022-06-22T03:15:27.911350Z",
     "shell.execute_reply": "2022-06-22T03:15:27.910247Z"
    },
    "papermill": {
     "duration": 35.818726,
     "end_time": "2022-06-22T03:15:27.913902",
     "exception": false,
     "start_time": "2022-06-22T03:14:52.095176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideo\r\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m948.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting fvcore\r\n",
      "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting av\r\n",
      "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting parameterized\r\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting iopath\r\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\r\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.63.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.1.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.0.1)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.4.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->pytorchvideo) (5.1.1)\r\n",
      "Building wheels for collected packages: pytorchvideo, fvcore\r\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188714 sha256=724d97347e62c1f0cf2f909a819b1d6d76a286883f2e08da036d4c96bb40e9d0\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/51/05/053b29bac2400cbbae2fb7cfc41afd280d627bca7c9363ca80\r\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=646185edad6d1565aeee343f9959348833f9e705790cb8d96583d7a6851108e7\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\r\n",
      "Successfully built pytorchvideo fvcore\r\n",
      "Installing collected packages: parameterized, av, iopath, fvcore, pytorchvideo\r\n",
      "Successfully installed av-9.2.0 fvcore-0.1.5.post20220512 iopath-0.1.9 parameterized-0.8.1 pytorchvideo-0.1.5\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting imblearn\r\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\r\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (from imblearn) (0.9.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.0.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.21.6)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (3.1.0)\r\n",
      "Installing collected packages: imblearn\r\n",
      "Successfully installed imblearn-0.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo\n",
    "!pip install torchsummary\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "236350bb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:27.992579Z",
     "iopub.status.busy": "2022-06-22T03:15:27.991791Z",
     "iopub.status.idle": "2022-06-22T03:15:31.102220Z",
     "shell.execute_reply": "2022-06-22T03:15:31.101475Z"
    },
    "papermill": {
     "duration": 3.151248,
     "end_time": "2022-06-22T03:15:31.104297",
     "exception": false,
     "start_time": "2022-06-22T03:15:27.953049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ") \n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize\n",
    "#from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.y)\n",
    "\n",
    "        # length = 0\n",
    "        # with open(self.file, 'rb') as f:\n",
    "        #     data = pickle.load(f)\n",
    "\n",
    "        # for x in data:\n",
    "        #     length += len(data[x])\n",
    "        # return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03a92a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.184263Z",
     "iopub.status.busy": "2022-06-22T03:15:31.183882Z",
     "iopub.status.idle": "2022-06-22T03:15:31.194395Z",
     "shell.execute_reply": "2022-06-22T03:15:31.193748Z"
    },
    "papermill": {
     "duration": 0.052679,
     "end_time": "2022-06-22T03:15:31.196156",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.143477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 720\n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_CHANNEL = 1\n",
    "NUM_FRAMES = 25\n",
    "NUM_CLASSES = 60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(x):\n",
    "    \n",
    "    transform =  ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose(\n",
    "            [\n",
    "                Lambda(lambda x: x/255.0),\n",
    "                Normalize(([0.2933, 0.4799, 0.6723]), ([0.1576, 0.1674, 0.2477])),\n",
    "                Grayscale(num_output_channels=1),\n",
    "                CenterCrop([720,900]),\n",
    "                Resize([512,512]),\n",
    "                Lambda(lambda x: x.permute(1,2,3,0)),#(channel, frames(depth), height, width)\n",
    "#                 Lambda(lambda x: print(x.shape))\n",
    "\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return transform(x)\n",
    "\n",
    "def lp_video(video,start_time, end_time):\n",
    "    global means,stds\n",
    "    global nst\n",
    "    \n",
    "    video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n",
    "            #print(video_data[\"video\"].shape)\n",
    " \n",
    "    if video_data[\"video\"] is None:\n",
    "        return None        # or pass\n",
    "    else:\n",
    "    \n",
    "        video_data[\"video\"] = video_data[\"video\"].permute(1,0,2,3)\n",
    "        \n",
    "        video_data = transform_data( video_data)\n",
    "\n",
    "        return video_data[\"video\"]\n",
    "\n",
    "# def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "#     train_inputs =[] #x\n",
    "#     train_classes = [] #y\n",
    "#     global neg\n",
    "\n",
    "#     with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "#         data = pickle.load(f)\n",
    "\n",
    "\n",
    "# # keys are files so iterate only limited files due to memory limitations.\n",
    "#     for key in list(data.keys())[start:end]:\n",
    "        \n",
    "#         filename = key\n",
    "#         \n",
    "#         video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "#     # file functions\n",
    "        \n",
    "#         for x in data[key]:\n",
    "#             if x[0] in range(0,5):\n",
    "#                 img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "#                 # start_time = x[1]\n",
    "#                 # end_time = x[2]\n",
    "#                 vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "#                 for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "#                     train_classes.append(img_cls)\n",
    "#                     train_inputs.append(m.view(-1).numpy())\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "#  #weights =torch.tensor([166.0, 370.0, 63.0, 232.0, 905.0, 102.0, 537.0, 993.0, 133.0, 9:154.0, 286.0, 435.0, 452.0, 779.0, 354.0, 136.0, 232.0, 2154.0, 374.0, 787.0, 337.0, 268.0, 954.0, 167.0, 543.0, 324.0, 669.0, 333.0, 211.0, 433.0, 140.0, 665.0, 107.0, 305.0, 162.0, 61.0, 1229.0, 958.0, 897.0, 543.0, 625.0, 334.0, 776.0, 111.0, 412.0, 583.0, 255.0, 103.0, 108.0, 201.0, 384.0, 632.0, 13.0, 322.0, 664.0, 435.0, 194.0, 322.0, 113.0, 462.0,2330.0])\n",
    "\n",
    "# #if under 100 ignore\n",
    "# #count round of train\n",
    "# #     sampling_stat = {0:3}\n",
    "#     sampling_stat = {0:500,1:500,2:70,3:500}  \n",
    "#     over = SMOTE(sampling_strategy=sampling_stat,k_neighbors=60)\n",
    "#     x_over, y_over = over.fit_resample(np.array(train_inputs), np.array(train_classes))\n",
    "#     print(\"Class:{} Count:{}\".format(*np.unique(y_over, return_counts=True)))  \n",
    " \n",
    "#     x_over = torch.from_numpy(x_over).view(-1,1,512,512)\n",
    "    \n",
    "   \n",
    "#     print(x_over[0].shape)\n",
    "#     print(x_over.shape)\n",
    "  \n",
    "            \n",
    "#     signds = SignDataset(x_over, y_over)\n",
    "#     trainlen = int(len(signds)*0.8)\n",
    "#     torch.manual_seed(0)\n",
    "    \n",
    "#     train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "#     trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#     valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "#     return trainloader, valloader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17399819",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.272375Z",
     "iopub.status.busy": "2022-06-22T03:15:31.271834Z",
     "iopub.status.idle": "2022-06-22T03:15:31.279341Z",
     "shell.execute_reply": "2022-06-22T03:15:31.278717Z"
    },
    "papermill": {
     "duration": 0.047074,
     "end_time": "2022-06-22T03:15:31.280990",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.233916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_testloader(batch_size,start,end):\n",
    "\n",
    "    time_start = []\n",
    "    time_end = []\n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    \n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        filename = key\n",
    "        \n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "\n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "            for m in torch.unbind(vid, dim=3):\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    \n",
    "    \n",
    "\n",
    "    testloader = DataLoader(signds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14a5d7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.358073Z",
     "iopub.status.busy": "2022-06-22T03:15:31.357579Z",
     "iopub.status.idle": "2022-06-22T03:15:31.363064Z",
     "shell.execute_reply": "2022-06-22T03:15:31.362429Z"
    },
    "papermill": {
     "duration": 0.045992,
     "end_time": "2022-06-22T03:15:31.364669",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.318677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_vid(batch_size,start,end):\n",
    "\n",
    "\n",
    "    train_inputs =[] #x\n",
    "\n",
    "    video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Val_Set/VALIDATION/MSSL_VAL_SET_VIDEOS/p01_n131.mp4\")\n",
    "    # file functions\n",
    "\n",
    "    vid = lp_video(video,start, end)\n",
    "\n",
    "    for m in torch.unbind(vid, dim=3):\n",
    "        train_inputs.append(m)\n",
    "\n",
    "    \n",
    "\n",
    "    testloader = DataLoader(train_inputs, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf58d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.444108Z",
     "iopub.status.busy": "2022-06-22T03:15:31.443748Z",
     "iopub.status.idle": "2022-06-22T03:15:31.499416Z",
     "shell.execute_reply": "2022-06-22T03:15:31.498702Z"
    },
    "papermill": {
     "duration": 0.099913,
     "end_time": "2022-06-22T03:15:31.501978",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.402065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a EfficientNetV2 Model as defined in:\n",
    "Mingxing Tan, Quoc V. Le. (2021). \n",
    "EfficientNetV2: Smaller Models and Faster Training\n",
    "arXiv preprint arXiv:2104.00298.\n",
    "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "# SiLU (Swish) activation function\n",
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    " \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
    "                SiLU(),\n",
    "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EffNetV2(nn.Module):\n",
    "    def __init__(self, cfgs, num_classes=60, width_mult=1.):\n",
    "        super(EffNetV2, self).__init__()\n",
    "        self.cfgs = cfgs\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(24 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(1, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = MBConv\n",
    "        for t, c, n, s, use_se in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            for i in range(n):\n",
    "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
    "                input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
    "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(output_channel, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.001)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def effnetv2_s(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-S model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  24,  2, 1, 0],\n",
    "        [4,  48,  4, 2, 0],\n",
    "        [4,  64,  4, 2, 0],\n",
    "        [4, 128,  6, 2, 1],\n",
    "        [6, 160,  9, 1, 1],\n",
    "        [6, 256, 15, 2, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_m(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-M model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  24,  3, 1, 0],\n",
    "        [4,  48,  5, 2, 0],\n",
    "        [4,  80,  5, 2, 0],\n",
    "        [4, 160,  7, 2, 1],\n",
    "        [6, 176, 14, 1, 1],\n",
    "        [6, 304, 18, 2, 1],\n",
    "        [6, 512,  5, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_l(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-L model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  7, 2, 0],\n",
    "        [4,  96,  7, 2, 0],\n",
    "        [4, 192, 10, 2, 1],\n",
    "        [6, 224, 19, 1, 1],\n",
    "        [6, 384, 25, 2, 1],\n",
    "        [6, 640,  7, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_xl(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-XL model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  8, 2, 0],\n",
    "        [4,  96,  8, 2, 0],\n",
    "        [4, 192, 16, 2, 1],\n",
    "        [6, 256, 24, 1, 1],\n",
    "        [6, 512, 32, 2, 1],\n",
    "        [6, 640,  8, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5ccd1",
   "metadata": {
    "papermill": {
     "duration": 0.038628,
     "end_time": "2022-06-22T03:15:31.593374",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.554746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f8bcd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.670188Z",
     "iopub.status.busy": "2022-06-22T03:15:31.669984Z",
     "iopub.status.idle": "2022-06-22T03:15:31.674973Z",
     "shell.execute_reply": "2022-06-22T03:15:31.674323Z"
    },
    "papermill": {
     "duration": 0.045402,
     "end_time": "2022-06-22T03:15:31.676649",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.631247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evluate(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            \n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            print(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f617ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.763710Z",
     "iopub.status.busy": "2022-06-22T03:15:31.763483Z",
     "iopub.status.idle": "2022-06-22T03:15:31.770099Z",
     "shell.execute_reply": "2022-06-22T03:15:31.769390Z"
    },
    "papermill": {
     "duration": 0.049386,
     "end_time": "2022-06-22T03:15:31.771764",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.722378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, device, loader,criterion, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            target = (target).to(device)\n",
    "\n",
    "            output = model(data.half())  \n",
    "            test_loss += criterion(output, target)\n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            #print(\"correct\", correct)\n",
    "    test_loss /= num_samples\n",
    "    print(' Val Accuracy: {}/{} ({:.0f}%) Val Loss: {:.6f}'.format(correct, num_samples,100. * correct / num_samples, test_loss))\n",
    "    return  test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf75b445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.848079Z",
     "iopub.status.busy": "2022-06-22T03:15:31.847882Z",
     "iopub.status.idle": "2022-06-22T03:15:31.860948Z",
     "shell.execute_reply": "2022-06-22T03:15:31.860319Z"
    },
    "papermill": {
     "duration": 0.05273,
     "end_time": "2022-06-22T03:15:31.862519",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.809789",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader,validloader,scheduler, optimizer,criterion,epochs,lr):\n",
    "    print(\"Train start\")\n",
    "    breakout = False\n",
    "    \n",
    "    last_loss = 100\n",
    "    patience = 3\n",
    "    triggertimes = 0\n",
    "    \n",
    "    \n",
    "    model.half()\n",
    "    model.cuda()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "        if optimizer.param_groups[0]['lr'] < 1e-5:    \n",
    "            scheduler.step()\n",
    "        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 93:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            \n",
    "            current_loss = test(model,device,validloader,criterion, mode = \"Validating\")\n",
    "            \n",
    "        \n",
    "            print(\"breakout\")\n",
    "            return model, optimizer\n",
    "            \n",
    "            \n",
    "\n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        \n",
    "        current_loss = test(model,device,validloader,criterion)\n",
    "        if current_loss > last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger Times for early stopping:', trigger_times)\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model, optimizer\n",
    "\n",
    "        else:\n",
    "            print('trigger times: 0')\n",
    "            trigger_times = 0\n",
    "\n",
    "        last_loss = current_loss\n",
    "    \n",
    "    \n",
    "        \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b324705a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:31.938708Z",
     "iopub.status.busy": "2022-06-22T03:15:31.938232Z",
     "iopub.status.idle": "2022-06-22T03:15:38.704038Z",
     "shell.execute_reply": "2022-06-22T03:15:38.703311Z"
    },
    "papermill": {
     "duration": 6.806279,
     "end_time": "2022-06-22T03:15:38.706097",
     "exception": false,
     "start_time": "2022-06-22T03:15:31.899818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 60\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('frontend',effnetv2_m())\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "\n",
    "# specify optimizer and learning rate\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        \n",
    "        {\"params\": model.frontend.parameters(), \"lr\": 1e-2},\n",
    "  ],\n",
    "  momentum = 0.9\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "state = torch.load(\"../input/fmosel/model_optimizer313.pt\")\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "model.half()\n",
    "model.cuda()\n",
    "optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "918a5a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:38.785243Z",
     "iopub.status.busy": "2022-06-22T03:15:38.785019Z",
     "iopub.status.idle": "2022-06-22T03:15:38.788531Z",
     "shell.execute_reply": "2022-06-22T03:15:38.787768Z"
    },
    "papermill": {
     "duration": 0.045935,
     "end_time": "2022-06-22T03:15:38.790690",
     "exception": false,
     "start_time": "2022-06-22T03:15:38.744755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5e8c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:38.867491Z",
     "iopub.status.busy": "2022-06-22T03:15:38.867295Z",
     "iopub.status.idle": "2022-06-22T03:15:38.870414Z",
     "shell.execute_reply": "2022-06-22T03:15:38.869667Z"
    },
    "papermill": {
     "duration": 0.043544,
     "end_time": "2022-06-22T03:15:38.872201",
     "exception": false,
     "start_time": "2022-06-22T03:15:38.828657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#summary(model, (1, 512, 640), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6210d1ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:38.949515Z",
     "iopub.status.busy": "2022-06-22T03:15:38.949300Z",
     "iopub.status.idle": "2022-06-22T03:15:38.958888Z",
     "shell.execute_reply": "2022-06-22T03:15:38.958206Z"
    },
    "papermill": {
     "duration": 0.050474,
     "end_time": "2022-06-22T03:15:38.960518",
     "exception": false,
     "start_time": "2022-06-22T03:15:38.910044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    global neg\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        \n",
    "        filename = key\n",
    "        \n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "        \n",
    "        for x in data[key]:\n",
    "            if x[0] in range(50,54):\n",
    "                img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "                # start_time = x[1]\n",
    "                # end_time = x[2]\n",
    "                vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "                for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "                    train_classes.append(img_cls)\n",
    "                    train_inputs.append(m)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "  \n",
    "            \n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4c6dacc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:15:39.038308Z",
     "iopub.status.busy": "2022-06-22T03:15:39.038110Z",
     "iopub.status.idle": "2022-06-22T03:17:44.912579Z",
     "shell.execute_reply": "2022-06-22T03:17:44.911732Z"
    },
    "papermill": {
     "duration": 125.916538,
     "end_time": "2022-06-22T03:17:44.914993",
     "exception": false,
     "start_time": "2022-06-22T03:15:38.998455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val Accuracy: 0/1084 (0%) Val Loss: 0.374268\n",
      " Val Accuracy: 0/271 (0%) Val Loss: 0.390381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3904, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=112)#73 not included \n",
    "test(model, device, trainloader,criterion, mode=\"test\")\n",
    "test(model, device, valloader,criterion, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58aa8d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:17:44.996079Z",
     "iopub.status.busy": "2022-06-22T03:17:44.995498Z",
     "iopub.status.idle": "2022-06-22T03:17:45.006405Z",
     "shell.execute_reply": "2022-06-22T03:17:45.005743Z"
    },
    "papermill": {
     "duration": 0.05418,
     "end_time": "2022-06-22T03:17:45.008093",
     "exception": false,
     "start_time": "2022-06-22T03:17:44.953913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    global neg\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        \n",
    "        filename = key\n",
    "        \n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "        \n",
    "        for x in data[key]:\n",
    "            if x[0] in range(0,4):\n",
    "                img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "                # start_time = x[1]\n",
    "                # end_time = x[2]\n",
    "                vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "                for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "                    train_classes.append(img_cls)\n",
    "                    train_inputs.append(m)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "  \n",
    "            \n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f067c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:17:45.088198Z",
     "iopub.status.busy": "2022-06-22T03:17:45.087928Z",
     "iopub.status.idle": "2022-06-22T03:18:56.488795Z",
     "shell.execute_reply": "2022-06-22T03:18:56.487040Z"
    },
    "papermill": {
     "duration": 71.454343,
     "end_time": "2022-06-22T03:18:56.501344",
     "exception": false,
     "start_time": "2022-06-22T03:17:45.047001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val Accuracy: 0/665 (0%) Val Loss: 0.507324\n",
      " Val Accuracy: 0/167 (0%) Val Loss: 0.519043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5190, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=112)#73 not included \n",
    "test(model, device, trainloader,criterion, mode=\"test\")\n",
    "test(model, device, valloader,criterion, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1daebc33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:18:56.785194Z",
     "iopub.status.busy": "2022-06-22T03:18:56.784849Z",
     "iopub.status.idle": "2022-06-22T03:18:56.800877Z",
     "shell.execute_reply": "2022-06-22T03:18:56.800169Z"
    },
    "papermill": {
     "duration": 0.135567,
     "end_time": "2022-06-22T03:18:56.803331",
     "exception": false,
     "start_time": "2022-06-22T03:18:56.667764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    global neg\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        \n",
    "        filename = key\n",
    "        \n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "        \n",
    "        for x in data[key]:\n",
    "            if x[0] in range(20,24):\n",
    "                img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "                # start_time = x[1]\n",
    "                # end_time = x[2]\n",
    "                vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "                for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "                    train_classes.append(img_cls)\n",
    "                    train_inputs.append(m)\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "  \n",
    "            \n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b160e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:18:56.940181Z",
     "iopub.status.busy": "2022-06-22T03:18:56.939876Z",
     "iopub.status.idle": "2022-06-22T03:21:22.276189Z",
     "shell.execute_reply": "2022-06-22T03:21:22.275410Z"
    },
    "papermill": {
     "duration": 145.406202,
     "end_time": "2022-06-22T03:21:22.278140",
     "exception": false,
     "start_time": "2022-06-22T03:18:56.871938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Val Accuracy: 0/1379 (0%) Val Loss: 0.513672\n",
      " Val Accuracy: 0/345 (0%) Val Loss: 0.519043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5190, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=112)#73 not included \n",
    "test(model, device, trainloader,criterion, mode=\"test\")\n",
    "test(model, device, valloader,criterion, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e0b0e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:22.362918Z",
     "iopub.status.busy": "2022-06-22T03:21:22.362695Z",
     "iopub.status.idle": "2022-06-22T03:21:22.366286Z",
     "shell.execute_reply": "2022-06-22T03:21:22.365484Z"
    },
    "papermill": {
     "duration": 0.048501,
     "end_time": "2022-06-22T03:21:22.368342",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.319841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=112)#73 not included \n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "# model, optimizer = train(model,device,trainloader,valloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9fcfada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:22.452353Z",
     "iopub.status.busy": "2022-06-22T03:21:22.452159Z",
     "iopub.status.idle": "2022-06-22T03:21:22.457276Z",
     "shell.execute_reply": "2022-06-22T03:21:22.456570Z"
    },
    "papermill": {
     "duration": 0.049538,
     "end_time": "2022-06-22T03:21:22.458968",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.409430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "#     train_inputs =[] #x\n",
    "#     train_classes = [] #y\n",
    "#     global neg\n",
    "\n",
    "#     with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "#         data = pickle.load(f)\n",
    "\n",
    "\n",
    "# # keys are files so iterate only limited files due to memory limitations.\n",
    "#     for key in list(data.keys())[start:end]:\n",
    "        \n",
    "#         filename = key\n",
    "        \n",
    "#         video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "#     # file functions\n",
    "        \n",
    "#         for x in data[key]:\n",
    "#             if x[0] in range(55,58):\n",
    "#                 img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "#                 # start_time = x[1]\n",
    "#                 # end_time = x[2]\n",
    "#                 vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "#                 for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "#                     train_classes.append(img_cls)\n",
    "#                     train_inputs.append(m.view(-1).numpy())\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "#  #435.0, 194.0, ,57:322.0, 113.0, 462.0\n",
    "\n",
    "# #if under 100 ignore\n",
    "# #count round of train\n",
    "# #     sampling_stat = {0:3}\n",
    "#     sampling_stat = {55:500,56:500, 57:500}  \n",
    "#     over = SMOTE(sampling_strategy=sampling_stat,k_neighbors=60)\n",
    "#     x_over, y_over = over.fit_resample(np.array(train_inputs), np.array(train_classes))\n",
    "#     print(\"Class:{} Count:{}\".format(*np.unique(y_over, return_counts=True)))  \n",
    " \n",
    "#     x_over = torch.from_numpy(x_over).view(-1,1,512,512)\n",
    "    \n",
    "   # print(x_over[0].shape)\n",
    "#     print(x_over.shape)\n",
    "  \n",
    "            \n",
    "#     signds = SignDataset(x_over, y_over)\n",
    "#     trainlen = int(len(signds)*0.8)\n",
    "#     torch.manual_seed(0)\n",
    "    \n",
    "#     train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "#     trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#     valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "#     return trainloader, valloader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ce9ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:22.544085Z",
     "iopub.status.busy": "2022-06-22T03:21:22.543568Z",
     "iopub.status.idle": "2022-06-22T03:21:22.547769Z",
     "shell.execute_reply": "2022-06-22T03:21:22.547100Z"
    },
    "papermill": {
     "duration": 0.048622,
     "end_time": "2022-06-22T03:21:22.549531",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.500909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=112)#73 not included \n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "# model, optimizer = train(model,device,trainloader,valloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "295aee96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:22.634844Z",
     "iopub.status.busy": "2022-06-22T03:21:22.634647Z",
     "iopub.status.idle": "2022-06-22T03:21:22.637592Z",
     "shell.execute_reply": "2022-06-22T03:21:22.636866Z"
    },
    "papermill": {
     "duration": 0.048021,
     "end_time": "2022-06-22T03:21:22.639525",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.591504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer313.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c164044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:22.726285Z",
     "iopub.status.busy": "2022-06-22T03:21:22.726074Z",
     "iopub.status.idle": "2022-06-22T03:21:22.731890Z",
     "shell.execute_reply": "2022-06-22T03:21:22.731109Z"
    },
    "papermill": {
     "duration": 0.052084,
     "end_time": "2022-06-22T03:21:22.734190",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.682106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "#     train_inputs =[] #x\n",
    "#     train_classes = [] #y\n",
    "#     global neg\n",
    "\n",
    "#     with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "#         data = pickle.load(f)\n",
    "\n",
    "\n",
    "# # keys are files so iterate only limited files due to memory limitations.\n",
    "#     for key in list(data.keys())[start:end]:\n",
    "        \n",
    "#         filename = key\n",
    "        \n",
    "#         video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "#     # file functions\n",
    "        \n",
    "#         for x in data[key]:\n",
    "#             if x[0] == 17:\n",
    "#                 img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "#                 # start_time = x[1]\n",
    "#                 # end_time = x[2]\n",
    "#                 vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "#                 for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "#                     train_classes.append(img_cls)\n",
    "#                     train_inputs.append(m.view(-1).numpy())\n",
    "    \n",
    "#     split = len(train_inputs)//3\n",
    "#     train_inputs = train_inputs[:split]\n",
    "#     train_classes = train_classes[:split]\n",
    "#     for key in list(data.keys())[start:end]:\n",
    "        \n",
    "#         filename = key\n",
    "        \n",
    "#         video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "#     # file functions\n",
    "        \n",
    "#         for x in data[key]:\n",
    "#             if x[0] in [59,30]:\n",
    "#                 img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "#                 # start_time = x[1]\n",
    "#                 # end_time = x[2]\n",
    "#                 vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "#                 for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "#                     train_classes.append(img_cls)\n",
    "#                     train_inputs.append(m.view(-1).numpy())\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "#  #435.0, 194.0, ,57:322.0, 113.0, 462.0\n",
    "\n",
    "# #if under 100 ignore\n",
    "# #count round of train\n",
    "# #     sampling_stat = {0:3}\n",
    "#     sampling_stat = {59:500,30:500}  \n",
    "#     over = SMOTE(sampling_strategy=sampling_stat,k_neighbors=60)\n",
    "#     x_over, y_over = over.fit_resample(np.array(train_inputs), np.array(train_classes))\n",
    "#     print(\"Class:{} Count:{}\".format(*np.unique(y_over, return_counts=True)))  \n",
    " \n",
    "#     x_over = torch.from_numpy(x_over).view(-1,1,512,512)\n",
    "    \n",
    "   \n",
    "#     print(x_over[0].shape)\n",
    "#     print(x_over.shape)\n",
    "  \n",
    "            \n",
    "#     signds = SignDataset(x_over, y_over)\n",
    "#     trainlen = int(len(signds)*0.8)\n",
    "#     torch.manual_seed(0)\n",
    "    \n",
    "#     train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "#     trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#     valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "#     return trainloader, valloader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f2354b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:22.820385Z",
     "iopub.status.busy": "2022-06-22T03:21:22.820180Z",
     "iopub.status.idle": "2022-06-22T03:21:22.823864Z",
     "shell.execute_reply": "2022-06-22T03:21:22.823215Z"
    },
    "papermill": {
     "duration": 0.048892,
     "end_time": "2022-06-22T03:21:22.825497",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.776605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=112)#73 not included \n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "# model, optimizer = train(model,device,trainloader,valloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10e48a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:22.911610Z",
     "iopub.status.busy": "2022-06-22T03:21:22.911059Z",
     "iopub.status.idle": "2022-06-22T03:21:22.914108Z",
     "shell.execute_reply": "2022-06-22T03:21:22.913459Z"
    },
    "papermill": {
     "duration": 0.047153,
     "end_time": "2022-06-22T03:21:22.915755",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.868602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer315.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03ee3eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T03:21:23.001747Z",
     "iopub.status.busy": "2022-06-22T03:21:23.001165Z",
     "iopub.status.idle": "2022-06-22T03:21:23.004465Z",
     "shell.execute_reply": "2022-06-22T03:21:23.003770Z"
    },
    "papermill": {
     "duration": 0.047906,
     "end_time": "2022-06-22T03:21:23.006127",
     "exception": false,
     "start_time": "2022-06-22T03:21:22.958221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#not done on 16:500,41:500,21:500,25:500 ,29 ,34:500,45, ,49:500,,53:500,54,, 58:500,44:500 also"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 401.529666,
   "end_time": "2022-06-22T03:21:25.634782",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-22T03:14:44.105116",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
