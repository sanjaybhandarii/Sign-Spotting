{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4fcbc96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:08:35.294538Z",
     "iopub.status.busy": "2022-06-03T16:08:35.293590Z",
     "iopub.status.idle": "2022-06-03T16:09:01.047548Z",
     "shell.execute_reply": "2022-06-03T16:09:01.046702Z"
    },
    "papermill": {
     "duration": 25.773807,
     "end_time": "2022-06-03T16:09:01.050184",
     "exception": false,
     "start_time": "2022-06-03T16:08:35.276377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideo\r\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting fvcore\r\n",
      "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting av\r\n",
      "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting parameterized\r\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting iopath\r\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\r\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.63.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.1.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.0.1)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.4.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->pytorchvideo) (5.1.1)\r\n",
      "Building wheels for collected packages: pytorchvideo, fvcore\r\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188714 sha256=b4ab39d5655009f17ae284f63dad7a01087446d764c69946b4b586e47533e8b1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/51/05/053b29bac2400cbbae2fb7cfc41afd280d627bca7c9363ca80\r\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=b7c6d7a053848a362c6e124ebce7d3ebaa454761ad8d4d2f25e3661c27f447d4\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\r\n",
      "Successfully built pytorchvideo fvcore\r\n",
      "Installing collected packages: parameterized, av, iopath, fvcore, pytorchvideo\r\n",
      "Successfully installed av-9.2.0 fvcore-0.1.5.post20220512 iopath-0.1.9 parameterized-0.8.1 pytorchvideo-0.1.5\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2876611f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:09:01.105213Z",
     "iopub.status.busy": "2022-06-03T16:09:01.104974Z",
     "iopub.status.idle": "2022-06-03T16:09:01.108788Z",
     "shell.execute_reply": "2022-06-03T16:09:01.108044Z"
    },
    "papermill": {
     "duration": 0.033641,
     "end_time": "2022-06-03T16:09:01.110495",
     "exception": false,
     "start_time": "2022-06-03T16:09:01.076854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start= 0\n",
    "end = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283d821b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-03T16:09:01.163094Z",
     "iopub.status.busy": "2022-06-03T16:09:01.162877Z",
     "iopub.status.idle": "2022-06-03T16:09:03.238157Z",
     "shell.execute_reply": "2022-06-03T16:09:03.237371Z"
    },
    "papermill": {
     "duration": 2.10425,
     "end_time": "2022-06-03T16:09:03.240486",
     "exception": false,
     "start_time": "2022-06-03T16:09:01.136236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ") \n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize\n",
    "#from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.y)\n",
    "\n",
    "        # length = 0\n",
    "        # with open(self.file, 'rb') as f:\n",
    "        #     data = pickle.load(f)\n",
    "\n",
    "        # for x in data:\n",
    "        #     length += len(data[x])\n",
    "        # return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87b6a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:09:03.294369Z",
     "iopub.status.busy": "2022-06-03T16:09:03.294153Z",
     "iopub.status.idle": "2022-06-03T16:09:03.312359Z",
     "shell.execute_reply": "2022-06-03T16:09:03.311708Z"
    },
    "papermill": {
     "duration": 0.047272,
     "end_time": "2022-06-03T16:09:03.314091",
     "exception": false,
     "start_time": "2022-06-03T16:09:03.266819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 720\n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_CHANNEL = 1\n",
    "NUM_FRAMES = 25\n",
    "NUM_CLASSES = 60\n",
    "\n",
    "\n",
    "\n",
    "train_inputs =[] #x\n",
    "train_classes = [] #y\n",
    "\n",
    "\n",
    "def transform_data(x, mean, std):\n",
    "    \n",
    "    transform =  ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose(\n",
    "            [\n",
    "                Lambda(lambda x: x/255.0),\n",
    "                \n",
    "                Normalize((mean,), (std,)),\n",
    "\n",
    "                CenterCrop([720,900]),\n",
    "                Resize([480,600]),\n",
    "                Lambda(lambda x: x.permute(1,2,3,0)),#(channel, frames(depth), height, width)\n",
    "\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return transform(x)\n",
    "\n",
    "def lp_video(video,start_time, end_time):\n",
    "    video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n",
    "            #print(video_data[\"video\"].shape)\n",
    "\n",
    "            \n",
    "    video_data[\"video\"] = Grayscale(num_output_channels=1)((video_data[\"video\"]).permute(1,0,2,3))\n",
    "#             video_data[\"video\"] = video_data[\"video\"]/255\n",
    "            #print(video_data[\"video\"].shape)\n",
    "            \n",
    "    std, mean = torch.std_mean(video_data[\"video\"])\n",
    "    std = std/255.0\n",
    "    mean = mean/255.0\n",
    "    video_data = transform_data( video_data, mean, std)\n",
    "\n",
    "    return video_data[\"video\"]\n",
    "\n",
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    time_start = []\n",
    "    time_end = []\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        filename = key\n",
    "        print(\"file\",filename)\n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "\n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "            for m in torch.unbind(vid, dim=3):\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "            #some negative classes too\n",
    "            for i in range(len(time_start)-1):\n",
    "                if time_end[i]- time_start[i+1]>400:\n",
    "                    start = time_end[i]+50\n",
    "                    end = time_end[i]+300\n",
    "                    vid = lp_video(video,x[1], x[2])\n",
    "                    for m in torch.unbind(vid, dim=3):\n",
    "                        train_classes.append(60)\n",
    "                        train_inputs.append(m)\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    vallen= int(len(val_test_set)*0.8)\n",
    "    val_set, test_set = torch.utils.data.random_split(val_test_set, [vallen, len(val_test_set)-vallen ])\n",
    "    valloader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    return trainloader, valloader, testloader \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815e7df",
   "metadata": {
    "papermill": {
     "duration": 0.02583,
     "end_time": "2022-06-03T16:09:03.366868",
     "exception": false,
     "start_time": "2022-06-03T16:09:03.341038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a9b487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:09:03.421842Z",
     "iopub.status.busy": "2022-06-03T16:09:03.421616Z",
     "iopub.status.idle": "2022-06-03T16:09:03.446356Z",
     "shell.execute_reply": "2022-06-03T16:09:03.445706Z"
    },
    "papermill": {
     "duration": 0.053459,
     "end_time": "2022-06-03T16:09:03.447979",
     "exception": false,
     "start_time": "2022-06-03T16:09:03.394520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate=12, depth=10, reduction=0.5, nClasses=61, bottleneck=True):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(1, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(7290, nClasses)\n",
    "       \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "  \n",
    "        out = self.trans1(self.dense1(out))\n",
    "\n",
    "        out = self.trans2(self.dense2(out))\n",
    "\n",
    "        out = self.dense3(out)\n",
    "\n",
    "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
    "       \n",
    "        out = out.view(out.size(0),-1)\n",
    "        \n",
    "        out = F.log_softmax(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2bf57c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:09:03.501268Z",
     "iopub.status.busy": "2022-06-03T16:09:03.501052Z",
     "iopub.status.idle": "2022-06-03T16:09:03.525361Z",
     "shell.execute_reply": "2022-06-03T16:09:03.524724Z"
    },
    "papermill": {
     "duration": 0.053825,
     "end_time": "2022-06-03T16:09:03.527096",
     "exception": false,
     "start_time": "2022-06-03T16:09:03.473271",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            target = (target).to(device)\n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]# get the index of the max log-probability\n",
    "            \n",
    "            num_samples += pred.shape[0]\n",
    "            \n",
    "#             print(\"num_samples:\", num_samples)\n",
    "            #print(torch.sum(pred==target))\n",
    "            correct += (pred == target).sum().item()\n",
    "            #print(\"correct\", correct)\n",
    "\n",
    "    acc = 100.0 * correct / num_samples\n",
    "    \n",
    "    print('{} Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                mode,correct, num_samples,\n",
    "                100. * correct / num_samples, acc))\n",
    "    \n",
    "def trainonval(model, device,validloader, optimizer, criterion, epochs):\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Training on Validation Set starts with lr\",optimizer.param_groups[0]['lr'])\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(validloader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            pred = (output.data).max(1, keepdim=True)[1]# get the index of the max log-probability\n",
    "            num_samples += pred.shape[0]\n",
    "            correct += torch.sum(pred.data==target)\n",
    "            \n",
    "        \n",
    "        #print(scheduler.get_last_lr())\n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 90:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy on Val set: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            test(model, device, testloader, mode = \"Test after Training on validation set\")\n",
    "                \n",
    "            return model, optimizer\n",
    "            \n",
    "    \n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        \n",
    "    return model, optimizer\n",
    "\n",
    "def train(model, device, train_loader,validloader,testloader, optimizer,criterion, epochs):\n",
    "    print(\"Train start\")\n",
    "    breakout = False\n",
    "    model.half()\n",
    "    model.cuda()\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]# get the index of the max log-probability\n",
    "            num_samples += pred.shape[0]\n",
    "            correct += torch.sum(pred==target)\n",
    "            \n",
    "        #if optimizer.param_groups[0]['lr'] == 1e-2:    \n",
    "        #scheduler.step()\n",
    "        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 95:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            \n",
    "            test(model,device,validloader, mode = \"Validating\")\n",
    "            test(model, device, testloader, mode = \"Test before Training on validation set\")\n",
    "            model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "            test(model, device, testloader, mode = \"Test after training on validation set\")\n",
    "            \n",
    "            \n",
    "            \n",
    "#             torch.save({\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "            \n",
    "            breakout = True\n",
    "            break\n",
    "            \n",
    "            \n",
    "            \n",
    "       \n",
    "        \n",
    "\n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        if (((epoch+1)%5) == 0) :\n",
    "            test(model,device,validloader)\n",
    "            model.train()\n",
    "            \n",
    "    if breakout:\n",
    "            print(\"breakout\")\n",
    "            return\n",
    "    \n",
    "    \n",
    "    \n",
    "    model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "    test(model, device, testloader, mode= \"test set \")\n",
    "    \n",
    "    \n",
    "        \n",
    "#     torch.save({\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f48415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:09:03.582537Z",
     "iopub.status.busy": "2022-06-03T16:09:03.582279Z",
     "iopub.status.idle": "2022-06-03T16:09:03.682061Z",
     "shell.execute_reply": "2022-06-03T16:09:03.681378Z"
    },
    "papermill": {
     "duration": 0.129024,
     "end_time": "2022-06-03T16:09:03.684191",
     "exception": false,
     "start_time": "2022-06-03T16:09:03.555167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 61\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('frontend',DenseNet()),\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer and learning rate\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        \n",
    "        {\"params\": model.frontend.parameters(), \"lr\": 1e-4},\n",
    "  ],\n",
    "  momentum = 0.9\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "# state = torch.load(\"../input/sign-classification/model_optimizer1.pt\")\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.half()\n",
    "# model.cuda()\n",
    "# optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e1f03a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:09:03.737763Z",
     "iopub.status.busy": "2022-06-03T16:09:03.737547Z",
     "iopub.status.idle": "2022-06-03T16:12:55.426960Z",
     "shell.execute_reply": "2022-06-03T16:12:55.423487Z"
    },
    "papermill": {
     "duration": 231.743139,
     "end_time": "2022-06-03T16:12:55.453541",
     "exception": false,
     "start_time": "2022-06-03T16:09:03.710402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n002\n",
      "file p06_n019\n",
      "file p06_n007\n",
      "file p01_n053\n",
      "file p04_n074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainloader, valloader, testloader = load_dataloaders(batch_size=32,start=start, end=end)#73 not included \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e64ca",
   "metadata": {
    "papermill": {
     "duration": 0.026535,
     "end_time": "2022-06-03T16:12:55.507217",
     "exception": false,
     "start_time": "2022-06-03T16:12:55.480682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4165cb21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:12:55.561693Z",
     "iopub.status.busy": "2022-06-03T16:12:55.561454Z",
     "iopub.status.idle": "2022-06-03T16:12:55.566380Z",
     "shell.execute_reply": "2022-06-03T16:12:55.565696Z"
    },
    "papermill": {
     "duration": 0.03478,
     "end_time": "2022-06-03T16:12:55.568795",
     "exception": false,
     "start_time": "2022-06-03T16:12:55.534015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 464941\n"
     ]
    }
   ],
   "source": [
    "print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22912b9",
   "metadata": {
    "papermill": {
     "duration": 0.026782,
     "end_time": "2022-06-03T16:12:55.622547",
     "exception": false,
     "start_time": "2022-06-03T16:12:55.595765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b80838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:12:55.678015Z",
     "iopub.status.busy": "2022-06-03T16:12:55.677829Z",
     "iopub.status.idle": "2022-06-03T16:12:57.009163Z",
     "shell.execute_reply": "2022-06-03T16:12:57.007182Z"
    },
    "papermill": {
     "duration": 1.361282,
     "end_time": "2022-06-03T16:12:57.010949",
     "exception": false,
     "start_time": "2022-06-03T16:12:55.649667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 480, 600]             216\n",
      "       BatchNorm2d-2         [-1, 24, 480, 600]              48\n",
      "            Conv2d-3         [-1, 48, 480, 600]           1,152\n",
      "       BatchNorm2d-4         [-1, 48, 480, 600]              96\n",
      "            Conv2d-5         [-1, 12, 480, 600]           5,184\n",
      "        Bottleneck-6         [-1, 36, 480, 600]               0\n",
      "       BatchNorm2d-7         [-1, 36, 480, 600]              72\n",
      "            Conv2d-8         [-1, 18, 480, 600]             648\n",
      "        Transition-9         [-1, 18, 240, 300]               0\n",
      "      BatchNorm2d-10         [-1, 18, 240, 300]              36\n",
      "           Conv2d-11         [-1, 48, 240, 300]             864\n",
      "      BatchNorm2d-12         [-1, 48, 240, 300]              96\n",
      "           Conv2d-13         [-1, 12, 240, 300]           5,184\n",
      "       Bottleneck-14         [-1, 30, 240, 300]               0\n",
      "      BatchNorm2d-15         [-1, 30, 240, 300]              60\n",
      "           Conv2d-16         [-1, 15, 240, 300]             450\n",
      "       Transition-17         [-1, 15, 120, 150]               0\n",
      "      BatchNorm2d-18         [-1, 15, 120, 150]              30\n",
      "           Conv2d-19         [-1, 48, 120, 150]             720\n",
      "      BatchNorm2d-20         [-1, 48, 120, 150]              96\n",
      "           Conv2d-21         [-1, 12, 120, 150]           5,184\n",
      "       Bottleneck-22         [-1, 27, 120, 150]               0\n",
      "      BatchNorm2d-23         [-1, 27, 120, 150]              54\n",
      "           Linear-24                   [-1, 61]         444,751\n",
      "         DenseNet-25                   [-1, 61]               0\n",
      "================================================================\n",
      "Total params: 464,941\n",
      "Trainable params: 464,941\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.10\n",
      "Forward/backward pass size (MB): 687.20\n",
      "Params size (MB): 1.77\n",
      "Estimated Total Size (MB): 690.07\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:107: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 480, 600), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eeca7c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:12:57.067558Z",
     "iopub.status.busy": "2022-06-03T16:12:57.067337Z",
     "iopub.status.idle": "2022-06-03T16:14:27.314395Z",
     "shell.execute_reply": "2022-06-03T16:14:27.312739Z"
    },
    "papermill": {
     "duration": 90.277335,
     "end_time": "2022-06-03T16:14:27.316336",
     "exception": false,
     "start_time": "2022-06-03T16:12:57.039001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:107: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Training Accuracy: 70728/3362 (2104%) Training Loss: 0.060634\n",
      "Validating Accuracy: 13760/672 (2048%)\n",
      "Test before Training on validation set Accuracy: 3217/169 (1904%)\n",
      "Training on Validation Set starts with lr 0.0001\n",
      "Epoch: 1 , Training Accuracy on Val set: 13760/672 (2048%) Training Loss: 0.057321\n",
      "Test after Training on validation set Accuracy: 3309/169 (1958%)\n",
      "Test after training on validation set Accuracy: 3309/169 (1958%)\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "train(model,device,trainloader,valloader, testloader,optimizer,criterion,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9cd313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T16:14:27.379358Z",
     "iopub.status.busy": "2022-06-03T16:14:27.379111Z",
     "iopub.status.idle": "2022-06-03T16:14:27.382925Z",
     "shell.execute_reply": "2022-06-03T16:14:27.382152Z"
    },
    "papermill": {
     "duration": 0.037596,
     "end_time": "2022-06-03T16:14:27.384981",
     "exception": false,
     "start_time": "2022-06-03T16:14:27.347385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from 21 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58dd35e",
   "metadata": {
    "papermill": {
     "duration": 0.031396,
     "end_time": "2022-06-03T16:14:27.446872",
     "exception": false,
     "start_time": "2022-06-03T16:14:27.415476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 362.48276,
   "end_time": "2022-06-03T16:14:29.696442",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-03T16:08:27.213682",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
