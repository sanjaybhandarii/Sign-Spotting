{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorchvideo","metadata":{"execution":{"iopub.status.busy":"2022-05-10T06:09:40.220713Z","iopub.execute_input":"2022-05-10T06:09:40.221028Z","iopub.status.idle":"2022-05-10T06:09:55.342387Z","shell.execute_reply.started":"2022-05-10T06:09:40.220944Z","shell.execute_reply":"2022-05-10T06:09:55.341412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n) \nimport pickle\n\nimport os\n\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torchvision.transforms import Compose, Lambda, Grayscale\nfrom torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\nfrom pytorchvideo.data.encoded_video import EncodedVideo\nfrom pytorchvideo.transforms import (\n    ApplyTransformToKey,\n    UniformTemporalSubsample,\n)\n\nfrom tqdm import tqdm\n\nfrom collections import OrderedDict\n\nimport torch.optim as optim\n\n\n\nclass SignDataset(Dataset):\n    def __init__(self,x,y):\n        self.x = x\n        print(\"len x\" , len(self.x))\n        self.y = y\n        print(\"len y\" , len(self.y))\n\n\n    def __len__(self):\n\n        return len(self.y)\n\n        # length = 0\n        # with open(self.file, 'rb') as f:\n        #     data = pickle.load(f)\n\n        # for x in data:\n        #     length += len(data[x])\n        # return length\n\n\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T06:09:55.344514Z","iopub.execute_input":"2022-05-10T06:09:55.344762Z","iopub.status.idle":"2022-05-10T06:09:56.929795Z","shell.execute_reply.started":"2022-05-10T06:09:55.344729Z","shell.execute_reply":"2022-05-10T06:09:56.928896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_HEIGHT = 720\nIMAGE_WIDTH = 800\nIMAGE_CHANNEL = 1\nNUM_FRAMES = 25\nNUM_CLASSES = 60\nmean = 0\nstd = 0\n\n\n\ninputs =[] #x\nclasses = [] #y\n\ntransform =  ApplyTransformToKey(\n    key=\"video\",\n    transform=Compose(\n        [\n            \n            Lambda(lambda x: x.permute(1,0,2,3)),#(frames(depth), channel, height, width) -> (channel, frames(depth), height, width)\n            \n            UniformTemporalSubsample(NUM_FRAMES),\n            Lambda(lambda x: x.permute(1,0,2,3)),#(frames(depth), channel, height, width)\n            \n            Lambda(lambda x: x/255.0), \n            NormalizeVideo((mean,), (std,)),\n            CenterCropVideo([720,800]),\n            Lambda(lambda x: x.permute(1,0,2,3)),#(channel, frames(depth), height, width)\n            \n            \n        ]\n        \n    ),\n)\ndef get_data_info(f):\n    for line in f:\n        a = line.split(',')\n        yield a\n\n\n\n\n\n\ndef load_dataloader(batch_size):\n    video = EncodedVideo.from_path(\"../input/signdataset/p01_n000.mp4\")\n        \n\n            \n\n\n    with open('../input/signdataset/p01_n000.txt') as f: \n        for x in get_data_info(f):\n            classes.append(int(x[0]))\n            start_time = x[1]\n            end_time = x[2]\n            \n            video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n\n            \n            video_data[\"video\"] = Grayscale(num_output_channels=1)((video_data[\"video\"]).permute(1,0,2,3))\n\n            #print(video_data[\"video\"].shape)\n            std, mean = torch.std_mean(video_data[\"video\"])\n            #print(std, mean)\n            video_data = transform( video_data)\n\n        # Move the inputs to the desired device\n            inputs.append(video_data[\"video\"])\n\n    signds = SignDataset(inputs, classes)\n    dataloader = DataLoader(signds, batch_size=batch_size, shuffle=True, num_workers=1)\n\n    return dataloader\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T06:09:56.931702Z","iopub.execute_input":"2022-05-10T06:09:56.932129Z","iopub.status.idle":"2022-05-10T06:09:56.944334Z","shell.execute_reply.started":"2022-05-10T06:09:56.932084Z","shell.execute_reply":"2022-05-10T06:09:56.943444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class conv_3d(nn.Module):\n    def __init__(self):\n        super(conv_3d, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv3d(1, 64, kernel_size=5, padding=1),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)))\n        \n\n        self._features = nn.Sequential(\n            self.conv1,\n            self.conv2\n        )\n\n\n    def forward(self, x):\n        out = self._features(x)\n        print(\"conv3d\", out.shape)\n        out= out.reshape(out.shape[0], out.shape[1]*out.shape[2], out.shape[3], out.shape[4])\n        print(\"reshape conv3d \",out.shape)\n        return out\n\nclass conv_2d(nn.Module):\n    def __init__(self):\n        super(conv_2d, self).__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(640, 640, kernel_size = 3, padding =1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n            \n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(640, 512, kernel_size = 3, padding =1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n        \n        self.conv3 = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size = 3, padding =1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n        \n    \n    def forward(self,x):\n        out = self.conv2(self.conv1(x))\n        out = self.conv3(out)\n        print(out.shape)\n        out = out.view(out.shape[0],-1)\n        print(out.shape)\n        return out\n\n\n  \n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T06:12:02.202485Z","iopub.execute_input":"2022-05-10T06:12:02.202778Z","iopub.status.idle":"2022-05-10T06:12:02.214142Z","shell.execute_reply.started":"2022-05-10T06:12:02.202746Z","shell.execute_reply":"2022-05-10T06:12:02.213174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nn_classes = 60\n\nmodel = nn.Sequential(OrderedDict([\n    ('frontend', conv_3d()),\n    ('mid', conv_2d()),\n    ('fc', nn.Sequential( nn.Dropout(p=0.4), nn.Linear(135168, 1024),nn.Linear(1024,256), nn.Linear(256,60) ))\n]))\n\n\n\n\n# specify loss function (categorical cross-entropy)\ncriterion = nn.CrossEntropyLoss()\n# specify optimizer and learning rate\noptimizer = optim.SGD(\n  [\n        {\"params\": model.fc.parameters(), \"lr\": 1e-3},\n        {\"params\": model.mid.parameters(), \"lr\": 1e-5},\n        {\"params\": model.frontend.parameters(), \"lr\": 1e-4},\n  ],\n  momentum = 0.9\n)\n\n\ndef train(model, device, train_loader, optimizer, criterion, epoch):\n    model.cuda()\n    model.train()\n    train_loss = 0\n    correct = 0\n    num_samples = 0\n    \n    for epoch in tqdm(range(epoch)):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            \n            data = data.to(device)\n            target = torch.tensor(target).to(device)\n\n    \n    \n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            num_samples += pred.shape[0]\n            print(num_samples)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            print(\"correct till now:\",correct)\n        \n    train_loss /= num_samples\n    print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n                epoch, correct, num_samples,\n                100. * correct / num_samples, train_loss))\n\n\ndataloader = load_dataloader(batch_size=1)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-10T06:12:05.990849Z","iopub.execute_input":"2022-05-10T06:12:05.991106Z","iopub.status.idle":"2022-05-10T06:12:13.4971Z","shell.execute_reply.started":"2022-05-10T06:12:05.99108Z","shell.execute_reply":"2022-05-10T06:12:13.496183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\ntrain(model,device,dataloader,optimizer,criterion,10)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T06:12:28.616405Z","iopub.execute_input":"2022-05-10T06:12:28.617115Z","iopub.status.idle":"2022-05-10T06:12:28.658369Z","shell.execute_reply.started":"2022-05-10T06:12:28.617077Z","shell.execute_reply":"2022-05-10T06:12:28.657043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}