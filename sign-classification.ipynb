{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d27bf13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:45:36.261747Z",
     "iopub.status.busy": "2022-06-13T15:45:36.261278Z",
     "iopub.status.idle": "2022-06-13T15:46:10.391681Z",
     "shell.execute_reply": "2022-06-13T15:46:10.390544Z"
    },
    "papermill": {
     "duration": 34.229889,
     "end_time": "2022-06-13T15:46:10.394581",
     "exception": false,
     "start_time": "2022-06-13T15:45:36.164692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideo\r\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m634.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting fvcore\r\n",
      "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting av\r\n",
      "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting parameterized\r\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting iopath\r\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\r\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.63.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.1.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.0.1)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.4.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->pytorchvideo) (5.1.1)\r\n",
      "Building wheels for collected packages: pytorchvideo, fvcore\r\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188714 sha256=c62303454bedd5100e11e2c7495edeb1bd2f6a96da02a814e88b80a9eeaeb199\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/51/05/053b29bac2400cbbae2fb7cfc41afd280d627bca7c9363ca80\r\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=295f28d804863a8b10fc20629fc30d259ea4f005957437edda5cbf890d89b0b6\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\r\n",
      "Successfully built pytorchvideo fvcore\r\n",
      "Installing collected packages: parameterized, av, iopath, fvcore, pytorchvideo\r\n",
      "Successfully installed av-9.2.0 fvcore-0.1.5.post20220512 iopath-0.1.9 parameterized-0.8.1 pytorchvideo-0.1.5\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b24c2ae",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:10.522011Z",
     "iopub.status.busy": "2022-06-13T15:46:10.521703Z",
     "iopub.status.idle": "2022-06-13T15:46:13.572642Z",
     "shell.execute_reply": "2022-06-13T15:46:13.571597Z"
    },
    "papermill": {
     "duration": 3.116931,
     "end_time": "2022-06-13T15:46:13.575459",
     "exception": false,
     "start_time": "2022-06-13T15:46:10.458528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ") \n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize\n",
    "#from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.y)\n",
    "\n",
    "        # length = 0\n",
    "        # with open(self.file, 'rb') as f:\n",
    "        #     data = pickle.load(f)\n",
    "\n",
    "        # for x in data:\n",
    "        #     length += len(data[x])\n",
    "        # return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffe27fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:13.697558Z",
     "iopub.status.busy": "2022-06-13T15:46:13.697241Z",
     "iopub.status.idle": "2022-06-13T15:46:13.717915Z",
     "shell.execute_reply": "2022-06-13T15:46:13.716852Z"
    },
    "papermill": {
     "duration": 0.085152,
     "end_time": "2022-06-13T15:46:13.720486",
     "exception": false,
     "start_time": "2022-06-13T15:46:13.635334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 720\n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_CHANNEL = 1\n",
    "NUM_FRAMES = 25\n",
    "NUM_CLASSES = 60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(x):\n",
    "    \n",
    "    transform =  ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose(\n",
    "            [\n",
    "                Lambda(lambda x: x/255.0),\n",
    "                Normalize(([0.2933, 0.4799, 0.6723]), ([0.1576, 0.1674, 0.2477])),\n",
    "                Grayscale(num_output_channels=1),\n",
    "                CenterCrop([720,900]),\n",
    "                Resize([512,512]),\n",
    "                Lambda(lambda x: x.permute(1,2,3,0)),#(channel, frames(depth), height, width)\n",
    "#                 Lambda(lambda x: print(x.shape))\n",
    "\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return transform(x)\n",
    "\n",
    "def lp_video(video,start_time, end_time):\n",
    "    global means,stds\n",
    "    global nst\n",
    "    \n",
    "    video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n",
    "            #print(video_data[\"video\"].shape)\n",
    " \n",
    "    if video_data[\"video\"] is None:\n",
    "        return None        # or pass\n",
    "    else:\n",
    "    \n",
    "        #video_data[\"video\"] = Grayscale(num_output_channels=1)((video_data[\"video\"]).permute(1,0,2,3))\n",
    "    #             video_data[\"video\"] = video_data[\"video\"]/255\n",
    "                #print(video_data[\"video\"].shape)\n",
    "        video_data[\"video\"] = video_data[\"video\"].permute(1,0,2,3)\n",
    "        \n",
    "#         std, mean = torch.std_mean(video_data[\"video\"],dim=[0,2,3])\n",
    "#         std = std/255.0\n",
    "#         #print(std)\n",
    "#         mean = mean/255.0\n",
    "#         #print(mean)\n",
    "#         means += mean\n",
    "#         stds +=std\n",
    "#         nst += 1\n",
    "        video_data = transform_data( video_data)\n",
    "\n",
    "        return video_data[\"video\"]\n",
    "\n",
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    global neg\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        time_start = []\n",
    "        time_end = []\n",
    "        filename = key\n",
    "        print(\"file\",filename)\n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "        \n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "            for m in torch.unbind(vid, dim=3):\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "            #some negative classes too\n",
    "        for i in range(len(time_start)-2):\n",
    "            if (time_start[i+1]- time_end[i])>1000.0:\n",
    "                start_t = time_end[i]+800.0\n",
    "                end_t = time_end[i]+880.0\n",
    "                vid = lp_video(video,start_t, end_t)\n",
    "                if vid is None:\n",
    "                    break\n",
    "                else:\n",
    "                    for m in torch.unbind(vid, dim=3):\n",
    "                        train_classes.append(60)\n",
    "                        train_inputs.append(m)\n",
    "            \n",
    "        \n",
    "            \n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "#     train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(signds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "#     valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "    return trainloader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887b104b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:13.844881Z",
     "iopub.status.busy": "2022-06-13T15:46:13.844552Z",
     "iopub.status.idle": "2022-06-13T15:46:13.856746Z",
     "shell.execute_reply": "2022-06-13T15:46:13.855760Z"
    },
    "papermill": {
     "duration": 0.076605,
     "end_time": "2022-06-13T15:46:13.858823",
     "exception": false,
     "start_time": "2022-06-13T15:46:13.782218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_testloader(batch_size,start,end):\n",
    "\n",
    "    time_start = []\n",
    "    time_end = []\n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    \n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        filename = key\n",
    "        print(\"file\",filename)\n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "\n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "            for m in torch.unbind(vid, dim=3):\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "            #some negative classes too\n",
    "        for i in range(0,len(time_start)//2):\n",
    "            if (time_start[i+1]- time_end[i])>1000.0:\n",
    "                start_t = time_end[i]+700.0\n",
    "                end_t = time_end[i]+780.0\n",
    "                vid = lp_video(video,start_t, end_t)\n",
    "                if vid is None:\n",
    "                    break\n",
    "                else:\n",
    "                    for m in torch.unbind(vid, dim=3):\n",
    "                        train_classes.append(60)\n",
    "                        train_inputs.append(m)\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    \n",
    "    \n",
    "\n",
    "    testloader = DataLoader(signds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "433d1251",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:13.978577Z",
     "iopub.status.busy": "2022-06-13T15:46:13.977723Z",
     "iopub.status.idle": "2022-06-13T15:46:13.984633Z",
     "shell.execute_reply": "2022-06-13T15:46:13.983817Z"
    },
    "papermill": {
     "duration": 0.069035,
     "end_time": "2022-06-13T15:46:13.986659",
     "exception": false,
     "start_time": "2022-06-13T15:46:13.917624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_vid(batch_size,start,end):\n",
    "\n",
    "\n",
    "    train_inputs =[] #x\n",
    "\n",
    "    video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Val_Set/VALIDATION/MSSL_VAL_SET_VIDEOS/p01_n131.mp4\")\n",
    "    # file functions\n",
    "\n",
    "    vid = lp_video(video,start, end)\n",
    "\n",
    "    for m in torch.unbind(vid, dim=3):\n",
    "        train_inputs.append(m)\n",
    "\n",
    "    \n",
    "\n",
    "    testloader = DataLoader(train_inputs, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d435d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:14.110228Z",
     "iopub.status.busy": "2022-06-13T15:46:14.110007Z",
     "iopub.status.idle": "2022-06-13T15:46:14.163716Z",
     "shell.execute_reply": "2022-06-13T15:46:14.162773Z"
    },
    "papermill": {
     "duration": 0.117838,
     "end_time": "2022-06-13T15:46:14.166057",
     "exception": false,
     "start_time": "2022-06-13T15:46:14.048219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a EfficientNetV2 Model as defined in:\n",
    "Mingxing Tan, Quoc V. Le. (2021). \n",
    "EfficientNetV2: Smaller Models and Faster Training\n",
    "arXiv preprint arXiv:2104.00298.\n",
    "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "# SiLU (Swish) activation function\n",
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    " \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
    "                SiLU(),\n",
    "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EffNetV2(nn.Module):\n",
    "    def __init__(self, cfgs, num_classes=61, width_mult=1.):\n",
    "        super(EffNetV2, self).__init__()\n",
    "        self.cfgs = cfgs\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(24 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(1, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = MBConv\n",
    "        for t, c, n, s, use_se in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            for i in range(n):\n",
    "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
    "                input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
    "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(output_channel, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.001)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def effnetv2_s(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-S model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  24,  2, 1, 0],\n",
    "        [4,  48,  4, 2, 0],\n",
    "        [4,  64,  4, 2, 0],\n",
    "        [4, 128,  6, 2, 1],\n",
    "        [6, 160,  9, 1, 1],\n",
    "        [6, 256, 15, 2, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_m(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-M model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  24,  3, 1, 0],\n",
    "        [4,  48,  5, 2, 0],\n",
    "        [4,  80,  5, 2, 0],\n",
    "        [4, 160,  7, 2, 1],\n",
    "        [6, 176, 14, 1, 1],\n",
    "        [6, 304, 18, 2, 1],\n",
    "        [6, 512,  5, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_l(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-L model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  7, 2, 0],\n",
    "        [4,  96,  7, 2, 0],\n",
    "        [4, 192, 10, 2, 1],\n",
    "        [6, 224, 19, 1, 1],\n",
    "        [6, 384, 25, 2, 1],\n",
    "        [6, 640,  7, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_xl(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-XL model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  8, 2, 0],\n",
    "        [4,  96,  8, 2, 0],\n",
    "        [4, 192, 16, 2, 1],\n",
    "        [6, 256, 24, 1, 1],\n",
    "        [6, 512, 32, 2, 1],\n",
    "        [6, 640,  8, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ec911",
   "metadata": {
    "papermill": {
     "duration": 0.058602,
     "end_time": "2022-06-13T15:46:14.283420",
     "exception": false,
     "start_time": "2022-06-13T15:46:14.224818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f25e566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:14.405054Z",
     "iopub.status.busy": "2022-06-13T15:46:14.404288Z",
     "iopub.status.idle": "2022-06-13T15:46:14.410697Z",
     "shell.execute_reply": "2022-06-13T15:46:14.409753Z"
    },
    "papermill": {
     "duration": 0.069037,
     "end_time": "2022-06-13T15:46:14.413209",
     "exception": false,
     "start_time": "2022-06-13T15:46:14.344172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evluate(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            \n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            print(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b2c8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:14.534562Z",
     "iopub.status.busy": "2022-06-13T15:46:14.533815Z",
     "iopub.status.idle": "2022-06-13T15:46:14.541803Z",
     "shell.execute_reply": "2022-06-13T15:46:14.540887Z"
    },
    "papermill": {
     "duration": 0.071838,
     "end_time": "2022-06-13T15:46:14.544118",
     "exception": false,
     "start_time": "2022-06-13T15:46:14.472280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            target = (target).to(device)\n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            #print(\"correct\", correct)\n",
    "\n",
    "    acc = 100.0 * correct / num_samples\n",
    "    \n",
    "    print('{} Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                mode,correct, num_samples,\n",
    "                100. * correct / num_samples, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56f57dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:14.666055Z",
     "iopub.status.busy": "2022-06-13T15:46:14.665700Z",
     "iopub.status.idle": "2022-06-13T15:46:14.679934Z",
     "shell.execute_reply": "2022-06-13T15:46:14.678968Z"
    },
    "papermill": {
     "duration": 0.077509,
     "end_time": "2022-06-13T15:46:14.682015",
     "exception": false,
     "start_time": "2022-06-13T15:46:14.604506",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "def train(model, device, train_loader,scheduler, optimizer,criterion,epochs,lr):\n",
    "    print(\"Train start\")\n",
    "    breakout = False\n",
    "    model.half()\n",
    "    model.cuda()\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "        if optimizer.param_groups[0]['lr'] != 1e-3:    \n",
    "            scheduler.step()\n",
    "        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 95:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            \n",
    "           # test(model,device,validloader, mode = \"Validating\")\n",
    "#             test(model, device, testloader, mode = \"Test before Training on validation set\")\n",
    "           # model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "#             test(model, device, testloader, mode = \"Test after training on validation set\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer34.pt\")\n",
    "            \n",
    "            breakout = True\n",
    "            print(\"breakout\")\n",
    "            return model, optimizer\n",
    "            \n",
    "            \n",
    "            \n",
    "       \n",
    "        \n",
    "\n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "#         if (((epoch+1)%5) == 0) :\n",
    "#             test(model,device,validloader)\n",
    "#             model.train()\n",
    "            \n",
    "    if breakout:\n",
    "            print(\"breakout\")\n",
    "            return\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "    #test(model, device, validloader, mode= \"test set \")\n",
    "    \n",
    "    \n",
    "        \n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer34.pt\")\n",
    "    return model, optimizer\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b15241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:14.804140Z",
     "iopub.status.busy": "2022-06-13T15:46:14.803875Z",
     "iopub.status.idle": "2022-06-13T15:46:20.117352Z",
     "shell.execute_reply": "2022-06-13T15:46:20.116388Z"
    },
    "papermill": {
     "duration": 5.37614,
     "end_time": "2022-06-13T15:46:20.119725",
     "exception": false,
     "start_time": "2022-06-13T15:46:14.743585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 61\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('frontend',effnetv2_m())\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "\n",
    "# specify optimizer and learning rate\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        \n",
    "        {\"params\": model.frontend.parameters(), \"lr\": 1e-2},\n",
    "  ],\n",
    "  momentum = 0.9\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "samples_per_cls = [166.0, 370.0, 63.0, 232.0, 905.0, 102.0, 537.0, 993.0, 133.0, 154.0, 286.0, 435.0, 452.0, 779.0, 354.0, 136.0, 232.0, 2154.0, 374.0, 787.0, 337.0, 268.0, 954.0, 167.0, 543.0, 324.0, 669.0, 333.0, 211.0, 433.0, 140.0, 665.0, 107.0, 305.0, 162.0, 61.0, 1229.0, 958.0, 897.0, 543.0, 625.0, 334.0, 776.0, 111.0, 412.0, 583.0, 255.0, 103.0, 108.0, 201.0, 384.0, 632.0, 13.0, 322.0, 664.0, 435.0, 194.0, 322.0, 113.0, 462.0,2330.0]\n",
    "no_of_classes=61\n",
    "beta=0.9999\n",
    "effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "weights = (1.0 - beta) / np.array(effective_num)\n",
    "weights = weights / np.sum(weights) * no_of_classes\n",
    "    \n",
    "\n",
    "weights = torch.FloatTensor(weights).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight = weights.half())\n",
    "\n",
    "# state = torch.load(\"../input/sign-classification/model_optimizer34.pt\")\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.half()\n",
    "# model.cuda()\n",
    "# optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90f2595a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:20.356547Z",
     "iopub.status.busy": "2022-06-13T15:46:20.355937Z",
     "iopub.status.idle": "2022-06-13T15:46:20.364280Z",
     "shell.execute_reply": "2022-06-13T15:46:20.362465Z"
    },
    "papermill": {
     "duration": 0.136548,
     "end_time": "2022-06-13T15:46:20.367880",
     "exception": false,
     "start_time": "2022-06-13T15:46:20.231332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1eaa4fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:20.586545Z",
     "iopub.status.busy": "2022-06-13T15:46:20.586192Z",
     "iopub.status.idle": "2022-06-13T15:46:20.590752Z",
     "shell.execute_reply": "2022-06-13T15:46:20.589729Z"
    },
    "papermill": {
     "duration": 0.115914,
     "end_time": "2022-06-13T15:46:20.593579",
     "exception": false,
     "start_time": "2022-06-13T15:46:20.477665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#summary(model, (1, 512, 640), device='cpu')#1334+986 =2330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7911960",
   "metadata": {
    "papermill": {
     "duration": 0.10481,
     "end_time": "2022-06-13T15:46:20.801379",
     "exception": false,
     "start_time": "2022-06-13T15:46:20.696569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3241e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T15:46:20.931289Z",
     "iopub.status.busy": "2022-06-13T15:46:20.930994Z",
     "iopub.status.idle": "2022-06-13T16:24:37.897739Z",
     "shell.execute_reply": "2022-06-13T16:24:37.896599Z"
    },
    "papermill": {
     "duration": 2297.032863,
     "end_time": "2022-06-13T16:24:37.900947",
     "exception": false,
     "start_time": "2022-06-13T15:46:20.868084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n002\n",
      "file p06_n019\n",
      "file p06_n007\n",
      "file p01_n053\n",
      "file p04_n074\n",
      "file p05_n077\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 78/1710 (5%) Training Loss: 0.239065\n",
      "Epoch: 2 , Training Accuracy: 111/1710 (6%) Training Loss: 0.213755\n",
      "Epoch: 3 , Training Accuracy: 156/1710 (9%) Training Loss: 0.197290\n",
      "Epoch: 4 , Training Accuracy: 189/1710 (11%) Training Loss: 0.184309\n",
      "Epoch: 5 , Training Accuracy: 236/1710 (14%) Training Loss: 0.172478\n",
      "Epoch: 6 , Training Accuracy: 348/1710 (20%) Training Loss: 0.152598\n",
      "Epoch: 7 , Training Accuracy: 466/1710 (27%) Training Loss: 0.134245\n",
      "Epoch: 8 , Training Accuracy: 591/1710 (35%) Training Loss: 0.116057\n",
      "Epoch: 9 , Training Accuracy: 730/1710 (43%) Training Loss: 0.098180\n",
      "Epoch: 10 , Training Accuracy: 839/1710 (49%) Training Loss: 0.085124\n",
      "Epoch: 11 , Training Accuracy: 979/1710 (57%) Training Loss: 0.069424\n",
      "Epoch: 12 , Training Accuracy: 1103/1710 (65%) Training Loss: 0.057940\n",
      "Epoch: 13 , Training Accuracy: 1201/1710 (70%) Training Loss: 0.049599\n",
      "Epoch: 14 , Training Accuracy: 1247/1710 (73%) Training Loss: 0.044901\n",
      "Epoch: 15 , Training Accuracy: 1357/1710 (79%) Training Loss: 0.035451\n",
      "Epoch: 16 , Training Accuracy: 1418/1710 (83%) Training Loss: 0.029253\n",
      "Epoch: 17 , Training Accuracy: 1480/1710 (87%) Training Loss: 0.023390\n",
      "Epoch: 18 , Training Accuracy: 1565/1710 (92%) Training Loss: 0.018469\n",
      "Epoch: 19 , Training Accuracy: 1584/1710 (93%) Training Loss: 0.017269\n",
      "Epoch: 20 , Training Accuracy: 1607/1710 (94%) Training Loss: 0.015769\n",
      "Epoch: 21 , Training Accuracy: 1627/1710 (95%) Training Loss: 0.013776\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=0, end=6)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fcad60c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T16:24:38.041029Z",
     "iopub.status.busy": "2022-06-13T16:24:38.040741Z",
     "iopub.status.idle": "2022-06-13T16:46:43.675967Z",
     "shell.execute_reply": "2022-06-13T16:46:43.674835Z"
    },
    "papermill": {
     "duration": 1325.706434,
     "end_time": "2022-06-13T16:46:43.679080",
     "exception": false,
     "start_time": "2022-06-13T16:24:37.972646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p05_n027\n",
      "file p05_n022\n",
      "file p01_n110\n",
      "file p06_n036\n",
      "file p01_n069\n",
      "file p06_n109\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 391/1778 (22%) Training Loss: 0.240775\n",
      "Epoch: 2 , Training Accuracy: 864/1778 (49%) Training Loss: 0.121036\n",
      "Epoch: 3 , Training Accuracy: 1116/1778 (63%) Training Loss: 0.084216\n",
      "Epoch: 4 , Training Accuracy: 1302/1778 (73%) Training Loss: 0.058492\n",
      "Epoch: 5 , Training Accuracy: 1410/1778 (79%) Training Loss: 0.044962\n",
      "Epoch: 6 , Training Accuracy: 1480/1778 (83%) Training Loss: 0.034984\n",
      "Epoch: 7 , Training Accuracy: 1509/1778 (85%) Training Loss: 0.028838\n",
      "Epoch: 8 , Training Accuracy: 1575/1778 (89%) Training Loss: 0.020265\n",
      "Epoch: 9 , Training Accuracy: 1620/1778 (91%) Training Loss: 0.018197\n",
      "Epoch: 10 , Training Accuracy: 1669/1778 (94%) Training Loss: 0.013386\n",
      "Epoch: 11 , Training Accuracy: 1710/1778 (96%) Training Loss: 0.010872\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=6, end=12)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "391e37e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T16:46:43.843267Z",
     "iopub.status.busy": "2022-06-13T16:46:43.843017Z",
     "iopub.status.idle": "2022-06-13T16:57:51.709023Z",
     "shell.execute_reply": "2022-06-13T16:57:51.707300Z"
    },
    "papermill": {
     "duration": 667.94977,
     "end_time": "2022-06-13T16:57:51.711474",
     "exception": false,
     "start_time": "2022-06-13T16:46:43.761704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p04_n089\n",
      "file p01_n044\n",
      "file p05_n037\n",
      "file p04_n038\n",
      "file p06_n055\n",
      "file p01_n099\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 779/1726 (45%) Training Loss: 0.152698\n",
      "Epoch: 2 , Training Accuracy: 1365/1726 (79%) Training Loss: 0.052371\n",
      "Epoch: 3 , Training Accuracy: 1536/1726 (89%) Training Loss: 0.026756\n",
      "Epoch: 4 , Training Accuracy: 1612/1726 (93%) Training Loss: 0.013165\n",
      "Epoch: 5 , Training Accuracy: 1665/1726 (96%) Training Loss: 0.007257\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=12, end=18)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c9a7f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T16:57:51.883660Z",
     "iopub.status.busy": "2022-06-13T16:57:51.882824Z",
     "iopub.status.idle": "2022-06-13T17:06:45.088715Z",
     "shell.execute_reply": "2022-06-13T17:06:45.087726Z"
    },
    "papermill": {
     "duration": 533.295082,
     "end_time": "2022-06-13T17:06:45.092034",
     "exception": false,
     "start_time": "2022-06-13T16:57:51.796952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p02_n066\n",
      "file p04_n035\n",
      "file p01_n057\n",
      "file p01_n105\n",
      "file p04_n058\n",
      "file p05_n043\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 714/1378 (52%) Training Loss: 0.149710\n",
      "Epoch: 2 , Training Accuracy: 1182/1378 (86%) Training Loss: 0.044491\n",
      "Epoch: 3 , Training Accuracy: 1261/1378 (92%) Training Loss: 0.023082\n",
      "Epoch: 4 , Training Accuracy: 1280/1378 (93%) Training Loss: 0.017196\n",
      "Epoch: 5 , Training Accuracy: 1322/1378 (96%) Training Loss: 0.009697\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=18, end=24)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cefd3240",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:06:45.277222Z",
     "iopub.status.busy": "2022-06-13T17:06:45.276924Z",
     "iopub.status.idle": "2022-06-13T17:15:49.770498Z",
     "shell.execute_reply": "2022-06-13T17:15:49.769186Z"
    },
    "papermill": {
     "duration": 544.588338,
     "end_time": "2022-06-13T17:15:49.773897",
     "exception": false,
     "start_time": "2022-06-13T17:06:45.185559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p06_n020\n",
      "file p02_n001\n",
      "file p04_n032\n",
      "file p05_n084\n",
      "file p01_n102\n",
      "file p06_n096\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 784/1673 (47%) Training Loss: 0.132834\n",
      "Epoch: 2 , Training Accuracy: 1429/1673 (85%) Training Loss: 0.027780\n",
      "Epoch: 3 , Training Accuracy: 1549/1673 (93%) Training Loss: 0.013524\n",
      "Epoch: 4 , Training Accuracy: 1621/1673 (97%) Training Loss: 0.007814\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=24, end=30)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2d762c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:15:49.965579Z",
     "iopub.status.busy": "2022-06-13T17:15:49.965254Z",
     "iopub.status.idle": "2022-06-13T17:24:24.111017Z",
     "shell.execute_reply": "2022-06-13T17:24:24.109498Z"
    },
    "papermill": {
     "duration": 514.242491,
     "end_time": "2022-06-13T17:24:24.113547",
     "exception": false,
     "start_time": "2022-06-13T17:15:49.871056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p02_n104\n",
      "file p01_n090\n",
      "file p01_n040\n",
      "file p04_n073\n",
      "file p01_n067\n",
      "file p05_n004\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 869/1597 (54%) Training Loss: 0.111727\n",
      "Epoch: 2 , Training Accuracy: 1375/1597 (86%) Training Loss: 0.025981\n",
      "Epoch: 3 , Training Accuracy: 1494/1597 (94%) Training Loss: 0.013084\n",
      "Epoch: 4 , Training Accuracy: 1550/1597 (97%) Training Loss: 0.009380\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=30, end=36)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e5ae515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:24:24.414254Z",
     "iopub.status.busy": "2022-06-13T17:24:24.413959Z",
     "iopub.status.idle": "2022-06-13T17:32:17.858664Z",
     "shell.execute_reply": "2022-06-13T17:32:17.857414Z"
    },
    "papermill": {
     "duration": 473.598002,
     "end_time": "2022-06-13T17:32:17.862157",
     "exception": false,
     "start_time": "2022-06-13T17:24:24.264155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p06_n061\n",
      "file p05_n092\n",
      "file p06_n015\n",
      "file p04_n106\n",
      "file p05_n095\n",
      "file p02_n072\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 1028/1800 (57%) Training Loss: 0.101794\n",
      "Epoch: 2 , Training Accuracy: 1627/1800 (90%) Training Loss: 0.016722\n",
      "Epoch: 3 , Training Accuracy: 1719/1800 (96%) Training Loss: 0.007794\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=36, end=42)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efcb3af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:32:18.068661Z",
     "iopub.status.busy": "2022-06-13T17:32:18.068393Z",
     "iopub.status.idle": "2022-06-13T17:39:12.014464Z",
     "shell.execute_reply": "2022-06-13T17:39:12.013236Z"
    },
    "papermill": {
     "duration": 414.047203,
     "end_time": "2022-06-13T17:39:12.018043",
     "exception": false,
     "start_time": "2022-06-13T17:32:17.970840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p05_n031\n",
      "file p01_n060\n",
      "file p05_n048\n",
      "file p05_n029\n",
      "file p01_n000\n",
      "file p01_n050\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 885/1525 (58%) Training Loss: 0.125260\n",
      "Epoch: 2 , Training Accuracy: 1359/1525 (89%) Training Loss: 0.020447\n",
      "Epoch: 3 , Training Accuracy: 1453/1525 (95%) Training Loss: 0.008610\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=42, end=48)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a06efe62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:39:12.237616Z",
     "iopub.status.busy": "2022-06-13T17:39:12.237261Z",
     "iopub.status.idle": "2022-06-13T17:47:00.310158Z",
     "shell.execute_reply": "2022-06-13T17:47:00.308911Z"
    },
    "papermill": {
     "duration": 468.179343,
     "end_time": "2022-06-13T17:47:00.313490",
     "exception": false,
     "start_time": "2022-06-13T17:39:12.134147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p05_n025\n",
      "file p04_n011\n",
      "file p01_n023\n",
      "file p01_n042\n",
      "file p01_n111\n",
      "file p05_n083\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 856/1502 (57%) Training Loss: 0.108837\n",
      "Epoch: 2 , Training Accuracy: 1340/1502 (89%) Training Loss: 0.023798\n",
      "Epoch: 3 , Training Accuracy: 1425/1502 (95%) Training Loss: 0.018582\n",
      "Epoch: 4 , Training Accuracy: 1439/1502 (96%) Training Loss: 0.014398\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=48, end=54)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6aedf62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:47:00.535430Z",
     "iopub.status.busy": "2022-06-13T17:47:00.535173Z",
     "iopub.status.idle": "2022-06-13T17:51:26.150628Z",
     "shell.execute_reply": "2022-06-13T17:51:26.149675Z"
    },
    "papermill": {
     "duration": 265.72786,
     "end_time": "2022-06-13T17:51:26.152894",
     "exception": false,
     "start_time": "2022-06-13T17:47:00.425034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p05_n094\n",
      "file p04_n103\n",
      "file p01_n093\n",
      "file p04_n003\n",
      "file p05_n078\n",
      "file p01_n052\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 646/1000 (65%) Training Loss: 0.084154\n",
      "Epoch: 2 , Training Accuracy: 928/1000 (93%) Training Loss: 0.011814\n",
      "Epoch: 3 , Training Accuracy: 969/1000 (97%) Training Loss: 0.006261\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=54, end=60)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e1af0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:51:26.380178Z",
     "iopub.status.busy": "2022-06-13T17:51:26.379914Z",
     "iopub.status.idle": "2022-06-13T17:58:57.709957Z",
     "shell.execute_reply": "2022-06-13T17:58:57.708846Z"
    },
    "papermill": {
     "duration": 451.447538,
     "end_time": "2022-06-13T17:58:57.713465",
     "exception": false,
     "start_time": "2022-06-13T17:51:26.265927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p02_n098\n",
      "file p06_n013\n",
      "file p05_n006\n",
      "file p01_n063\n",
      "file p02_n097\n",
      "file p05_n065\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 985/1783 (55%) Training Loss: 0.095674\n",
      "Epoch: 2 , Training Accuracy: 1589/1783 (89%) Training Loss: 0.017318\n",
      "Epoch: 3 , Training Accuracy: 1728/1783 (97%) Training Loss: 0.007770\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=60, end=66)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a0e4861",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T17:58:57.955593Z",
     "iopub.status.busy": "2022-06-13T17:58:57.955346Z",
     "iopub.status.idle": "2022-06-13T18:03:24.964167Z",
     "shell.execute_reply": "2022-06-13T18:03:24.961414Z"
    },
    "papermill": {
     "duration": 267.134135,
     "end_time": "2022-06-13T18:03:24.969056",
     "exception": false,
     "start_time": "2022-06-13T17:58:57.834921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n108\n",
      "file p06_n018\n",
      "file p05_n009\n",
      "file p05_n071\n",
      "file p04_n091\n",
      "file p05_n075\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 710/1036 (69%) Training Loss: 0.085804\n",
      "Epoch: 2 , Training Accuracy: 984/1036 (95%) Training Loss: 0.011048\n",
      "Epoch: 3 , Training Accuracy: 1003/1036 (97%) Training Loss: 0.006574\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=66, end=72)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0b7318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:03:25.222886Z",
     "iopub.status.busy": "2022-06-13T18:03:25.222630Z",
     "iopub.status.idle": "2022-06-13T18:10:17.102311Z",
     "shell.execute_reply": "2022-06-13T18:10:17.101224Z"
    },
    "papermill": {
     "duration": 412.004215,
     "end_time": "2022-06-13T18:10:17.104659",
     "exception": false,
     "start_time": "2022-06-13T18:03:25.100444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n012\n",
      "file p05_n076\n",
      "file p04_n005\n",
      "file p04_n046\n",
      "file p01_n045\n",
      "file p05_n041\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 1099/1620 (68%) Training Loss: 0.077422\n",
      "Epoch: 2 , Training Accuracy: 1512/1620 (93%) Training Loss: 0.010693\n",
      "Epoch: 3 , Training Accuracy: 1581/1620 (98%) Training Loss: 0.005313\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=72, end=78)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e47df6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:10:17.356876Z",
     "iopub.status.busy": "2022-06-13T18:10:17.356059Z",
     "iopub.status.idle": "2022-06-13T18:17:47.577138Z",
     "shell.execute_reply": "2022-06-13T18:17:47.575852Z"
    },
    "papermill": {
     "duration": 450.351795,
     "end_time": "2022-06-13T18:17:47.580702",
     "exception": false,
     "start_time": "2022-06-13T18:10:17.228907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n008\n",
      "file p02_n101\n",
      "file p01_n085\n",
      "file p05_n064\n",
      "file p05_n017\n",
      "file p01_n028\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 1035/1666 (62%) Training Loss: 0.094722\n",
      "Epoch: 2 , Training Accuracy: 1531/1666 (92%) Training Loss: 0.013871\n",
      "Epoch: 3 , Training Accuracy: 1607/1666 (96%) Training Loss: 0.008048\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=78, end=84)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75dbe45d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:17:47.856613Z",
     "iopub.status.busy": "2022-06-13T18:17:47.856303Z",
     "iopub.status.idle": "2022-06-13T18:25:27.702436Z",
     "shell.execute_reply": "2022-06-13T18:25:27.700914Z"
    },
    "papermill": {
     "duration": 459.986011,
     "end_time": "2022-06-13T18:25:27.706756",
     "exception": false,
     "start_time": "2022-06-13T18:17:47.720745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p02_n079\n",
      "file p01_n010\n",
      "file p01_n082\n",
      "file p01_n107\n",
      "file p02_n088\n",
      "file p01_n087\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 703/1433 (49%) Training Loss: 0.138630\n",
      "Epoch: 2 , Training Accuracy: 1194/1433 (83%) Training Loss: 0.025169\n",
      "Epoch: 3 , Training Accuracy: 1354/1433 (94%) Training Loss: 0.010094\n",
      "Epoch: 4 , Training Accuracy: 1396/1433 (97%) Training Loss: 0.005477\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=84, end=90)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ad2e2ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:25:28.048453Z",
     "iopub.status.busy": "2022-06-13T18:25:28.047839Z",
     "iopub.status.idle": "2022-06-13T18:31:49.755803Z",
     "shell.execute_reply": "2022-06-13T18:31:49.754578Z"
    },
    "papermill": {
     "duration": 381.874064,
     "end_time": "2022-06-13T18:31:49.759371",
     "exception": false,
     "start_time": "2022-06-13T18:25:27.885307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p05_n086\n",
      "file p06_n024\n",
      "file p05_n070\n",
      "file p04_n016\n",
      "file p04_n068\n",
      "file p04_n026\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 956/1441 (66%) Training Loss: 0.085118\n",
      "Epoch: 2 , Training Accuracy: 1341/1441 (93%) Training Loss: 0.013862\n",
      "Epoch: 3 , Training Accuracy: 1401/1441 (97%) Training Loss: 0.009330\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=90, end=96)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdf3dc62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:31:50.072363Z",
     "iopub.status.busy": "2022-06-13T18:31:50.072088Z",
     "iopub.status.idle": "2022-06-13T18:37:38.652025Z",
     "shell.execute_reply": "2022-06-13T18:37:38.650743Z"
    },
    "papermill": {
     "duration": 348.730619,
     "end_time": "2022-06-13T18:37:38.655660",
     "exception": false,
     "start_time": "2022-06-13T18:31:49.925041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p06_n039\n",
      "file p06_n014\n",
      "file p05_n059\n",
      "file p02_n051\n",
      "file p06_n056\n",
      "file p04_n080\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 870/1394 (62%) Training Loss: 0.089716\n",
      "Epoch: 2 , Training Accuracy: 1285/1394 (92%) Training Loss: 0.014932\n",
      "Epoch: 3 , Training Accuracy: 1347/1394 (97%) Training Loss: 0.006831\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=96, end=102)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fda2bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:37:38.960343Z",
     "iopub.status.busy": "2022-06-13T18:37:38.960055Z",
     "iopub.status.idle": "2022-06-13T18:42:44.452819Z",
     "shell.execute_reply": "2022-06-13T18:42:44.451661Z"
    },
    "papermill": {
     "duration": 305.640237,
     "end_time": "2022-06-13T18:42:44.456325",
     "exception": false,
     "start_time": "2022-06-13T18:37:38.816088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p05_n062\n",
      "file p04_n034\n",
      "file p04_n049\n",
      "file p06_n033\n",
      "file p02_n100\n",
      "file p04_n030\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 854/1250 (68%) Training Loss: 0.080728\n",
      "Epoch: 2 , Training Accuracy: 1181/1250 (94%) Training Loss: 0.010762\n",
      "Epoch: 3 , Training Accuracy: 1215/1250 (97%) Training Loss: 0.007015\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=102, end=108)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1e0765b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:42:44.767465Z",
     "iopub.status.busy": "2022-06-13T18:42:44.766863Z",
     "iopub.status.idle": "2022-06-13T18:47:16.461142Z",
     "shell.execute_reply": "2022-06-13T18:47:16.459864Z"
    },
    "papermill": {
     "duration": 271.851312,
     "end_time": "2022-06-13T18:47:16.463758",
     "exception": false,
     "start_time": "2022-06-13T18:42:44.612446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p06_n054\n",
      "file p05_n047\n",
      "file p06_n021\n",
      "file p01_n081\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 667/1124 (59%) Training Loss: 0.090490\n",
      "Epoch: 2 , Training Accuracy: 992/1124 (88%) Training Loss: 0.013405\n",
      "Epoch: 3 , Training Accuracy: 1070/1124 (95%) Training Loss: 0.007672\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_dataloaders(batch_size=16,start=108, end=112)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,scheduler,optimizer,criterion,30,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a208dd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:47:16.777010Z",
     "iopub.status.busy": "2022-06-13T18:47:16.776639Z",
     "iopub.status.idle": "2022-06-13T18:50:32.035085Z",
     "shell.execute_reply": "2022-06-13T18:50:32.033685Z"
    },
    "papermill": {
     "duration": 195.570172,
     "end_time": "2022-06-13T18:50:32.189148",
     "exception": false,
     "start_time": "2022-06-13T18:47:16.618976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n002\n",
      "file p06_n019\n",
      "file p06_n007\n",
      "file p01_n053\n",
      "file p04_n074\n",
      "file p05_n077\n",
      "Validate Accuracy: 349/1831 (19%)\n"
     ]
    }
   ],
   "source": [
    "trainloader = load_testloader(batch_size=16,start=0, end=6)#73 not included \n",
    "test(model,device,trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0a53a",
   "metadata": {
    "papermill": {
     "duration": 0.156672,
     "end_time": "2022-06-13T18:50:32.538270",
     "exception": false,
     "start_time": "2022-06-13T18:50:32.381598",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e3f3638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-13T18:50:32.861674Z",
     "iopub.status.busy": "2022-06-13T18:50:32.861343Z",
     "iopub.status.idle": "2022-06-13T18:50:42.676396Z",
     "shell.execute_reply": "2022-06-13T18:50:42.675416Z"
    },
    "papermill": {
     "duration": 9.985075,
     "end_time": "2022-06-13T18:50:42.678782",
     "exception": false,
     "start_time": "2022-06-13T18:50:32.693707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([60, 60, 50, 50, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60],\n",
      "       device='cuda:0')\n",
      "tensor([60, 60, 60, 60, 60,  8, 60, 20,  8, 60,  8, 60, 60, 50, 60, 60],\n",
      "       device='cuda:0')\n",
      "tensor([60, 60, 20, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 20],\n",
      "       device='cuda:0')\n",
      "tensor([50, 60, 60, 60, 60, 60, 13,  8, 60,  8,  8, 60, 60, 60, 60, 20],\n",
      "       device='cuda:0')\n",
      "tensor([60,  8,  8, 60, 60, 45, 60, 60, 60, 60, 12, 60, 45, 60, 60, 60],\n",
      "       device='cuda:0')\n",
      "tensor([60, 60, 60,  8, 60, 50, 60, 19, 60, 60, 60, 60, 60, 60, 60, 19],\n",
      "       device='cuda:0')\n",
      "tensor([60, 60, 60, 60], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "trainloader= load_vid(batch_size=16,start=3000, end=7000)#73 not included \n",
    "evluate(model,device,trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ade8a2",
   "metadata": {
    "papermill": {
     "duration": 0.164423,
     "end_time": "2022-06-13T18:50:43.004746",
     "exception": false,
     "start_time": "2022-06-13T18:50:42.840323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2c8a7",
   "metadata": {
    "papermill": {
     "duration": 0.163843,
     "end_time": "2022-06-13T18:50:43.328309",
     "exception": false,
     "start_time": "2022-06-13T18:50:43.164466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11119.265671,
   "end_time": "2022-06-13T18:50:45.637123",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-13T15:45:26.371452",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
