{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794d00ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:29.988849Z",
     "iopub.status.busy": "2022-06-05T15:01:29.988252Z",
     "iopub.status.idle": "2022-06-05T15:01:51.822507Z",
     "shell.execute_reply": "2022-06-05T15:01:51.821722Z"
    },
    "papermill": {
     "duration": 21.85593,
     "end_time": "2022-06-05T15:01:51.824971",
     "exception": false,
     "start_time": "2022-06-05T15:01:29.969041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting av\r\n",
      "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: av\r\n",
      "Successfully installed av-9.2.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef739765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:51.879521Z",
     "iopub.status.busy": "2022-06-05T15:01:51.879289Z",
     "iopub.status.idle": "2022-06-05T15:01:51.882825Z",
     "shell.execute_reply": "2022-06-05T15:01:51.882130Z"
    },
    "papermill": {
     "duration": 0.03273,
     "end_time": "2022-06-05T15:01:51.884635",
     "exception": false,
     "start_time": "2022-06-05T15:01:51.851905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start= 0\n",
    "end = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00dd052c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:51.936621Z",
     "iopub.status.busy": "2022-06-05T15:01:51.935998Z",
     "iopub.status.idle": "2022-06-05T15:01:53.930889Z",
     "shell.execute_reply": "2022-06-05T15:01:53.930141Z"
    },
    "papermill": {
     "duration": 2.023211,
     "end_time": "2022-06-05T15:01:53.932905",
     "exception": false,
     "start_time": "2022-06-05T15:01:51.909694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ") \n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize,ConvertImageDtype,ToTensor\n",
    "from torchvision.io import read_video\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.y)\n",
    "\n",
    "        # length = 0\n",
    "        # with open(self.file, 'rb') as f:\n",
    "        #     data = pickle.load(f)\n",
    "\n",
    "        # for x in data:\n",
    "        #     length += len(data[x])\n",
    "        # return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e46b5e",
   "metadata": {
    "papermill": {
     "duration": 0.035598,
     "end_time": "2022-06-05T15:01:54.008967",
     "exception": false,
     "start_time": "2022-06-05T15:01:53.973369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b4a3cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.061210Z",
     "iopub.status.busy": "2022-06-05T15:01:54.060989Z",
     "iopub.status.idle": "2022-06-05T15:01:54.077302Z",
     "shell.execute_reply": "2022-06-05T15:01:54.076703Z"
    },
    "papermill": {
     "duration": 0.044278,
     "end_time": "2022-06-05T15:01:54.078931",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.034653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "IMAGE_CHANNEL = 1\n",
    "NUM_FRAMES = 10\n",
    "NUM_CLASSES = 61\n",
    "\n",
    "\n",
    "\n",
    "train_inputs =[] #x\n",
    "train_classes = [] #y\n",
    "\n",
    "def preprocess(batch,mean,std):\n",
    "    transforms = Compose(\n",
    "        [\n",
    "            \n",
    "            Lambda(lambda x: x/255.0),\n",
    "            CenterCrop(size=(720, 900)),\n",
    "            \n",
    "            Normalize(mean=mean, std=std),  # map [0, 1] into [-1, 1]\n",
    "            Grayscale(num_output_channels=1),\n",
    "            Resize(size=(512, 512)),\n",
    "        ]\n",
    "    )\n",
    "    batch = transforms(batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "def lp_video(filename,start_time, end_time):\n",
    "    frames, _, _ = read_video(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\",start_pts=start_time/1000.0,end_pts= end_time/1000.0,pts_unit='sec')\n",
    "    \n",
    "    frames = frames.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "    frames = ConvertImageDtype(torch.float32)(frames)\n",
    "    \n",
    "\n",
    "    \n",
    "  \n",
    "            \n",
    "    std, mean = torch.std_mean(frames)\n",
    "    std = std/255.0\n",
    "    mean = mean/255.0\n",
    "\n",
    "    # tList = [m for m in torch.unbind(video_data, dim=4) ]\n",
    "    # y = torch.stack(tList, dim=0)\n",
    "        \n",
    "            \n",
    "            \n",
    "    img1_batch = preprocess( frames, mean, std)\n",
    "    \n",
    "    return img1_batch\n",
    "\n",
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    time_start = []\n",
    "    time_end = []\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        \n",
    "        print(\" \\n file:\",key)\n",
    "    \n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            \n",
    "            vid = lp_video(key,x[1], x[2])\n",
    "\n",
    "            for m in vid:\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "            #some negative classes too\n",
    "        for i in range(0,5):\n",
    "            if  time_start[i+1]-time_end[i]>800:\n",
    "                start_t = time_end[i]+500\n",
    "                end_t = time_end[i]+550\n",
    "                vid = lp_video(key,start, end)\n",
    "                for m in vid:\n",
    "                    print(\"here\")\n",
    "                    train_classes.append(60)\n",
    "                    train_inputs.append(m)\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "    return trainloader, valloader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b917a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.131586Z",
     "iopub.status.busy": "2022-06-05T15:01:54.131403Z",
     "iopub.status.idle": "2022-06-05T15:01:54.158888Z",
     "shell.execute_reply": "2022-06-05T15:01:54.158176Z"
    },
    "papermill": {
     "duration": 0.05495,
     "end_time": "2022-06-05T15:01:54.160459",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.105509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In ResNet, we see how the skip connection added as identity function from the inputs\n",
    "to interact with the Conv layers. But in DenseNet, we see instead of adding skip \n",
    "connection to Conv layers, we can append or concat the output of identity function\n",
    "with output of Conv layers.\n",
    "\n",
    "In ResNet, it is little tedious to make the dimensions to match for adding the skip\n",
    "connection and Conv Layers, but it is much simpler in DenseNet, as we concat the \n",
    "both the X and Conv's output.\n",
    "\n",
    "The key idea or the reason its called DenseNet is because the next layers not only get\n",
    "the input from previous layer but also preceeding layers before the previous layer. So \n",
    "the next layer becomes dense as it loaded with output from previous layers.\n",
    "\n",
    "Check Figure 7.7.2 from https://d2l.ai/chapter_convolutional-modern/densenet.html for \n",
    "why DenseNet is Dense?\n",
    "\n",
    "Two blocks comprise DenseNet, one is DenseBlock for concat operation and other is \n",
    "transition layer for controlling channels meaning dimensions (recall 1x1 Conv).\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\"\"\"\n",
    "In ResNet, we see how the skip connection added as identity function from the inputs\n",
    "to interact with the Conv layers. But in DenseNet, we see instead of adding skip \n",
    "connection to Conv layers, we can append or concat the output of identity function\n",
    "with output of Conv layers.\n",
    "\n",
    "In ResNet, it is little tedious to make the dimensions to match for adding the skip\n",
    "connection and Conv Layers, but it is much simpler in DenseNet, as we concat the \n",
    "both the X and Conv's output.\n",
    "\n",
    "The key idea or the reason its called DenseNet is because the next layers not only get\n",
    "the input from previous layer but also preceeding layers before the previous layer. So \n",
    "the next layer becomes dense as it loaded with output from previous layers.\n",
    "\n",
    "Check Figure 7.7.2 from https://d2l.ai/chapter_convolutional-modern/densenet.html for \n",
    "why DenseNet is Dense?\n",
    "\n",
    "Two blocks comprise DenseNet, one is DenseBlock for concat operation and other is \n",
    "transition layer for controlling channels meaning dimensions (recall 1x1 Conv).\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_channel=1, growthRate=12, depth=10, reduction=0.5, n_classes=61, bottleneck=True):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(input_channel, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans3 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        nChannels = nOutChannels\n",
    "        self.dense4 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(nChannels, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = torch.squeeze(F.adaptive_avg_pool2d(out, 1))\n",
    "        out = F.log_softmax(self.fc(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92da7ef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.211964Z",
     "iopub.status.busy": "2022-06-05T15:01:54.211493Z",
     "iopub.status.idle": "2022-06-05T15:01:54.215643Z",
     "shell.execute_reply": "2022-06-05T15:01:54.214897Z"
    },
    "papermill": {
     "duration": 0.032267,
     "end_time": "2022-06-05T15:01:54.217492",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.185225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(outputs: Variable, labels: Variable) -> float:\n",
    "    \"\"\"Evaluate neural network outputs against non-one-hotted labels.\"\"\"\n",
    "    Y = labels.numpy()\n",
    "    Yhat = np.argmax(outputs, axis=1)\n",
    "    return float(np.sum(Yhat == Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323be292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.268540Z",
     "iopub.status.busy": "2022-06-05T15:01:54.268359Z",
     "iopub.status.idle": "2022-06-05T15:01:54.274853Z",
     "shell.execute_reply": "2022-06-05T15:01:54.274085Z"
    },
    "papermill": {
     "duration": 0.034479,
     "end_time": "2022-06-05T15:01:54.276827",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.242348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            target = (target).to(device)\n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            #print(\"correct\", correct)\n",
    "\n",
    "    acc = 100.0 * correct / num_samples\n",
    "    \n",
    "    print('{} Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                mode,correct, num_samples,\n",
    "                100. * correct / num_samples, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34124d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.327814Z",
     "iopub.status.busy": "2022-06-05T15:01:54.327607Z",
     "iopub.status.idle": "2022-06-05T15:01:54.336649Z",
     "shell.execute_reply": "2022-06-05T15:01:54.335893Z"
    },
    "papermill": {
     "duration": 0.036321,
     "end_time": "2022-06-05T15:01:54.338479",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.302158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainonval(model, device,validloader, optimizer, criterion, epochs):\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Training on Validation Set starts with lr\",optimizer.param_groups[0]['lr'])\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(validloader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        #print(scheduler.get_last_lr())\n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 90:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy on Val set: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            test(model, device, testloader, mode = \"Test after Training on validation set\")\n",
    "                \n",
    "            return model, optimizer\n",
    "            \n",
    "    \n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9364b596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.389148Z",
     "iopub.status.busy": "2022-06-05T15:01:54.388965Z",
     "iopub.status.idle": "2022-06-05T15:01:54.401219Z",
     "shell.execute_reply": "2022-06-05T15:01:54.400563Z"
    },
    "papermill": {
     "duration": 0.039732,
     "end_time": "2022-06-05T15:01:54.402878",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.363146",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "def train(model, device, train_loader,validloader,scheduler, optimizer,criterion, epochs):\n",
    "    print(\"Train start\")\n",
    "    breakout = False\n",
    "    model.half()\n",
    "    model.cuda()\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "        #if optimizer.param_groups[0]['lr'] == 1e-2:    \n",
    "        scheduler.step()\n",
    "        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 95:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            \n",
    "            test(model,device,validloader, mode = \"Validating\")\n",
    "#             test(model, device, testloader, mode = \"Test before Training on validation set\")\n",
    "           # model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "#             test(model, device, testloader, mode = \"Test after training on validation set\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "            \n",
    "            breakout = True\n",
    "            print(\"breakout\")\n",
    "            return\n",
    "            \n",
    "            \n",
    "            \n",
    "       \n",
    "        \n",
    "\n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        if (((epoch+1)%5) == 0) :\n",
    "            test(model,device,validloader)\n",
    "            model.train()\n",
    "            \n",
    "    if breakout:\n",
    "            print(\"breakout\")\n",
    "            return\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "    test(model, device, validloader, mode= \"test set \")\n",
    "    \n",
    "    \n",
    "        \n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03217d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.453728Z",
     "iopub.status.busy": "2022-06-05T15:01:54.453501Z",
     "iopub.status.idle": "2022-06-05T15:01:54.567429Z",
     "shell.execute_reply": "2022-06-05T15:01:54.566783Z"
    },
    "papermill": {
     "duration": 0.141854,
     "end_time": "2022-06-05T15:01:54.569500",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.427646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 61\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('frontend',DenseNet()),\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer and learning rate\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        \n",
    "        {\"params\": model.frontend.parameters(), \"lr\": 1e-1},\n",
    "  ],\n",
    "  momentum = 0.9\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "# state = torch.load(\"../input/sign-classification/model_optimizer1.pt\")\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.half()\n",
    "# model.cuda()\n",
    "# optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51931fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:01:54.621104Z",
     "iopub.status.busy": "2022-06-05T15:01:54.620915Z",
     "iopub.status.idle": "2022-06-05T15:03:18.030567Z",
     "shell.execute_reply": "2022-06-05T15:03:18.029834Z"
    },
    "papermill": {
     "duration": 83.437652,
     "end_time": "2022-06-05T15:03:18.032482",
     "exception": false,
     "start_time": "2022-06-05T15:01:54.594830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " file: p01_n002\n",
      "here\n",
      "here\n",
      "here\n",
      " \n",
      " file: p06_n019\n",
      "here\n",
      "here\n",
      "here\n",
      " \n",
      " file: p06_n007\n",
      "here\n",
      "here\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainloader, valloader = load_dataloaders(batch_size=32,start=start, end=end)#73 not included \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6659a79",
   "metadata": {
    "papermill": {
     "duration": 0.027122,
     "end_time": "2022-06-05T15:03:18.087786",
     "exception": false,
     "start_time": "2022-06-05T15:03:18.060664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19699a5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:03:18.144170Z",
     "iopub.status.busy": "2022-06-05T15:03:18.143940Z",
     "iopub.status.idle": "2022-06-05T15:03:18.147495Z",
     "shell.execute_reply": "2022-06-05T15:03:18.146769Z"
    },
    "papermill": {
     "duration": 0.033303,
     "end_time": "2022-06-05T15:03:18.149337",
     "exception": false,
     "start_time": "2022-06-05T15:03:18.116034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba88c0",
   "metadata": {
    "papermill": {
     "duration": 0.026725,
     "end_time": "2022-06-05T15:03:18.203672",
     "exception": false,
     "start_time": "2022-06-05T15:03:18.176947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5eb11b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:03:18.259816Z",
     "iopub.status.busy": "2022-06-05T15:03:18.259608Z",
     "iopub.status.idle": "2022-06-05T15:03:18.262547Z",
     "shell.execute_reply": "2022-06-05T15:03:18.261898Z"
    },
    "papermill": {
     "duration": 0.032832,
     "end_time": "2022-06-05T15:03:18.264160",
     "exception": false,
     "start_time": "2022-06-05T15:03:18.231328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#summary(model, (1, 512, 512), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a0557eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:03:18.319335Z",
     "iopub.status.busy": "2022-06-05T15:03:18.318766Z",
     "iopub.status.idle": "2022-06-05T15:14:36.852467Z",
     "shell.execute_reply": "2022-06-05T15:14:36.851563Z"
    },
    "papermill": {
     "duration": 678.563822,
     "end_time": "2022-06-05T15:14:36.854553",
     "exception": false,
     "start_time": "2022-06-05T15:03:18.290731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:155: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Training Accuracy: 91/755 (12%) Training Loss: 0.115403\n",
      "Epoch: 2 , Training Accuracy: 150/755 (20%) Training Loss: 0.090576\n",
      "Epoch: 3 , Training Accuracy: 146/755 (19%) Training Loss: 0.084564\n",
      "Epoch: 4 , Training Accuracy: 156/755 (21%) Training Loss: 0.081271\n",
      "Epoch: 5 , Training Accuracy: 148/755 (20%) Training Loss: 0.079320\n",
      "Validate Accuracy: 32/189 (17%)\n",
      "Epoch: 6 , Training Accuracy: 167/755 (22%) Training Loss: 0.076676\n",
      "Epoch: 7 , Training Accuracy: 166/755 (22%) Training Loss: 0.073272\n",
      "Epoch: 8 , Training Accuracy: 189/755 (25%) Training Loss: 0.071267\n",
      "Epoch: 9 , Training Accuracy: 185/755 (25%) Training Loss: 0.070034\n",
      "Epoch: 10 , Training Accuracy: 194/755 (26%) Training Loss: 0.073555\n",
      "Validate Accuracy: 32/189 (17%)\n",
      "Epoch: 11 , Training Accuracy: 184/755 (24%) Training Loss: 0.074395\n",
      "Epoch: 12 , Training Accuracy: 213/755 (28%) Training Loss: 0.067698\n",
      "Epoch: 13 , Training Accuracy: 193/755 (26%) Training Loss: 0.068218\n",
      "Epoch: 14 , Training Accuracy: 170/755 (23%) Training Loss: 0.071590\n",
      "Epoch: 15 , Training Accuracy: 219/755 (29%) Training Loss: 0.069174\n",
      "Validate Accuracy: 53/189 (28%)\n",
      "Epoch: 16 , Training Accuracy: 213/755 (28%) Training Loss: 0.066862\n",
      "Epoch: 17 , Training Accuracy: 222/755 (29%) Training Loss: 0.065542\n",
      "Epoch: 18 , Training Accuracy: 244/755 (32%) Training Loss: 0.064281\n",
      "Epoch: 19 , Training Accuracy: 223/755 (30%) Training Loss: 0.066916\n",
      "Epoch: 20 , Training Accuracy: 238/755 (32%) Training Loss: 0.063363\n",
      "Validate Accuracy: 14/189 (7%)\n",
      "Epoch: 21 , Training Accuracy: 228/755 (30%) Training Loss: 0.059843\n",
      "Epoch: 22 , Training Accuracy: 283/755 (37%) Training Loss: 0.058878\n",
      "Epoch: 23 , Training Accuracy: 265/755 (35%) Training Loss: 0.058872\n",
      "Epoch: 24 , Training Accuracy: 281/755 (37%) Training Loss: 0.059180\n",
      "Epoch: 25 , Training Accuracy: 265/755 (35%) Training Loss: 0.058409\n",
      "Validate Accuracy: 17/189 (9%)\n",
      "Epoch: 26 , Training Accuracy: 264/755 (35%) Training Loss: 0.059857\n",
      "Epoch: 27 , Training Accuracy: 267/755 (35%) Training Loss: 0.059079\n",
      "Epoch: 28 , Training Accuracy: 297/755 (39%) Training Loss: 0.057023\n",
      "Epoch: 29 , Training Accuracy: 265/755 (35%) Training Loss: 0.059454\n",
      "Epoch: 30 , Training Accuracy: 285/755 (38%) Training Loss: 0.055982\n",
      "Validate Accuracy: 34/189 (18%)\n",
      "Epoch: 31 , Training Accuracy: 265/755 (35%) Training Loss: 0.058342\n",
      "Epoch: 32 , Training Accuracy: 307/755 (41%) Training Loss: 0.056904\n",
      "Epoch: 33 , Training Accuracy: 299/755 (40%) Training Loss: 0.057018\n",
      "Epoch: 34 , Training Accuracy: 269/755 (36%) Training Loss: 0.056055\n",
      "Epoch: 35 , Training Accuracy: 301/755 (40%) Training Loss: 0.055544\n",
      "Validate Accuracy: 54/189 (29%)\n",
      "Epoch: 36 , Training Accuracy: 299/755 (40%) Training Loss: 0.057232\n",
      "Epoch: 37 , Training Accuracy: 294/755 (39%) Training Loss: 0.056624\n",
      "Epoch: 38 , Training Accuracy: 292/755 (39%) Training Loss: 0.057709\n",
      "Epoch: 39 , Training Accuracy: 295/755 (39%) Training Loss: 0.056152\n",
      "Epoch: 40 , Training Accuracy: 301/755 (40%) Training Loss: 0.055641\n",
      "Validate Accuracy: 65/189 (34%)\n",
      "Epoch: 41 , Training Accuracy: 320/755 (42%) Training Loss: 0.055598\n",
      "Epoch: 42 , Training Accuracy: 321/755 (43%) Training Loss: 0.054911\n",
      "Epoch: 43 , Training Accuracy: 330/755 (44%) Training Loss: 0.055161\n",
      "Epoch: 44 , Training Accuracy: 324/755 (43%) Training Loss: 0.054078\n",
      "Epoch: 45 , Training Accuracy: 323/755 (43%) Training Loss: 0.053873\n",
      "Validate Accuracy: 88/189 (47%)\n",
      "Epoch: 46 , Training Accuracy: 316/755 (42%) Training Loss: 0.053636\n",
      "Epoch: 47 , Training Accuracy: 328/755 (43%) Training Loss: 0.054424\n",
      "Epoch: 48 , Training Accuracy: 326/755 (43%) Training Loss: 0.054019\n",
      "Epoch: 49 , Training Accuracy: 334/755 (44%) Training Loss: 0.054506\n",
      "Epoch: 50 , Training Accuracy: 305/755 (40%) Training Loss: 0.054804\n",
      "Validate Accuracy: 91/189 (48%)\n",
      "test set  Accuracy: 91/189 (48%)\n"
     ]
    }
   ],
   "source": [
    "train(model,device,trainloader,valloader,scheduler,optimizer,criterion,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7738d076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-05T15:14:36.948211Z",
     "iopub.status.busy": "2022-06-05T15:14:36.947519Z",
     "iopub.status.idle": "2022-06-05T15:14:36.952282Z",
     "shell.execute_reply": "2022-06-05T15:14:36.951602Z"
    },
    "papermill": {
     "duration": 0.053425,
     "end_time": "2022-06-05T15:14:36.953834",
     "exception": false,
     "start_time": "2022-06-05T15:14:36.900409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from 21 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2c863",
   "metadata": {
    "papermill": {
     "duration": 0.045867,
     "end_time": "2022-06-05T15:14:37.045309",
     "exception": false,
     "start_time": "2022-06-05T15:14:36.999442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 797.608517,
   "end_time": "2022-06-05T15:14:39.512350",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-05T15:01:21.903833",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
