{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5315054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:44:00.004339Z",
     "iopub.status.busy": "2022-05-26T12:44:00.003931Z",
     "iopub.status.idle": "2022-05-26T12:44:16.338492Z",
     "shell.execute_reply": "2022-05-26T12:44:16.337745Z"
    },
    "papermill": {
     "duration": 16.353846,
     "end_time": "2022-05-26T12:44:16.340827",
     "exception": false,
     "start_time": "2022-05-26T12:43:59.986981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorchvideo in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (0.1.5)\n",
      "Requirement already satisfied: networkx in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from pytorchvideo) (2.8)\n",
      "Requirement already satisfied: fvcore in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from pytorchvideo) (0.1.5.post20220506)\n",
      "Requirement already satisfied: av in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from pytorchvideo) (9.2.0)\n",
      "Requirement already satisfied: parameterized in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from pytorchvideo) (0.8.1)\n",
      "Requirement already satisfied: iopath in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from pytorchvideo) (0.1.9)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from fvcore->pytorchvideo) (0.1.8)\n",
      "Requirement already satisfied: numpy in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from fvcore->pytorchvideo) (1.22.3)\n",
      "Requirement already satisfied: tabulate in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from fvcore->pytorchvideo) (0.8.9)\n",
      "Requirement already satisfied: tqdm in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from fvcore->pytorchvideo) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from fvcore->pytorchvideo) (6.0)\n",
      "Requirement already satisfied: Pillow in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from fvcore->pytorchvideo) (9.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from fvcore->pytorchvideo) (1.1.0)\n",
      "Requirement already satisfied: portalocker in /home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages (from iopath->pytorchvideo) (2.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/home/chaos/.virtualenvs/dsenv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7ef26c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:44:16.397708Z",
     "iopub.status.busy": "2022-05-26T12:44:16.397407Z",
     "iopub.status.idle": "2022-05-26T12:44:16.401139Z",
     "shell.execute_reply": "2022-05-26T12:44:16.400497Z"
    },
    "papermill": {
     "duration": 0.033697,
     "end_time": "2022-05-26T12:44:16.402778",
     "exception": false,
     "start_time": "2022-05-26T12:44:16.369081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start= 0\n",
    "end = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a17e010",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-26T12:44:16.458621Z",
     "iopub.status.busy": "2022-05-26T12:44:16.458026Z",
     "iopub.status.idle": "2022-05-26T12:44:18.675401Z",
     "shell.execute_reply": "2022-05-26T12:44:18.674700Z"
    },
    "papermill": {
     "duration": 2.247058,
     "end_time": "2022-05-26T12:44:18.677408",
     "exception": false,
     "start_time": "2022-05-26T12:44:16.430350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaos/.virtualenvs/dsenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ") \n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize\n",
    "#from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.y)\n",
    "\n",
    "        # length = 0\n",
    "        # with open(self.file, 'rb') as f:\n",
    "        #     data = pickle.load(f)\n",
    "\n",
    "        # for x in data:\n",
    "        #     length += len(data[x])\n",
    "        # return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 720\n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_CHANNEL = 1\n",
    "NUM_FRAMES = 25\n",
    "NUM_CLASSES = 60\n",
    "\n",
    "\n",
    "\n",
    "train_inputs =[] #x\n",
    "train_classes = [] #y\n",
    "\n",
    "\n",
    "def transform_data(x, mean, std):\n",
    "    \n",
    "    transform =  ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose(\n",
    "            [\n",
    "                Lambda(lambda x: x/255.0),\n",
    "                \n",
    "                Normalize((mean,), (std,)),\n",
    "\n",
    "                CenterCrop([720,900]),\n",
    "                Resize([480,600]),\n",
    "                Lambda(lambda x: x.permute(1,2,3,0)),#(channel, frames(depth), height, width)\n",
    "\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return transform(x)\n",
    "\n",
    "def lp_video(video,start_time, end_time):\n",
    "    video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n",
    "            #print(video_data[\"video\"].shape)\n",
    "\n",
    "            \n",
    "    video_data[\"video\"] = Grayscale(num_output_channels=1)((video_data[\"video\"]).permute(1,0,2,3))\n",
    "#             video_data[\"video\"] = video_data[\"video\"]/255\n",
    "            #print(video_data[\"video\"].shape)\n",
    "            \n",
    "    std, mean = torch.std_mean(video_data[\"video\"])\n",
    "    std = std/255.0\n",
    "    mean = mean/255.0\n",
    "    video_data = transform_data( video_data, mean, std)\n",
    "\n",
    "    return video_data[\"video\"]\n",
    "\n",
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    time_start = []\n",
    "    time_end = []\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        filename = key\n",
    "        print(\"file\",filename)\n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "\n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "            for m in torch.unbind(vid, dim=3):\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "            #some negative classes too\n",
    "            for i in range(len(time_start)-1):\n",
    "                if time_end[i]- time_start[i+1]>400:\n",
    "                    start = time_end[i]+50\n",
    "                    end = time_end[i]+300\n",
    "                    vid = lp_video(video,x[1], x[2])\n",
    "                    for m in torch.unbind(vid, dim=3):\n",
    "                        train_classes.append(60)\n",
    "                        train_inputs.append(m)\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    vallen= int(len(val_test_set)*0.8)\n",
    "    val_set, test_set = torch.utils.data.random_split(val_test_set, [vallen, len(val_test_set)-vallen ])\n",
    "    valloader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    return trainloader, valloader, testloader \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c9b6d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n002\n",
      "torch.Size([3, 720, 1280, 25])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n",
      "torch.Size([3, 720, 1280])\n"
     ]
    }
   ],
   "source": [
    "with open('MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "for key in list(data.keys())[start:end]:\n",
    "    filename = key\n",
    "    print(\"file\",filename)\n",
    "    video = EncodedVideo.from_path(\"p04_n074.mp4\")\n",
    "    video_data = video.get_clip(1, 2)\n",
    "    video_data['video'] = video_data['video'].permute(0,2,3,1)\n",
    "    print(video_data[\"video\"].shape)\n",
    "    for m in torch.unbind(video_data['video'], dim=3):\n",
    "        print(m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f8cc40a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:44:18.806040Z",
     "iopub.status.busy": "2022-05-26T12:44:18.805814Z",
     "iopub.status.idle": "2022-05-26T12:44:18.849231Z",
     "shell.execute_reply": "2022-05-26T12:44:18.848434Z"
    },
    "papermill": {
     "duration": 0.07329,
     "end_time": "2022-05-26T12:44:18.850959",
     "exception": false,
     "start_time": "2022-05-26T12:44:18.777669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate=12, depth=10, reduction=0.5, nClasses=61, bottleneck=True):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(1, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(nChannels, nClasses)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
    "        out = F.log_softmax(self.fc(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817746d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:44:18.907873Z",
     "iopub.status.busy": "2022-05-26T12:44:18.907653Z",
     "iopub.status.idle": "2022-05-26T12:44:18.931411Z",
     "shell.execute_reply": "2022-05-26T12:44:18.930758Z"
    },
    "papermill": {
     "duration": 0.053612,
     "end_time": "2022-05-26T12:44:18.933025",
     "exception": false,
     "start_time": "2022-05-26T12:44:18.879413",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            target = (target).to(device)\n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]# get the index of the max log-probability\n",
    "            \n",
    "            num_samples += pred.shape[0]\n",
    "            \n",
    "#             print(\"num_samples:\", num_samples)\n",
    "            #print(torch.sum(pred==target))\n",
    "            correct += (pred == target).sum().item()\n",
    "            #print(\"correct\", correct)\n",
    "\n",
    "    acc = 100.0 * correct / num_samples\n",
    "    \n",
    "    print('{} Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                mode,correct, num_samples,\n",
    "                100. * correct / num_samples, acc))\n",
    "    \n",
    "def trainonval(model, device,validloader, optimizer, criterion, epochs):\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Training on Validation Set starts with lr\",optimizer.param_groups[0]['lr'])\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(validloader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            pred = (output.data).max(1, keepdim=True)[1]# get the index of the max log-probability\n",
    "            num_samples += pred.shape[0]\n",
    "            correct += torch.sum(pred.data==target)\n",
    "            \n",
    "        \n",
    "        #print(scheduler.get_last_lr())\n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 90:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy on Val set: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            test(model, device, testloader, mode = \"Test after Training on validation set\")\n",
    "                \n",
    "            return model, optimizer\n",
    "            \n",
    "    \n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        \n",
    "    return model, optimizer\n",
    "\n",
    "def train(model, device, train_loader,validloader,testloader, optimizer,scheduler,criterion, epochs):\n",
    "    print(\"Train start\")\n",
    "    breakout = False\n",
    "    model.half()\n",
    "    model.cuda()\n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = 1e-2\n",
    "    optimizer.param_groups[1]['lr'] = 1e-3\n",
    "  \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]# get the index of the max log-probability\n",
    "            num_samples += pred.shape[0]\n",
    "            correct += torch.sum(pred==target)\n",
    "            \n",
    "        #if optimizer.param_groups[0]['lr'] == 1e-2:    \n",
    "        scheduler.step()\n",
    "        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 93:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            \n",
    "            test(model,device,validloader, mode = \"Validating\")\n",
    "            test(model, device, testloader, mode = \"Test before Training on validation set\")\n",
    "            model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "            test(model, device, testloader, mode = \"Test after training on validation set\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "            \n",
    "            breakout = True\n",
    "            break\n",
    "            \n",
    "            \n",
    "            \n",
    "       \n",
    "        \n",
    "\n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        if (((epoch+1)%5) == 0) :\n",
    "            test(model,device,validloader)\n",
    "            model.train()\n",
    "            \n",
    "    if breakout:\n",
    "            print(\"breakout\")\n",
    "            return\n",
    "    \n",
    "    \n",
    "    \n",
    "    model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "    test(model, device, testloader, mode= \"test set \")\n",
    "    \n",
    "    \n",
    "        \n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad309600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:44:18.987458Z",
     "iopub.status.busy": "2022-05-26T12:44:18.986902Z",
     "iopub.status.idle": "2022-05-26T12:44:19.313123Z",
     "shell.execute_reply": "2022-05-26T12:44:19.312266Z"
    },
    "papermill": {
     "duration": 0.355647,
     "end_time": "2022-05-26T12:44:19.315250",
     "exception": false,
     "start_time": "2022-05-26T12:44:18.959603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 60\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('frontend',DenseNet()),\n",
    "    ('fc', nn.Sequential(nn.Linear(1458, 60)))\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer and learning rate\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        {\"params\": model.fc.parameters(), \"lr\": 1e-2},\n",
    "        {\"params\": model.frontend.parameters(), \"lr\": 1e-2},\n",
    "  ],\n",
    "  momentum = 0.9\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "# state = torch.load(\"../input/sign-classification/model_optimizer1.pt\")\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.half()\n",
    "# model.cuda()\n",
    "# optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3994d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:44:19.370805Z",
     "iopub.status.busy": "2022-05-26T12:44:19.370143Z",
     "iopub.status.idle": "2022-05-26T12:47:17.440506Z",
     "shell.execute_reply": "2022-05-26T12:47:17.439764Z"
    },
    "papermill": {
     "duration": 178.100089,
     "end_time": "2022-05-26T12:47:17.442569",
     "exception": false,
     "start_time": "2022-05-26T12:44:19.342480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n002\n",
      "file p06_n019\n",
      "file p06_n007\n",
      "file p01_n053\n",
      "file p04_n074\n",
      "file p05_n077\n",
      "file p05_n027\n",
      "file p05_n022\n",
      "file p01_n110\n",
      "file p06_n036\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainloader, valloader, testloader = load_dataloaders(batch_size=4,start=start, end=end)#73 not included \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a645d",
   "metadata": {
    "papermill": {
     "duration": 0.029873,
     "end_time": "2022-05-26T12:47:17.502281",
     "exception": false,
     "start_time": "2022-05-26T12:47:17.472408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe88853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:47:17.562573Z",
     "iopub.status.busy": "2022-05-26T12:47:17.562307Z",
     "iopub.status.idle": "2022-05-26T12:47:17.567633Z",
     "shell.execute_reply": "2022-05-26T12:47:17.566915Z"
    },
    "papermill": {
     "duration": 0.03812,
     "end_time": "2022-05-26T12:47:17.570051",
     "exception": false,
     "start_time": "2022-05-26T12:47:17.531931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 25887050\n"
     ]
    }
   ],
   "source": [
    "print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7b86313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T12:47:17.632406Z",
     "iopub.status.busy": "2022-05-26T12:47:17.631677Z",
     "iopub.status.idle": "2022-05-26T14:13:07.817202Z",
     "shell.execute_reply": "2022-05-26T14:13:07.816142Z"
    },
    "papermill": {
     "duration": 5150.218814,
     "end_time": "2022-05-26T14:13:07.820089",
     "exception": false,
     "start_time": "2022-05-26T12:47:17.601275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 41/167 (25%) Training Loss: 0.978516\n",
      "Epoch: 2 , Training Accuracy: 52/167 (31%) Training Loss: 0.937851\n",
      "Epoch: 3 , Training Accuracy: 39/167 (23%) Training Loss: 0.909770\n",
      "Epoch: 4 , Training Accuracy: 51/167 (31%) Training Loss: 0.889666\n",
      "Epoch: 5 , Training Accuracy: 54/167 (32%) Training Loss: 0.876579\n",
      "Validate Accuracy: 2/33 (6%)\n",
      "Epoch: 6 , Training Accuracy: 64/167 (38%) Training Loss: 0.862685\n",
      "Epoch: 7 , Training Accuracy: 48/167 (29%) Training Loss: 0.834628\n",
      "Epoch: 8 , Training Accuracy: 69/167 (41%) Training Loss: 0.807108\n",
      "Epoch: 9 , Training Accuracy: 79/167 (47%) Training Loss: 0.788256\n",
      "Epoch: 10 , Training Accuracy: 75/167 (45%) Training Loss: 0.761169\n",
      "Validate Accuracy: 12/33 (36%)\n",
      "Epoch: 11 , Training Accuracy: 72/167 (43%) Training Loss: 0.755807\n",
      "Epoch: 12 , Training Accuracy: 78/167 (47%) Training Loss: 0.736621\n",
      "Epoch: 13 , Training Accuracy: 95/167 (57%) Training Loss: 0.704394\n",
      "Epoch: 14 , Training Accuracy: 81/167 (49%) Training Loss: 0.682331\n",
      "Epoch: 15 , Training Accuracy: 90/167 (54%) Training Loss: 0.652718\n",
      "Validate Accuracy: 9/33 (27%)\n",
      "Epoch: 16 , Training Accuracy: 105/167 (63%) Training Loss: 0.608691\n",
      "Epoch: 17 , Training Accuracy: 112/167 (67%) Training Loss: 0.610895\n",
      "Epoch: 18 , Training Accuracy: 111/167 (66%) Training Loss: 0.608995\n",
      "Epoch: 19 , Training Accuracy: 109/167 (65%) Training Loss: 0.596241\n",
      "Epoch: 20 , Training Accuracy: 119/167 (71%) Training Loss: 0.587587\n",
      "Validate Accuracy: 14/33 (42%)\n",
      "Epoch: 21 , Training Accuracy: 123/167 (74%) Training Loss: 0.588101\n",
      "Epoch: 22 , Training Accuracy: 125/167 (75%) Training Loss: 0.591610\n",
      "Epoch: 23 , Training Accuracy: 118/167 (71%) Training Loss: 0.583797\n",
      "Epoch: 24 , Training Accuracy: 93/167 (56%) Training Loss: 0.605872\n",
      "Epoch: 25 , Training Accuracy: 98/167 (59%) Training Loss: 0.595662\n",
      "Validate Accuracy: 17/33 (52%)\n",
      "Epoch: 26 , Training Accuracy: 132/167 (79%) Training Loss: 0.568131\n",
      "Epoch: 27 , Training Accuracy: 114/167 (68%) Training Loss: 0.590575\n",
      "Epoch: 28 , Training Accuracy: 121/167 (72%) Training Loss: 0.585043\n",
      "Epoch: 29 , Training Accuracy: 111/167 (66%) Training Loss: 0.584353\n",
      "Epoch: 30 , Training Accuracy: 121/167 (72%) Training Loss: 0.561249\n",
      "Validate Accuracy: 12/33 (36%)\n",
      "Epoch: 31 , Training Accuracy: 107/167 (64%) Training Loss: 0.579254\n",
      "Epoch: 32 , Training Accuracy: 111/167 (66%) Training Loss: 0.575944\n",
      "Epoch: 33 , Training Accuracy: 127/167 (76%) Training Loss: 0.564892\n",
      "Epoch: 34 , Training Accuracy: 126/167 (75%) Training Loss: 0.574827\n",
      "Epoch: 35 , Training Accuracy: 123/167 (74%) Training Loss: 0.564494\n",
      "Validate Accuracy: 17/33 (52%)\n",
      "Epoch: 36 , Training Accuracy: 121/167 (72%) Training Loss: 0.585370\n",
      "Epoch: 37 , Training Accuracy: 121/167 (72%) Training Loss: 0.576169\n",
      "Epoch: 38 , Training Accuracy: 125/167 (75%) Training Loss: 0.558933\n",
      "Epoch: 39 , Training Accuracy: 115/167 (69%) Training Loss: 0.584774\n",
      "Epoch: 40 , Training Accuracy: 106/167 (63%) Training Loss: 0.584546\n",
      "Validate Accuracy: 11/33 (33%)\n",
      "Epoch: 41 , Training Accuracy: 112/167 (67%) Training Loss: 0.571862\n",
      "Epoch: 42 , Training Accuracy: 108/167 (65%) Training Loss: 0.568886\n",
      "Epoch: 43 , Training Accuracy: 106/167 (63%) Training Loss: 0.578037\n",
      "Epoch: 44 , Training Accuracy: 111/167 (66%) Training Loss: 0.599943\n",
      "Epoch: 45 , Training Accuracy: 110/167 (66%) Training Loss: 0.573593\n",
      "Validate Accuracy: 11/33 (33%)\n",
      "Epoch: 46 , Training Accuracy: 106/167 (63%) Training Loss: 0.592148\n",
      "Epoch: 47 , Training Accuracy: 118/167 (71%) Training Loss: 0.554857\n",
      "Epoch: 48 , Training Accuracy: 118/167 (71%) Training Loss: 0.574184\n",
      "Epoch: 49 , Training Accuracy: 100/167 (60%) Training Loss: 0.575879\n",
      "Epoch: 50 , Training Accuracy: 106/167 (63%) Training Loss: 0.569014\n",
      "Validate Accuracy: 16/33 (48%)\n",
      "Training on Validation Set starts with lr 1e-05\n",
      "Epoch: 1 , Training Accuracy: 11/33 (33%) Training Loss: 1.133523\n",
      "Epoch: 2 , Training Accuracy: 11/33 (33%) Training Loss: 1.056108\n",
      "Epoch: 3 , Training Accuracy: 11/33 (33%) Training Loss: 1.009766\n",
      "Epoch: 4 , Training Accuracy: 12/33 (36%) Training Loss: 0.960138\n",
      "Epoch: 5 , Training Accuracy: 9/33 (27%) Training Loss: 1.102036\n",
      "Epoch: 6 , Training Accuracy: 14/33 (42%) Training Loss: 1.062204\n",
      "Epoch: 7 , Training Accuracy: 11/33 (33%) Training Loss: 0.990945\n",
      "Epoch: 8 , Training Accuracy: 7/33 (21%) Training Loss: 1.060488\n",
      "Epoch: 9 , Training Accuracy: 9/33 (27%) Training Loss: 0.918098\n",
      "Epoch: 10 , Training Accuracy: 12/33 (36%) Training Loss: 0.996745\n",
      "Epoch: 11 , Training Accuracy: 7/33 (21%) Training Loss: 0.918701\n",
      "Epoch: 12 , Training Accuracy: 9/33 (27%) Training Loss: 1.113518\n",
      "Epoch: 13 , Training Accuracy: 6/33 (18%) Training Loss: 1.002131\n",
      "Epoch: 14 , Training Accuracy: 12/33 (36%) Training Loss: 0.997218\n",
      "Epoch: 15 , Training Accuracy: 11/33 (33%) Training Loss: 0.952799\n",
      "Epoch: 16 , Training Accuracy: 8/33 (24%) Training Loss: 0.972301\n",
      "Epoch: 17 , Training Accuracy: 10/33 (30%) Training Loss: 1.039832\n",
      "Epoch: 18 , Training Accuracy: 9/33 (27%) Training Loss: 1.097005\n",
      "Epoch: 19 , Training Accuracy: 11/33 (33%) Training Loss: 1.059245\n",
      "Epoch: 20 , Training Accuracy: 7/33 (21%) Training Loss: 0.937174\n",
      "Epoch: 21 , Training Accuracy: 9/33 (27%) Training Loss: 0.932602\n",
      "Epoch: 22 , Training Accuracy: 14/33 (42%) Training Loss: 1.101267\n",
      "Epoch: 23 , Training Accuracy: 6/33 (18%) Training Loss: 1.048414\n",
      "Epoch: 24 , Training Accuracy: 10/33 (30%) Training Loss: 0.962092\n",
      "Epoch: 25 , Training Accuracy: 10/33 (30%) Training Loss: 0.983783\n",
      "Epoch: 26 , Training Accuracy: 10/33 (30%) Training Loss: 1.004202\n",
      "Epoch: 27 , Training Accuracy: 10/33 (30%) Training Loss: 1.065755\n",
      "Epoch: 28 , Training Accuracy: 8/33 (24%) Training Loss: 1.117069\n",
      "Epoch: 29 , Training Accuracy: 6/33 (18%) Training Loss: 1.071319\n",
      "Epoch: 30 , Training Accuracy: 9/33 (27%) Training Loss: 1.054806\n",
      "Epoch: 31 , Training Accuracy: 9/33 (27%) Training Loss: 1.091383\n",
      "Epoch: 32 , Training Accuracy: 10/33 (30%) Training Loss: 1.061612\n",
      "Epoch: 33 , Training Accuracy: 10/33 (30%) Training Loss: 0.944543\n",
      "Epoch: 34 , Training Accuracy: 9/33 (27%) Training Loss: 1.081972\n",
      "Epoch: 35 , Training Accuracy: 7/33 (21%) Training Loss: 1.092566\n",
      "Epoch: 36 , Training Accuracy: 7/33 (21%) Training Loss: 0.968424\n",
      "Epoch: 37 , Training Accuracy: 11/33 (33%) Training Loss: 0.905042\n",
      "Epoch: 38 , Training Accuracy: 7/33 (21%) Training Loss: 1.032375\n",
      "Epoch: 39 , Training Accuracy: 9/33 (27%) Training Loss: 1.066998\n",
      "Epoch: 40 , Training Accuracy: 11/33 (33%) Training Loss: 1.068774\n",
      "Epoch: 41 , Training Accuracy: 7/33 (21%) Training Loss: 0.972575\n",
      "Epoch: 42 , Training Accuracy: 13/33 (39%) Training Loss: 1.059008\n",
      "Epoch: 43 , Training Accuracy: 14/33 (42%) Training Loss: 1.052616\n",
      "Epoch: 44 , Training Accuracy: 8/33 (24%) Training Loss: 1.112808\n",
      "Epoch: 45 , Training Accuracy: 8/33 (24%) Training Loss: 1.077947\n",
      "Epoch: 46 , Training Accuracy: 11/33 (33%) Training Loss: 1.024740\n",
      "Epoch: 47 , Training Accuracy: 12/33 (36%) Training Loss: 1.048917\n",
      "Epoch: 48 , Training Accuracy: 12/33 (36%) Training Loss: 1.075402\n",
      "Epoch: 49 , Training Accuracy: 10/33 (30%) Training Loss: 1.056818\n",
      "Epoch: 50 , Training Accuracy: 9/33 (27%) Training Loss: 1.068951\n",
      "test set  Accuracy: 3/9 (33%)\n"
     ]
    }
   ],
   "source": [
    "train(model,device,trainloader,valloader, testloader,optimizer,scheduler,criterion,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4ce12",
   "metadata": {
    "papermill": {
     "duration": 0.063852,
     "end_time": "2022-05-26T14:13:07.978109",
     "exception": false,
     "start_time": "2022-05-26T14:13:07.914257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bc12e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T14:13:08.107348Z",
     "iopub.status.busy": "2022-05-26T14:13:08.107086Z",
     "iopub.status.idle": "2022-05-26T14:13:08.110757Z",
     "shell.execute_reply": "2022-05-26T14:13:08.110078Z"
    },
    "papermill": {
     "duration": 0.070157,
     "end_time": "2022-05-26T14:13:08.112374",
     "exception": false,
     "start_time": "2022-05-26T14:13:08.042217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " #test(model, device, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3bca192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-26T14:13:08.241146Z",
     "iopub.status.busy": "2022-05-26T14:13:08.240926Z",
     "iopub.status.idle": "2022-05-26T14:13:08.244988Z",
     "shell.execute_reply": "2022-05-26T14:13:08.244316Z"
    },
    "papermill": {
     "duration": 0.070907,
     "end_time": "2022-05-26T14:13:08.246603",
     "exception": false,
     "start_time": "2022-05-26T14:13:08.175696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from 21 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23d769",
   "metadata": {
    "papermill": {
     "duration": 0.063949,
     "end_time": "2022-05-26T14:13:08.374583",
     "exception": false,
     "start_time": "2022-05-26T14:13:08.310634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5359.465345,
   "end_time": "2022-05-26T14:13:11.221820",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-26T12:43:51.756475",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
