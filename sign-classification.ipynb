{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ce9854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:38:29.045195Z",
     "iopub.status.busy": "2022-06-20T01:38:29.044714Z",
     "iopub.status.idle": "2022-06-20T01:39:04.778675Z",
     "shell.execute_reply": "2022-06-20T01:39:04.777849Z"
    },
    "papermill": {
     "duration": 35.754634,
     "end_time": "2022-06-20T01:39:04.781319",
     "exception": false,
     "start_time": "2022-06-20T01:38:29.026685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideo\r\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting fvcore\r\n",
      "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting av\r\n",
      "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting parameterized\r\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting iopath\r\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\r\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.63.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.1.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.0.1)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.4.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->pytorchvideo) (5.1.1)\r\n",
      "Building wheels for collected packages: pytorchvideo, fvcore\r\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188714 sha256=2d5a3e89d98d4c019246bd9f7c156ba96099bb770643e3cc4dc1c7705a346327\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/51/05/053b29bac2400cbbae2fb7cfc41afd280d627bca7c9363ca80\r\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=d71d89c0b2920f04be88f0a9e5c522ca1d06308fea84521223d96bcbabb95d86\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\r\n",
      "Successfully built pytorchvideo fvcore\r\n",
      "Installing collected packages: parameterized, av, iopath, fvcore, pytorchvideo\r\n",
      "Successfully installed av-9.2.0 fvcore-0.1.5.post20220512 iopath-0.1.9 parameterized-0.8.1 pytorchvideo-0.1.5\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting imblearn\r\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\r\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (from imblearn) (0.9.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (3.1.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.21.6)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.7.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.0.1)\r\n",
      "Installing collected packages: imblearn\r\n",
      "Successfully installed imblearn-0.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo\n",
    "!pip install torchsummary\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32748b8d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:04.846436Z",
     "iopub.status.busy": "2022-06-20T01:39:04.846202Z",
     "iopub.status.idle": "2022-06-20T01:39:08.211783Z",
     "shell.execute_reply": "2022-06-20T01:39:08.211058Z"
    },
    "papermill": {
     "duration": 3.40006,
     "end_time": "2022-06-20T01:39:08.213869",
     "exception": false,
     "start_time": "2022-06-20T01:39:04.813809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ") \n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize\n",
    "#from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.y)\n",
    "\n",
    "        # length = 0\n",
    "        # with open(self.file, 'rb') as f:\n",
    "        #     data = pickle.load(f)\n",
    "\n",
    "        # for x in data:\n",
    "        #     length += len(data[x])\n",
    "        # return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f746393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.278240Z",
     "iopub.status.busy": "2022-06-20T01:39:08.278044Z",
     "iopub.status.idle": "2022-06-20T01:39:08.295356Z",
     "shell.execute_reply": "2022-06-20T01:39:08.294728Z"
    },
    "papermill": {
     "duration": 0.051285,
     "end_time": "2022-06-20T01:39:08.296887",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.245602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 720\n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_CHANNEL = 1\n",
    "NUM_FRAMES = 25\n",
    "NUM_CLASSES = 60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def transform_data(x):\n",
    "    \n",
    "    transform =  ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose(\n",
    "            [\n",
    "                Lambda(lambda x: x/255.0),\n",
    "                Normalize(([0.2933, 0.4799, 0.6723]), ([0.1576, 0.1674, 0.2477])),\n",
    "                Grayscale(num_output_channels=1),\n",
    "                CenterCrop([720,900]),\n",
    "                Resize([512,512]),\n",
    "                Lambda(lambda x: x.permute(1,2,3,0)),#(channel, frames(depth), height, width)\n",
    "#                 Lambda(lambda x: print(x.shape))\n",
    "\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return transform(x)\n",
    "\n",
    "def lp_video(video,start_time, end_time):\n",
    "    global means,stds\n",
    "    global nst\n",
    "    \n",
    "    video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n",
    "            #print(video_data[\"video\"].shape)\n",
    " \n",
    "    if video_data[\"video\"] is None:\n",
    "        return None        # or pass\n",
    "    else:\n",
    "    \n",
    "        video_data[\"video\"] = video_data[\"video\"].permute(1,0,2,3)\n",
    "        \n",
    "        video_data = transform_data( video_data)\n",
    "\n",
    "        return video_data[\"video\"]\n",
    "\n",
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    \n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    global neg\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        \n",
    "        filename = key\n",
    "        print(\"file\",filename)\n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "        \n",
    "        for x in data[key]:\n",
    "            if x[0] in range(5):\n",
    "                img_cls = x[0]\n",
    "\n",
    "\n",
    "\n",
    "                # start_time = x[1]\n",
    "                # end_time = x[2]\n",
    "                vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "                for m in torch.unbind(vid, dim=3):\n",
    "                    \n",
    "                    train_classes.append(img_cls)\n",
    "                    train_inputs.append(m.view(-1).numpy())\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    " #weights =torch.tensor([166.0, 370.0, 63.0, 232.0, 905.0, 102.0, 537.0, 993.0, 133.0, 154.0, 286.0, 435.0, 452.0, 779.0, 354.0, 136.0, 232.0, 2154.0, 374.0, 787.0, 337.0, 268.0, 954.0, 167.0, 543.0, 324.0, 669.0, 333.0, 211.0, 433.0, 140.0, 665.0, 107.0, 305.0, 162.0, 61.0, 1229.0, 958.0, 897.0, 543.0, 625.0, 334.0, 776.0, 111.0, 412.0, 583.0, 255.0, 103.0, 108.0, 201.0, 384.0, 632.0, 13.0, 322.0, 664.0, 435.0, 194.0, 322.0, 113.0, 462.0,2330.0])\n",
    "\n",
    "#if under 100 ignore\n",
    "#count round of train\n",
    "    sampling_stat = {0:500,1:500}\n",
    "   # sampling_stat = {0:500,1:500,2:67.0,3:500}  \n",
    "    over = SMOTE(sampling_strategy=sampling_stat,k_neighbors=2)\n",
    "    x_over, y_over = over.fit_resample(np.array(train_inputs), np.array(train_classes))\n",
    "    print(\"Class:{} Count:{}\".format(*np.unique(y_over, return_counts=True)))  \n",
    " \n",
    "    x_over = torch.from_numpy(x_over).view(-1,1,512,512)\n",
    "    \n",
    "   \n",
    "    print(x_over[0].shape)\n",
    "    print(x_over.shape)\n",
    "  \n",
    "            \n",
    "    signds = SignDataset(x_over, y_over)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "  \n",
    "    return trainloader, valloader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29113d50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.362287Z",
     "iopub.status.busy": "2022-06-20T01:39:08.361644Z",
     "iopub.status.idle": "2022-06-20T01:39:08.369947Z",
     "shell.execute_reply": "2022-06-20T01:39:08.369313Z"
    },
    "papermill": {
     "duration": 0.044271,
     "end_time": "2022-06-20T01:39:08.371981",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.327710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_testloader(batch_size,start,end):\n",
    "\n",
    "    time_start = []\n",
    "    time_end = []\n",
    "    train_inputs =[] #x\n",
    "    train_classes = [] #y\n",
    "    \n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        filename = key\n",
    "        print(\"file\",filename)\n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "\n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "            for m in torch.unbind(vid, dim=3):\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    \n",
    "    \n",
    "\n",
    "    testloader = DataLoader(signds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b27030fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.435798Z",
     "iopub.status.busy": "2022-06-20T01:39:08.435613Z",
     "iopub.status.idle": "2022-06-20T01:39:08.440330Z",
     "shell.execute_reply": "2022-06-20T01:39:08.439599Z"
    },
    "papermill": {
     "duration": 0.037778,
     "end_time": "2022-06-20T01:39:08.441885",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.404107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_vid(batch_size,start,end):\n",
    "\n",
    "\n",
    "    train_inputs =[] #x\n",
    "\n",
    "    video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Val_Set/VALIDATION/MSSL_VAL_SET_VIDEOS/p01_n131.mp4\")\n",
    "    # file functions\n",
    "\n",
    "    vid = lp_video(video,start, end)\n",
    "\n",
    "    for m in torch.unbind(vid, dim=3):\n",
    "        train_inputs.append(m)\n",
    "\n",
    "    \n",
    "\n",
    "    testloader = DataLoader(train_inputs, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    return testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b74eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.506109Z",
     "iopub.status.busy": "2022-06-20T01:39:08.505882Z",
     "iopub.status.idle": "2022-06-20T01:39:08.557793Z",
     "shell.execute_reply": "2022-06-20T01:39:08.557072Z"
    },
    "papermill": {
     "duration": 0.086397,
     "end_time": "2022-06-20T01:39:08.559970",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.473573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a EfficientNetV2 Model as defined in:\n",
    "Mingxing Tan, Quoc V. Le. (2021). \n",
    "EfficientNetV2: Smaller Models and Faster Training\n",
    "arXiv preprint arXiv:2104.00298.\n",
    "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n",
    "\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "# SiLU (Swish) activation function\n",
    "if hasattr(nn, 'SiLU'):\n",
    "    SiLU = nn.SiLU\n",
    "else:\n",
    "    # For compatibility with old PyTorch versions\n",
    "    class SiLU(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x * torch.sigmoid(x)\n",
    "\n",
    " \n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, oup, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
    "                SiLU(),\n",
    "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
    "        super(MBConv, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "        if use_se:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                SELayer(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # fused\n",
    "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class EffNetV2(nn.Module):\n",
    "    def __init__(self, cfgs, num_classes=60, width_mult=1.):\n",
    "        super(EffNetV2, self).__init__()\n",
    "        self.cfgs = cfgs\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(24 * width_mult, 8)\n",
    "        layers = [conv_3x3_bn(1, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = MBConv\n",
    "        for t, c, n, s, use_se in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 8)\n",
    "            for i in range(n):\n",
    "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
    "                input_channel = output_channel\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
    "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(output_channel, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.001)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def effnetv2_s(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-S model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  24,  2, 1, 0],\n",
    "        [4,  48,  4, 2, 0],\n",
    "        [4,  64,  4, 2, 0],\n",
    "        [4, 128,  6, 2, 1],\n",
    "        [6, 160,  9, 1, 1],\n",
    "        [6, 256, 15, 2, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_m(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-M model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  24,  3, 1, 0],\n",
    "        [4,  48,  5, 2, 0],\n",
    "        [4,  80,  5, 2, 0],\n",
    "        [4, 160,  7, 2, 1],\n",
    "        [6, 176, 14, 1, 1],\n",
    "        [6, 304, 18, 2, 1],\n",
    "        [6, 512,  5, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_l(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-L model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  7, 2, 0],\n",
    "        [4,  96,  7, 2, 0],\n",
    "        [4, 192, 10, 2, 1],\n",
    "        [6, 224, 19, 1, 1],\n",
    "        [6, 384, 25, 2, 1],\n",
    "        [6, 640,  7, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)\n",
    "\n",
    "\n",
    "def effnetv2_xl(**kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNetV2-XL model\n",
    "    \"\"\"\n",
    "    cfgs = [\n",
    "        # t, c, n, s, SE\n",
    "        [1,  32,  4, 1, 0],\n",
    "        [4,  64,  8, 2, 0],\n",
    "        [4,  96,  8, 2, 0],\n",
    "        [4, 192, 16, 2, 1],\n",
    "        [6, 256, 24, 1, 1],\n",
    "        [6, 512, 32, 2, 1],\n",
    "        [6, 640,  8, 1, 1],\n",
    "    ]\n",
    "    return EffNetV2(cfgs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890f6a3",
   "metadata": {
    "papermill": {
     "duration": 0.040237,
     "end_time": "2022-06-20T01:39:08.638863",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.598626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54c03b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.718918Z",
     "iopub.status.busy": "2022-06-20T01:39:08.718434Z",
     "iopub.status.idle": "2022-06-20T01:39:08.725510Z",
     "shell.execute_reply": "2022-06-20T01:39:08.724809Z"
    },
    "papermill": {
     "duration": 0.047557,
     "end_time": "2022-06-20T01:39:08.727329",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.679772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evluate(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            \n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            print(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40d805f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.792975Z",
     "iopub.status.busy": "2022-06-20T01:39:08.792396Z",
     "iopub.status.idle": "2022-06-20T01:39:08.799402Z",
     "shell.execute_reply": "2022-06-20T01:39:08.798807Z"
    },
    "papermill": {
     "duration": 0.040846,
     "end_time": "2022-06-20T01:39:08.800960",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.760114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, device, loader,criterion, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            target = (target).to(device)\n",
    "\n",
    "            output = model(data.half())  \n",
    "            test_loss += criterion(output, target)\n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            #print(\"correct\", correct)\n",
    "    test_loss /= num_samples\n",
    "    print(' Val Accuracy: {}/{} ({:.0f}%) Val Loss: {:.6f}'.format(correct, num_samples,100. * correct / num_samples, test_loss))\n",
    "    return  test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a252eb3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.864849Z",
     "iopub.status.busy": "2022-06-20T01:39:08.864629Z",
     "iopub.status.idle": "2022-06-20T01:39:08.876624Z",
     "shell.execute_reply": "2022-06-20T01:39:08.875997Z"
    },
    "papermill": {
     "duration": 0.046075,
     "end_time": "2022-06-20T01:39:08.878349",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.832274",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader,validloader,scheduler, optimizer,criterion,epochs,lr):\n",
    "    print(\"Train start\")\n",
    "    breakout = False\n",
    "    \n",
    "    last_loss = 100\n",
    "    patience = 3\n",
    "    triggertimes = 0\n",
    "    \n",
    "    \n",
    "    model.half()\n",
    "    model.cuda()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "        if optimizer.param_groups[0]['lr'] < 1e-5:    \n",
    "            scheduler.step()\n",
    "        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 93:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            \n",
    "            current_loss = test(model,device,validloader,criterion, mode = \"Validating\")\n",
    "            \n",
    "        \n",
    "            print(\"breakout\")\n",
    "            return model, optimizer\n",
    "            \n",
    "            \n",
    "\n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        \n",
    "        current_loss = test(model,device,validloader,criterion)\n",
    "        if current_loss > last_loss:\n",
    "            trigger_times += 1\n",
    "            print('Trigger Times for early stopping:', trigger_times)\n",
    "\n",
    "            if trigger_times >= patience:\n",
    "                print('Early stopping!\\nStart to test process.')\n",
    "                return model, optimizer\n",
    "\n",
    "        else:\n",
    "            print('trigger times: 0')\n",
    "            trigger_times = 0\n",
    "\n",
    "        last_loss = current_loss\n",
    "    \n",
    "    \n",
    "        \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fe9029",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:08.941848Z",
     "iopub.status.busy": "2022-06-20T01:39:08.941646Z",
     "iopub.status.idle": "2022-06-20T01:39:09.989546Z",
     "shell.execute_reply": "2022-06-20T01:39:09.988772Z"
    },
    "papermill": {
     "duration": 1.082738,
     "end_time": "2022-06-20T01:39:09.991869",
     "exception": false,
     "start_time": "2022-06-20T01:39:08.909131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 60\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('frontend',effnetv2_m())\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "\n",
    "# specify optimizer and learning rate\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        \n",
    "        {\"params\": model.frontend.parameters(), \"lr\": 1e-2},\n",
    "  ],\n",
    "  momentum = 0.9\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# state = torch.load(\"../input/fmosel/model_optimizer34.pt\")\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.half()\n",
    "# model.cuda()\n",
    "# optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edbf89f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:10.057150Z",
     "iopub.status.busy": "2022-06-20T01:39:10.056903Z",
     "iopub.status.idle": "2022-06-20T01:39:10.060335Z",
     "shell.execute_reply": "2022-06-20T01:39:10.059620Z"
    },
    "papermill": {
     "duration": 0.038059,
     "end_time": "2022-06-20T01:39:10.061970",
     "exception": false,
     "start_time": "2022-06-20T01:39:10.023911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdeb309a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:10.124906Z",
     "iopub.status.busy": "2022-06-20T01:39:10.124442Z",
     "iopub.status.idle": "2022-06-20T01:39:10.127520Z",
     "shell.execute_reply": "2022-06-20T01:39:10.126843Z"
    },
    "papermill": {
     "duration": 0.036287,
     "end_time": "2022-06-20T01:39:10.129093",
     "exception": false,
     "start_time": "2022-06-20T01:39:10.092806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#summary(model, (1, 512, 640), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cd999d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:39:10.192350Z",
     "iopub.status.busy": "2022-06-20T01:39:10.191859Z",
     "iopub.status.idle": "2022-06-20T01:42:28.812850Z",
     "shell.execute_reply": "2022-06-20T01:42:28.811645Z"
    },
    "papermill": {
     "duration": 198.65518,
     "end_time": "2022-06-20T01:42:28.815483",
     "exception": false,
     "start_time": "2022-06-20T01:39:10.160303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n002\n",
      "file p06_n019\n",
      "file p06_n007\n",
      "file p01_n053\n",
      "file p04_n074\n",
      "file p05_n077\n",
      "file p05_n027\n",
      "file p05_n022\n",
      "file p01_n110\n",
      "file p06_n036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (500) in class 0 will be larger than the number of samples in the majority class (class #4 -> 108)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n",
      "/opt/conda/lib/python3.7/site-packages/imblearn/utils/_validation.py:300: UserWarning: After over-sampling, the number of samples (500) in class 1 will be larger than the number of samples in the majority class (class #4 -> 108)\n",
      "  f\"After over-sampling, the number of samples ({n_samples})\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:[0 1 3 4] Count:[500 500  17 108]\n",
      "torch.Size([1, 512, 512])\n",
      "torch.Size([1125, 1, 512, 512])\n",
      "Train start\n",
      "Epoch: 1 , Training Accuracy: 643/900 (71%) Training Loss: 0.058005\n",
      " Val Accuracy: 101/225 (45%) Val Loss: 0.115295\n",
      "trigger times: 0\n",
      "Epoch: 2 , Training Accuracy: 836/900 (93%) Training Loss: 0.012649\n",
      " Val Accuracy: 117/225 (52%) Val Loss: 0.237793\n",
      "Trigger Times for early stopping: 1\n",
      "Epoch: 3 , Training Accuracy: 861/900 (96%) Training Loss: 0.008507\n",
      " Val Accuracy: 211/225 (94%) Val Loss: 0.008804\n",
      "breakout\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader = load_dataloaders(batch_size=16,start=0, end=10)#73 not included \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
    "model, optimizer = train(model,device,trainloader,valloader,scheduler,optimizer,criterion,30,1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872b25a",
   "metadata": {
    "papermill": {
     "duration": 0.036342,
     "end_time": "2022-06-20T01:42:28.888738",
     "exception": false,
     "start_time": "2022-06-20T01:42:28.852396",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc829964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T01:42:28.962617Z",
     "iopub.status.busy": "2022-06-20T01:42:28.962388Z",
     "iopub.status.idle": "2022-06-20T01:42:29.406837Z",
     "shell.execute_reply": "2022-06-20T01:42:29.405981Z"
    },
    "papermill": {
     "duration": 0.484312,
     "end_time": "2022-06-20T01:42:29.409145",
     "exception": false,
     "start_time": "2022-06-20T01:42:28.924833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer3477.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e4617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-11T05:14:57.294674Z",
     "iopub.status.busy": "2022-06-11T05:14:57.294393Z",
     "iopub.status.idle": "2022-06-11T05:21:50.529745Z",
     "shell.execute_reply": "2022-06-11T05:21:50.528621Z",
     "shell.execute_reply.started": "2022-06-11T05:14:57.294643Z"
    },
    "papermill": {
     "duration": 0.037909,
     "end_time": "2022-06-20T01:42:29.485230",
     "exception": false,
     "start_time": "2022-06-20T01:42:29.447321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 249.960138,
   "end_time": "2022-06-20T01:42:31.044653",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-20T01:38:21.084515",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
