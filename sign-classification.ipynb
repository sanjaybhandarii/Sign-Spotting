{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab404bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:24.039594Z",
     "iopub.status.busy": "2022-06-04T13:29:24.039225Z",
     "iopub.status.idle": "2022-06-04T13:29:54.117144Z",
     "shell.execute_reply": "2022-06-04T13:29:54.116177Z"
    },
    "papermill": {
     "duration": 30.10008,
     "end_time": "2022-06-04T13:29:54.120675",
     "exception": false,
     "start_time": "2022-06-04T13:29:24.020595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorchvideo\r\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting fvcore\r\n",
      "  Downloading fvcore-0.1.5.post20220512.tar.gz (50 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting av\r\n",
      "  Downloading av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting parameterized\r\n",
      "  Downloading parameterized-0.8.1-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting iopath\r\n",
      "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from pytorchvideo) (2.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.21.6)\r\n",
      "Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.1.8)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (6.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (4.63.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (1.1.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (9.0.1)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore->pytorchvideo) (0.8.9)\r\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath->pytorchvideo) (2.4.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->pytorchvideo) (5.1.1)\r\n",
      "Building wheels for collected packages: pytorchvideo, fvcore\r\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188714 sha256=1bde33a075d19a83748d9323e96f06ae9da7fc8a97ec09e4edfac53c7cc14650\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/51/05/053b29bac2400cbbae2fb7cfc41afd280d627bca7c9363ca80\r\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=a6b774cf6570de9d4a2b231d20ce53092adf8bcebce7198b175f854ad40954cb\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/20/f9/a11a0dd63f4c13678b2a5ec488e48078756505c7777b75b29e\r\n",
      "Successfully built pytorchvideo fvcore\r\n",
      "Installing collected packages: parameterized, av, iopath, fvcore, pytorchvideo\r\n",
      "Successfully installed av-9.2.0 fvcore-0.1.5.post20220512 iopath-0.1.9 parameterized-0.8.1 pytorchvideo-0.1.5\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting torchsummary\r\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\r\n",
      "Installing collected packages: torchsummary\r\n",
      "Successfully installed torchsummary-1.5.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorchvideo\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3415069f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:54.190642Z",
     "iopub.status.busy": "2022-06-04T13:29:54.190352Z",
     "iopub.status.idle": "2022-06-04T13:29:54.194321Z",
     "shell.execute_reply": "2022-06-04T13:29:54.193247Z"
    },
    "papermill": {
     "duration": 0.040302,
     "end_time": "2022-06-04T13:29:54.196418",
     "exception": false,
     "start_time": "2022-06-04T13:29:54.156116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start= 0\n",
    "end = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514e5567",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:54.265052Z",
     "iopub.status.busy": "2022-06-04T13:29:54.264849Z",
     "iopub.status.idle": "2022-06-04T13:29:56.372003Z",
     "shell.execute_reply": "2022-06-04T13:29:56.371300Z"
    },
    "papermill": {
     "duration": 2.14364,
     "end_time": "2022-06-04T13:29:56.374059",
     "exception": false,
     "start_time": "2022-06-04T13:29:54.230419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ") \n",
    "import pickle\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda, Grayscale,Normalize, CenterCrop,Resize\n",
    "#from torchvision.transforms._transforms_video import CenterCropVideo\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "import sys\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SignDataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.y)\n",
    "\n",
    "        # length = 0\n",
    "        # with open(self.file, 'rb') as f:\n",
    "        #     data = pickle.load(f)\n",
    "\n",
    "        # for x in data:\n",
    "        #     length += len(data[x])\n",
    "        # return length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9a8e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:56.436375Z",
     "iopub.status.busy": "2022-06-04T13:29:56.436133Z",
     "iopub.status.idle": "2022-06-04T13:29:56.454363Z",
     "shell.execute_reply": "2022-06-04T13:29:56.453586Z"
    },
    "papermill": {
     "duration": 0.051776,
     "end_time": "2022-06-04T13:29:56.456336",
     "exception": false,
     "start_time": "2022-06-04T13:29:56.404560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 720\n",
    "IMAGE_WIDTH = 800\n",
    "IMAGE_CHANNEL = 1\n",
    "NUM_FRAMES = 10\n",
    "NUM_CLASSES = 60\n",
    "\n",
    "\n",
    "\n",
    "train_inputs =[] #x\n",
    "train_classes = [] #y\n",
    "\n",
    "\n",
    "def transform_data(x, mean, std):\n",
    "    \n",
    "    transform =  ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose(\n",
    "            [\n",
    "                Lambda(lambda x: x.permute(1,0,2,3)),#(frames(depth), channel, height, width) -> (channel, frames(depth), height, width)\n",
    "\n",
    "                UniformTemporalSubsample(NUM_FRAMES),\n",
    "                Lambda(lambda x: x.permute(1,0,2,3)),#(frames(depth), channel, height, width)\n",
    "                Lambda(lambda x: x/255.0),\n",
    "                \n",
    "                Normalize((mean,), (std,)),\n",
    "\n",
    "                #CenterCrop([720,900]),\n",
    "                Resize([480,600]),\n",
    "                Lambda(lambda x: x.permute(1,2,3,0)),#(channel, frames(depth), height, width)\n",
    "\n",
    "            ]\n",
    "\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return transform(x)\n",
    "\n",
    "def lp_video(video,start_time, end_time):\n",
    "    video_data = video.get_clip(start_sec=float(start_time)/1000.0, end_sec=float(end_time)/1000.0)\n",
    "            #print(video_data[\"video\"].shape)\n",
    "\n",
    "            \n",
    "    video_data[\"video\"] = Grayscale(num_output_channels=1)((video_data[\"video\"]).permute(1,0,2,3))\n",
    "#             video_data[\"video\"] = video_data[\"video\"]/255\n",
    "            #print(video_data[\"video\"].shape)\n",
    "            \n",
    "    std, mean = torch.std_mean(video_data[\"video\"])\n",
    "    std = std/255.0\n",
    "    mean = mean/255.0\n",
    "    video_data = transform_data( video_data, mean, std)\n",
    "\n",
    "    return video_data[\"video\"]\n",
    "\n",
    "def load_dataloaders(batch_size,start,end):\n",
    "\n",
    "    time_start = []\n",
    "    time_end = []\n",
    "\n",
    "    with open('../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_GT.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "\n",
    "# keys are files so iterate only limited files due to memory limitations.\n",
    "    for key in list(data.keys())[start:end]:\n",
    "        filename = key\n",
    "        print(\"file\",filename)\n",
    "        video = EncodedVideo.from_path(\"../input/signdataset/sign/MSSL_Train_Set/TRAIN/MSSL_TRAIN_SET_VIDEOS_ELAN/\"+filename+\".mp4\")\n",
    "    # file functions\n",
    "\n",
    "        for x in data[key]:\n",
    "            img_cls = x[0]\n",
    "            time_start.append(x[1])\n",
    "            time_end.append(x[2])\n",
    "            \n",
    "            \n",
    "            # start_time = x[1]\n",
    "            # end_time = x[2]\n",
    "            vid = lp_video(video,x[1], x[2])\n",
    "\n",
    "            for m in torch.unbind(vid, dim=3):\n",
    "                train_classes.append(img_cls)\n",
    "                train_inputs.append(m)\n",
    "\n",
    "\n",
    "            #some negative classes too\n",
    "            for i in range(len(time_start)-1):\n",
    "                if time_end[i]- time_start[i+1]>1500:\n",
    "                    start = time_end[i]+1200\n",
    "                    end = time_end[i]+1400\n",
    "                    vid = lp_video(video,x[1], x[2])\n",
    "                    for m in torch.unbind(vid, dim=3):\n",
    "                        train_classes.append(60)\n",
    "                        train_inputs.append(m)\n",
    "\n",
    "            \n",
    "        \n",
    "            \n",
    "\n",
    "    signds = SignDataset(train_inputs, train_classes)\n",
    "    trainlen = int(len(signds)*0.8)\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    train_set, val_test_set = torch.utils.data.random_split(signds, [trainlen, len(signds)-trainlen])\n",
    "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    valloader = DataLoader(val_test_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "  \n",
    "    return trainloader, valloader\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de12a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:56.519533Z",
     "iopub.status.busy": "2022-06-04T13:29:56.519308Z",
     "iopub.status.idle": "2022-06-04T13:29:56.546960Z",
     "shell.execute_reply": "2022-06-04T13:29:56.546254Z"
    },
    "papermill": {
     "duration": 0.060651,
     "end_time": "2022-06-04T13:29:56.548680",
     "exception": false,
     "start_time": "2022-06-04T13:29:56.488029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In ResNet, we see how the skip connection added as identity function from the inputs\n",
    "to interact with the Conv layers. But in DenseNet, we see instead of adding skip \n",
    "connection to Conv layers, we can append or concat the output of identity function\n",
    "with output of Conv layers.\n",
    "\n",
    "In ResNet, it is little tedious to make the dimensions to match for adding the skip\n",
    "connection and Conv Layers, but it is much simpler in DenseNet, as we concat the \n",
    "both the X and Conv's output.\n",
    "\n",
    "The key idea or the reason its called DenseNet is because the next layers not only get\n",
    "the input from previous layer but also preceeding layers before the previous layer. So \n",
    "the next layer becomes dense as it loaded with output from previous layers.\n",
    "\n",
    "Check Figure 7.7.2 from https://d2l.ai/chapter_convolutional-modern/densenet.html for \n",
    "why DenseNet is Dense?\n",
    "\n",
    "Two blocks comprise DenseNet, one is DenseBlock for concat operation and other is \n",
    "transition layer for controlling channels meaning dimensions (recall 1x1 Conv).\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\"\"\"\n",
    "In ResNet, we see how the skip connection added as identity function from the inputs\n",
    "to interact with the Conv layers. But in DenseNet, we see instead of adding skip \n",
    "connection to Conv layers, we can append or concat the output of identity function\n",
    "with output of Conv layers.\n",
    "\n",
    "In ResNet, it is little tedious to make the dimensions to match for adding the skip\n",
    "connection and Conv Layers, but it is much simpler in DenseNet, as we concat the \n",
    "both the X and Conv's output.\n",
    "\n",
    "The key idea or the reason its called DenseNet is because the next layers not only get\n",
    "the input from previous layer but also preceeding layers before the previous layer. So \n",
    "the next layer becomes dense as it loaded with output from previous layers.\n",
    "\n",
    "Check Figure 7.7.2 from https://d2l.ai/chapter_convolutional-modern/densenet.html for \n",
    "why DenseNet is Dense?\n",
    "\n",
    "Two blocks comprise DenseNet, one is DenseBlock for concat operation and other is \n",
    "transition layer for controlling channels meaning dimensions (recall 1x1 Conv).\n",
    "\"\"\"\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4*growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n",
    "                               padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_channel=1, growthRate=12, depth=10, reduction=0.5, n_classes=61, bottleneck=True):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2\n",
    "\n",
    "        nChannels = 2*growthRate\n",
    "        self.conv1 = nn.Conv2d(input_channel, nChannels, kernel_size=3, padding=1,\n",
    "                               bias=False)\n",
    "        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans3 = Transition(nChannels, nOutChannels)\n",
    "        \n",
    "        nChannels = nOutChannels\n",
    "        self.dense4 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(nChannels, n_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = torch.squeeze(F.adaptive_avg_pool2d(out, 1))\n",
    "        out = F.log_softmax(self.fc(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9733493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:56.609786Z",
     "iopub.status.busy": "2022-06-04T13:29:56.609577Z",
     "iopub.status.idle": "2022-06-04T13:29:56.613659Z",
     "shell.execute_reply": "2022-06-04T13:29:56.613000Z"
    },
    "papermill": {
     "duration": 0.036523,
     "end_time": "2022-06-04T13:29:56.615331",
     "exception": false,
     "start_time": "2022-06-04T13:29:56.578808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(outputs: Variable, labels: Variable) -> float:\n",
    "    \"\"\"Evaluate neural network outputs against non-one-hotted labels.\"\"\"\n",
    "    Y = labels.numpy()\n",
    "    Yhat = np.argmax(outputs, axis=1)\n",
    "    return float(np.sum(Yhat == Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab96e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:56.676423Z",
     "iopub.status.busy": "2022-06-04T13:29:56.676232Z",
     "iopub.status.idle": "2022-06-04T13:29:56.682477Z",
     "shell.execute_reply": "2022-06-04T13:29:56.681838Z"
    },
    "papermill": {
     "duration": 0.038376,
     "end_time": "2022-06-04T13:29:56.684211",
     "exception": false,
     "start_time": "2022-06-04T13:29:56.645835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def test(model, device, loader, mode=\"Validate\"):\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():    \n",
    "            data = data.to(device)\n",
    "            target = (target).to(device)\n",
    "\n",
    "            output = model(data.half())  \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            #print(\"correct\", correct)\n",
    "\n",
    "    acc = 100.0 * correct / num_samples\n",
    "    \n",
    "    print('{} Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                mode,correct, num_samples,\n",
    "                100. * correct / num_samples, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc42d1da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:56.744719Z",
     "iopub.status.busy": "2022-06-04T13:29:56.744535Z",
     "iopub.status.idle": "2022-06-04T13:29:56.754566Z",
     "shell.execute_reply": "2022-06-04T13:29:56.753938Z"
    },
    "papermill": {
     "duration": 0.042014,
     "end_time": "2022-06-04T13:29:56.756089",
     "exception": false,
     "start_time": "2022-06-04T13:29:56.714075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainonval(model, device,validloader, optimizer, criterion, epochs):\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Training on Validation Set starts with lr\",optimizer.param_groups[0]['lr'])\n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(validloader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        #print(scheduler.get_last_lr())\n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 90:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy on Val set: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            test(model, device, testloader, mode = \"Test after Training on validation set\")\n",
    "                \n",
    "            return model, optimizer\n",
    "            \n",
    "    \n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7eb540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:56.816955Z",
     "iopub.status.busy": "2022-06-04T13:29:56.816771Z",
     "iopub.status.idle": "2022-06-04T13:29:56.828499Z",
     "shell.execute_reply": "2022-06-04T13:29:56.827867Z"
    },
    "papermill": {
     "duration": 0.043961,
     "end_time": "2022-06-04T13:29:56.830046",
     "exception": false,
     "start_time": "2022-06-04T13:29:56.786085",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "def train(model, device, train_loader,validloader,scheduler, optimizer,criterion, epochs):\n",
    "    print(\"Train start\")\n",
    "    breakout = False\n",
    "    model.half()\n",
    "    model.cuda()\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        correct = 0\n",
    "        num_samples = 0\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.half())\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            _,pred = output.max(1)# get the index of the max log-probability\n",
    "            num_samples += target.size(0)\n",
    "            correct += (pred==target).sum().item()\n",
    "            \n",
    "        #if optimizer.param_groups[0]['lr'] == 1e-2:    \n",
    "        #scheduler.step()\n",
    "        #print(\"lr:\",optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        \n",
    "        if (100 * correct / num_samples) >= 95:\n",
    "            \n",
    "            print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,100. * correct / num_samples, train_loss))\n",
    "            \n",
    "            test(model,device,validloader, mode = \"Validating\")\n",
    "#             test(model, device, testloader, mode = \"Test before Training on validation set\")\n",
    "           # model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "#             test(model, device, testloader, mode = \"Test after training on validation set\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "            \n",
    "            breakout = True\n",
    "            print(\"breakout\")\n",
    "            return\n",
    "            \n",
    "            \n",
    "            \n",
    "       \n",
    "        \n",
    "\n",
    "        print('Epoch: {} , Training Accuracy: {}/{} ({:.0f}%) Training Loss: {:.6f}'.format(\n",
    "                epoch+1, correct, num_samples,\n",
    "                100. * correct / num_samples, train_loss))\n",
    "        if (((epoch+1)%5) == 0) :\n",
    "            test(model,device,validloader)\n",
    "            model.train()\n",
    "            \n",
    "    if breakout:\n",
    "            print(\"breakout\")\n",
    "            return\n",
    "    \n",
    "    \n",
    "    \n",
    "    #model, optimizer = trainonval(model, device, validloader, optimizer, criterion, epochs)\n",
    "    test(model, device, validloader, mode= \"test set \")\n",
    "    \n",
    "    \n",
    "        \n",
    "    torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"./model_optimizer1.pt\")\n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fcea0d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:56.892003Z",
     "iopub.status.busy": "2022-06-04T13:29:56.891431Z",
     "iopub.status.idle": "2022-06-04T13:29:56.984675Z",
     "shell.execute_reply": "2022-06-04T13:29:56.983986Z"
    },
    "papermill": {
     "duration": 0.126159,
     "end_time": "2022-06-04T13:29:56.986667",
     "exception": false,
     "start_time": "2022-06-04T13:29:56.860508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_classes = 61\n",
    "\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    ('frontend',DenseNet()),\n",
    "]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer and learning rate\n",
    "optimizer = optim.SGD(\n",
    "    [\n",
    "        \n",
    "        {\"params\": model.frontend.parameters(), \"lr\": 1e-3},\n",
    "  ],\n",
    "  momentum = 0.9\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "# state = torch.load(\"../input/sign-classification/model_optimizer1.pt\")\n",
    "# model.load_state_dict(state['model_state_dict'])\n",
    "# model.half()\n",
    "# model.cuda()\n",
    "# optimizer.load_state_dict(state['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f63154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:29:57.047841Z",
     "iopub.status.busy": "2022-06-04T13:29:57.047636Z",
     "iopub.status.idle": "2022-06-04T13:30:09.304660Z",
     "shell.execute_reply": "2022-06-04T13:30:09.302827Z"
    },
    "papermill": {
     "duration": 12.289954,
     "end_time": "2022-06-04T13:30:09.306876",
     "exception": false,
     "start_time": "2022-06-04T13:29:57.016922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file p01_n002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainloader, valloader = load_dataloaders(batch_size=32,start=start, end=end)#73 not included \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955f477",
   "metadata": {
    "papermill": {
     "duration": 0.033633,
     "end_time": "2022-06-04T13:30:09.375226",
     "exception": false,
     "start_time": "2022-06-04T13:30:09.341593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259e3cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:30:09.439251Z",
     "iopub.status.busy": "2022-06-04T13:30:09.439020Z",
     "iopub.status.idle": "2022-06-04T13:30:09.442727Z",
     "shell.execute_reply": "2022-06-04T13:30:09.441906Z"
    },
    "papermill": {
     "duration": 0.037605,
     "end_time": "2022-06-04T13:30:09.444508",
     "exception": false,
     "start_time": "2022-06-04T13:30:09.406903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print('Number of model parameters: {}'.format(sum([p.data.nelement() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00007570",
   "metadata": {
    "papermill": {
     "duration": 0.030948,
     "end_time": "2022-06-04T13:30:09.506599",
     "exception": false,
     "start_time": "2022-06-04T13:30:09.475651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04722514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:30:09.570365Z",
     "iopub.status.busy": "2022-06-04T13:30:09.569667Z",
     "iopub.status.idle": "2022-06-04T13:30:09.573016Z",
     "shell.execute_reply": "2022-06-04T13:30:09.572392Z"
    },
    "papermill": {
     "duration": 0.037137,
     "end_time": "2022-06-04T13:30:09.574648",
     "exception": false,
     "start_time": "2022-06-04T13:30:09.537511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#summary(model, (1, 480, 600), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436803cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:30:09.638548Z",
     "iopub.status.busy": "2022-06-04T13:30:09.638080Z",
     "iopub.status.idle": "2022-06-04T13:31:48.457808Z",
     "shell.execute_reply": "2022-06-04T13:31:48.456257Z"
    },
    "papermill": {
     "duration": 98.854083,
     "end_time": "2022-06-04T13:31:48.459729",
     "exception": false,
     "start_time": "2022-06-04T13:30:09.605646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:155: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 , Training Accuracy: 8/144 (6%) Training Loss: 0.142361\n",
      "Epoch: 2 , Training Accuracy: 5/144 (3%) Training Loss: 0.142334\n",
      "Epoch: 3 , Training Accuracy: 5/144 (3%) Training Loss: 0.141873\n",
      "Epoch: 4 , Training Accuracy: 7/144 (5%) Training Loss: 0.141629\n",
      "Epoch: 5 , Training Accuracy: 10/144 (7%) Training Loss: 0.141439\n",
      "Validate Accuracy: 0/36 (0%)\n",
      "Epoch: 6 , Training Accuracy: 23/144 (16%) Training Loss: 0.141222\n",
      "Epoch: 7 , Training Accuracy: 23/144 (16%) Training Loss: 0.141249\n",
      "Epoch: 8 , Training Accuracy: 4/144 (3%) Training Loss: 0.140761\n",
      "Epoch: 9 , Training Accuracy: 15/144 (10%) Training Loss: 0.140571\n",
      "Epoch: 10 , Training Accuracy: 17/144 (12%) Training Loss: 0.140340\n",
      "Validate Accuracy: 3/36 (8%)\n",
      "Epoch: 11 , Training Accuracy: 18/144 (12%) Training Loss: 0.139486\n",
      "Epoch: 12 , Training Accuracy: 22/144 (15%) Training Loss: 0.138685\n",
      "Epoch: 13 , Training Accuracy: 23/144 (16%) Training Loss: 0.138496\n",
      "Epoch: 14 , Training Accuracy: 24/144 (17%) Training Loss: 0.138143\n",
      "Epoch: 15 , Training Accuracy: 20/144 (14%) Training Loss: 0.137600\n",
      "Validate Accuracy: 4/36 (11%)\n",
      "Epoch: 16 , Training Accuracy: 18/144 (12%) Training Loss: 0.137655\n",
      "Epoch: 17 , Training Accuracy: 17/144 (12%) Training Loss: 0.137383\n",
      "Epoch: 18 , Training Accuracy: 21/144 (15%) Training Loss: 0.136542\n",
      "Epoch: 19 , Training Accuracy: 22/144 (15%) Training Loss: 0.135593\n",
      "Epoch: 20 , Training Accuracy: 25/144 (17%) Training Loss: 0.135484\n",
      "Validate Accuracy: 4/36 (11%)\n",
      "Epoch: 21 , Training Accuracy: 25/144 (17%) Training Loss: 0.135308\n",
      "Epoch: 22 , Training Accuracy: 25/144 (17%) Training Loss: 0.134861\n",
      "Epoch: 23 , Training Accuracy: 25/144 (17%) Training Loss: 0.134210\n",
      "Epoch: 24 , Training Accuracy: 25/144 (17%) Training Loss: 0.133613\n",
      "Epoch: 25 , Training Accuracy: 25/144 (17%) Training Loss: 0.132582\n",
      "Validate Accuracy: 5/36 (14%)\n",
      "Epoch: 26 , Training Accuracy: 25/144 (17%) Training Loss: 0.132189\n",
      "Epoch: 27 , Training Accuracy: 25/144 (17%) Training Loss: 0.131836\n",
      "Epoch: 28 , Training Accuracy: 25/144 (17%) Training Loss: 0.131334\n",
      "Epoch: 29 , Training Accuracy: 25/144 (17%) Training Loss: 0.130778\n",
      "Epoch: 30 , Training Accuracy: 25/144 (17%) Training Loss: 0.129774\n",
      "Validate Accuracy: 5/36 (14%)\n",
      "test set  Accuracy: 5/36 (14%)\n"
     ]
    }
   ],
   "source": [
    "train(model,device,trainloader,valloader,scheduler,optimizer,criterion,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a34ae650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-04T13:31:48.547170Z",
     "iopub.status.busy": "2022-06-04T13:31:48.546938Z",
     "iopub.status.idle": "2022-06-04T13:31:48.550595Z",
     "shell.execute_reply": "2022-06-04T13:31:48.549776Z"
    },
    "papermill": {
     "duration": 0.04912,
     "end_time": "2022-06-04T13:31:48.552475",
     "exception": false,
     "start_time": "2022-06-04T13:31:48.503355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from 21 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b21fca2",
   "metadata": {
    "papermill": {
     "duration": 0.041782,
     "end_time": "2022-06-04T13:31:48.636305",
     "exception": false,
     "start_time": "2022-06-04T13:31:48.594523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 154.282711,
   "end_time": "2022-06-04T13:31:50.301147",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-04T13:29:16.018436",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
